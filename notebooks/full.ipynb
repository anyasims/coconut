{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: CUDA_VISIBLE_DEVICES=5\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/80/anya/anaconda3/envs/coconut-1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import time\n",
    "import random\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "sys.path.append(os.path.abspath('/homes/80/anya/Documents/llm_tiny_ideas/super-tiny-lms-outer/super-tiny-lms'))\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# rank = int(os.environ[\"RANK\"])\n",
    "# print(rank)\n",
    "# world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "# print(world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constant_schedule_with_warmup(\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    num_warmup_steps: int,\n",
    "    last_epoch: int = -1,\n",
    "):\n",
    "    def lr_lambda(current_step):\n",
    "        return min(1, float(current_step) / float(max(1, num_warmup_steps)))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(dataset: Dataset, per_device_batch_size: int, rank : int, world_size : int, seed: int = 0):\n",
    "    dataset_length = len(dataset)\n",
    "    total_batch_size = per_device_batch_size * world_size\n",
    "    batches_per_dataset = dataset_length // total_batch_size\n",
    "    i = 0\n",
    "    epochs = 0\n",
    "    while True:\n",
    "        if i >= batches_per_dataset:\n",
    "            dataset = dataset.shuffle(seed + epochs)\n",
    "            i = 0\n",
    "            epochs += 1\n",
    "        start = i * total_batch_size + rank * per_device_batch_size\n",
    "        batch = dataset[start:start + per_device_batch_size]\n",
    "        yield batch\n",
    "\n",
    "def careful_repeat(data, num_repeats):\n",
    "    batch_size = data[list(data.keys())[0]].shape[0]\n",
    "    for k, v in data.items():\n",
    "        if v.ndim == 1:\n",
    "            data[k] = v.unsqueeze(1).repeat(1, num_repeats).reshape(batch_size*num_repeats, *v.shape[1:])\n",
    "        elif v.ndim == 2:\n",
    "            data[k] = v.unsqueeze(1).repeat(1, num_repeats, 1).reshape(batch_size*num_repeats, *v.shape[1:])\n",
    "    return data\n",
    "\n",
    "def get_model_param_stats(model, ref_model):\n",
    "    model_params = torch.cat([p.view(-1) for p in model.parameters() if p.requires_grad])\n",
    "    ref_model_params = torch.cat([p.view(-1) for p in ref_model.parameters()])\n",
    "    assert model_params.shape == ref_model_params.shape, f\"{model_params.shape=} {ref_model_params.shape=}\"\n",
    "    return {\n",
    "        \"params_with_grads_mean\": model_params.mean().item(),\n",
    "        \"params_with_grads_std\": model_params.std().item(),\n",
    "        \"distance_to_ref\": torch.nn.functional.mse_loss(model_params, ref_model_params).item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(logitss, temperature=0.0, top_k=None):\n",
    "    \"\"\"Get the next token\"\"\"\n",
    "    batch_size, seq_length, vocab_size = logitss.shape\n",
    "    assert seq_length == 1\n",
    "    logitss = logitss.squeeze(1)\n",
    "    if temperature == 0.0:\n",
    "        token_ids = torch.argmax(logitss, dim=-1)\n",
    "        probdists = torch.zeros_like(logitss)\n",
    "        probdists[torch.arange(batch_size), token_ids] = 1.0\n",
    "    else:\n",
    "        logitss = logitss / temperature\n",
    "        if top_k is not None:\n",
    "            logitss_k, idxs_k = torch.topk(logitss, min(top_k, vocab_size), dim=-1) # (batch_size, top_k)\n",
    "            probs_k = torch.nn.functional.softmax(logitss_k, dim=-1) # (batch_size, top_k)\n",
    "            idxs = torch.multinomial(probs_k, num_samples=1).squeeze(1) # (batch_size,)\n",
    "            token_ids = idxs_k[torch.arange(batch_size), idxs] # (batch_size)\n",
    "            probdists = torch.zeros_like(logitss)\n",
    "            # next_probdist_s[torch.arange(batch_size), idx_s_k.squeeze(1)] = probs_k.squeeze(1)\n",
    "            probdists.scatter_(1, idxs_k, probs_k)\n",
    "            if top_k == 1:\n",
    "                token_ids2 = torch.argmax(logitss, dim=-1)\n",
    "                probdists2 = torch.zeros_like(logitss)\n",
    "                probdists2[torch.arange(batch_size), token_ids2] = 1.0\n",
    "                assert (token_ids == token_ids2).all(), f\"{token_ids=}, {token_ids2=}\"\n",
    "                assert torch.allclose(probdists, probdists2), f\"{probdists2=}, {probdists2=}\"\n",
    "        else:\n",
    "            probdists = torch.nn.functional.softmax(logitss, dim=-1) # (batch_size, vocab_size)\n",
    "            token_ids = torch.multinomial(probdists, num_samples=1).squeeze(1) # (batch_size)\n",
    "\n",
    "    token_ids = token_ids.unsqueeze(1)\n",
    "    probdists = probdists.unsqueeze(1)\n",
    "    return token_ids, probdists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_step(model, next_input, attention_mask, position_ids, past_key_values, as_full_distribution=False):\n",
    "    batch_size = next_input.shape[0]\n",
    "    prev_seq_length = past_key_values[0][0].shape[2]\n",
    "    assert position_ids.shape == (batch_size, 1), f\"{position_ids.shape=}\"\n",
    "    assert attention_mask.shape == (batch_size, prev_seq_length+1), f\"{attention_mask.shape=}, {prev_seq_length=}\"\n",
    "\n",
    "    if as_full_distribution:\n",
    "        vocab_size = model.model.config.vocab_size\n",
    "        assert next_input.shape == (batch_size, 1, vocab_size)\n",
    "        all_embeds = model.model.embed_tokens.weight\n",
    "        hidden_dim = all_embeds.shape[1]\n",
    "        assert all_embeds.shape[0] == vocab_size\n",
    "        inputs_embeds = torch.matmul(next_input, all_embeds)\n",
    "        assert inputs_embeds.shape == (batch_size, 1, hidden_dim)\n",
    "        outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids=position_ids, past_key_values=past_key_values)\n",
    "    else:\n",
    "        assert next_input.shape == (batch_size, 1)\n",
    "        outputs = model(input_ids=next_input, attention_mask=attention_mask, position_ids=position_ids, past_key_values=past_key_values)\n",
    "    logits = outputs.logits\n",
    "    past_key_values = outputs.past_key_values\n",
    "    return logits, past_key_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "def batch_generate_rnn(\n",
    "        model,\n",
    "        ref_model,\n",
    "        questions_inputs,\n",
    "        answers_inputs,\n",
    "        max_steps=30,\n",
    "        step_for_answer=20,\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "        as_full_distribution=False,\n",
    "        dot_by_dot=False,\n",
    "        dot_by_dot_id=None,\n",
    "        inject_answer_prompt=False,\n",
    "        answer_prompt_ids=None,\n",
    "    ):\n",
    "    metrics = {}\n",
    "    assert not (as_full_distribution and dot_by_dot), f\"{as_full_distribution=}, {dot_by_dot=}\"\n",
    "    device = model.device\n",
    "    assert device.type == \"cuda\", f\"{model.device=}\"\n",
    "    assert ref_model.device == device, f\"{ref_model.device=}, {device=}\"\n",
    "    model.eval()\n",
    "    ref_model.eval()\n",
    "    batch_size = questions_inputs[\"input_ids\"].shape[0]\n",
    "    vocab_size = model.config.vocab_size\n",
    "    prompt_length = questions_inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    #### PROMPT FORWARD PASS\n",
    "    position_ids = questions_inputs[\"attention_mask\"].cumsum(dim=1) - 1\n",
    "    outputs = model(**questions_inputs, position_ids=position_ids)\n",
    "    with torch.no_grad():\n",
    "        ref_outputs = ref_model(**questions_inputs)\n",
    "    prompt_attention_mask = questions_inputs[\"attention_mask\"]\n",
    "    prompt_end_position_ids = position_ids[:, -1:] + 1\n",
    "    past_key_values = outputs.past_key_values\n",
    "    ref_past_keys_values = ref_outputs.past_key_values\n",
    "    logits = outputs.logits[:, -1:]\n",
    "    ref_logits = ref_outputs.logits[:, -1:]\n",
    "\n",
    "    def make_attention_mask(t, new_seq_length=1):\n",
    "        return torch.cat([prompt_attention_mask, torch.ones((batch_size, t+new_seq_length), device=device)], dim=1)\n",
    "    def make_position_ids(t, new_seq_length=1):\n",
    "        if new_seq_length == 1:\n",
    "            return prompt_end_position_ids + t\n",
    "        else:\n",
    "            return prompt_end_position_ids + t + torch.arange(new_seq_length).unsqueeze(0).to(device)\n",
    "\n",
    "    all_gen_logits = torch.zeros((batch_size, 0, vocab_size), device=device)\n",
    "    all_ref_logits = torch.zeros((batch_size, 0, vocab_size), device=device)\n",
    "    generations = torch.zeros((batch_size, 0), device=device, dtype=torch.int)\n",
    "    generations_without_injection = torch.zeros((batch_size, 0), device=device, dtype=torch.int)\n",
    "\n",
    "    ### REASONING FORWARD PASSES\n",
    "    for t in range(step_for_answer):\n",
    "        next_token_ids, next_probdists = get_next(logits, temperature=temperature, top_k=top_k)\n",
    "        all_gen_logits = torch.cat((all_gen_logits, logits), dim=1)\n",
    "        all_ref_logits = torch.cat((all_ref_logits, ref_logits), dim=1)\n",
    "        generations_without_injection = torch.cat((generations_without_injection, next_token_ids), dim=1)\n",
    "        generations = torch.cat((generations, next_token_ids), dim=1)\n",
    "        # forward pass of next token\n",
    "        if as_full_distribution:\n",
    "            next_input = next_probdists\n",
    "        elif dot_by_dot:\n",
    "            next_input = torch.full((batch_size, 1), dot_by_dot_id, dtype=torch.long, device=device)\n",
    "        else:\n",
    "            next_input = next_token_ids\n",
    "        logits, past_key_values = single_step(model,\n",
    "            next_input=next_input,\n",
    "            attention_mask=make_attention_mask(t),\n",
    "            position_ids=make_position_ids(t),\n",
    "            past_key_values=past_key_values,\n",
    "            as_full_distribution=as_full_distribution)\n",
    "        with torch.no_grad():\n",
    "            ref_logits, ref_past_keys_values = single_step(ref_model,\n",
    "                next_input=next_input,\n",
    "                attention_mask=make_attention_mask(t),\n",
    "                position_ids=make_position_ids(t),\n",
    "                past_key_values=ref_past_keys_values,\n",
    "                as_full_distribution=as_full_distribution)\n",
    "            \n",
    "    ### INJECT ANSWER PROMPT FORWARD PASS\n",
    "    t = step_for_answer\n",
    "    if inject_answer_prompt:\n",
    "        repeated_answer_prompt_ids = torch.tensor(answer_prompt_ids).unsqueeze(0).repeat(batch_size, 1).to(next_token_ids.dtype).to(device)\n",
    "        generations = torch.cat((generations, repeated_answer_prompt_ids), dim=1)\n",
    "        outputs = model(\n",
    "            input_ids=repeated_answer_prompt_ids,\n",
    "            attention_mask=make_attention_mask(step_for_answer, new_seq_length=len(answer_prompt_ids)),\n",
    "            position_ids=make_position_ids(step_for_answer, new_seq_length=len(answer_prompt_ids)),\n",
    "            past_key_values=past_key_values)\n",
    "        with torch.no_grad():\n",
    "            ref_outputs = ref_model(\n",
    "                input_ids=repeated_answer_prompt_ids,\n",
    "                attention_mask=make_attention_mask(t, new_seq_length=len(answer_prompt_ids)),\n",
    "                position_ids=make_position_ids(t, new_seq_length=len(answer_prompt_ids)),\n",
    "                past_key_values=ref_past_keys_values)\n",
    "        logits = outputs.logits\n",
    "        ref_logits = ref_outputs.logits\n",
    "        past_key_values = outputs.past_key_values\n",
    "        ref_past_keys_values = ref_outputs.past_key_values\n",
    "        t += len(answer_prompt_ids)\n",
    "\n",
    "    ### ANSWER FORWARD PASS\n",
    "    answer_length = answers_inputs[\"input_ids\"].shape[1]\n",
    "    answer_logits = model(\n",
    "        input_ids=answers_inputs[\"input_ids\"][:, :-1],\n",
    "        attention_mask=make_attention_mask(t, new_seq_length=answer_length-1),\n",
    "        position_ids=make_position_ids(t, new_seq_length=answer_length-1),\n",
    "        past_key_values=past_key_values).logits\n",
    "    with torch.no_grad():\n",
    "        ref_answer_logits = ref_model(\n",
    "            input_ids=answers_inputs[\"input_ids\"][:, :-1],\n",
    "            attention_mask=make_attention_mask(t+1, new_seq_length=answer_length-1),\n",
    "            position_ids=make_position_ids(t+1, new_seq_length=answer_length-1),\n",
    "            past_key_values=ref_past_keys_values).logits\n",
    "    answer_logits = torch.cat((all_gen_logits[:, -1:], answer_logits), dim=1)\n",
    "    ref_answer_logits = torch.cat((all_ref_logits[:, -1:], ref_answer_logits), dim=1)\n",
    "    per_token_answer_logps = torch.gather(answer_logits, 2, answers_inputs[\"input_ids\"].to(torch.long).unsqueeze(-1)).squeeze(-1)\n",
    "    per_token_ref_answer_logps = torch.gather(ref_answer_logits, 2, answers_inputs[\"input_ids\"].to(torch.long).unsqueeze(-1)).squeeze(-1)\n",
    "    assert per_token_answer_logps.shape == answers_inputs[\"attention_mask\"].shape, f\"{per_token_answer_logps.shape=}, {answers_inputs['attention_mask'].shape=}\"\n",
    "    answer_logps = (per_token_answer_logps * answers_inputs[\"attention_mask\"]).sum(dim=-1)\n",
    "    ref_answer_logps = (per_token_ref_answer_logps * answers_inputs[\"attention_mask\"]).sum(dim=-1)\n",
    "\n",
    "    answer_perplexity = torch.exp(-answer_logps / answers_inputs[\"attention_mask\"].sum(dim=-1))\n",
    "    answer_perplexity_ref = torch.exp(-ref_answer_logps / answers_inputs[\"attention_mask\"].sum(dim=-1))\n",
    "    metrics[\"answer_logps\"] = answer_logps.mean().item()\n",
    "    metrics[\"answer_logps_ref\"] = ref_answer_logps.mean().item()\n",
    "    metrics[\"answer_logps_diff\"] = (answer_logps - ref_answer_logps).mean().item()\n",
    "    metrics[\"answer_perplexity\"] = answer_perplexity.mean().item()\n",
    "    metrics[\"answer_perplexity_ref\"] = answer_perplexity_ref.mean().item()\n",
    "    metrics[\"answer_perplexity_diff\"] = (answer_perplexity - answer_perplexity_ref).mean().item()\n",
    "\n",
    "    ### CONTINUE FORWARD PASS STEPS\n",
    "    for t in range(t, max_steps):\n",
    "        next_token_ids, next_probdists = get_next(logits[:, -1:], temperature=temperature, top_k=top_k)\n",
    "        all_gen_logits = torch.cat((all_gen_logits, logits[:, -1:]), dim=1)\n",
    "        all_ref_logits = torch.cat((all_ref_logits, ref_logits[:, -1:]), dim=1)\n",
    "        generations_without_injection = torch.cat((generations_without_injection, next_token_ids), dim=1)\n",
    "        generations = torch.cat((generations, next_token_ids), dim=1)\n",
    "        # forward pass of next token\n",
    "        if as_full_distribution:\n",
    "            next_input = next_probdists\n",
    "        elif dot_by_dot:\n",
    "            next_input = torch.full((batch_size, 1), dot_by_dot_id, dtype=torch.long, device=device)\n",
    "        else:\n",
    "            next_input = next_token_ids\n",
    "        logits, past_key_values = single_step(model,\n",
    "            next_input=next_input,\n",
    "            attention_mask=make_attention_mask(t),\n",
    "            position_ids=make_position_ids(t),\n",
    "            past_key_values=past_key_values,\n",
    "            as_full_distribution=as_full_distribution)\n",
    "        with torch.no_grad():\n",
    "            ref_logits, ref_past_keys_values = single_step(ref_model,\n",
    "                next_input=next_input,\n",
    "                attention_mask=make_attention_mask(t),\n",
    "                position_ids=make_position_ids(t),\n",
    "                past_key_values=ref_past_keys_values,\n",
    "                as_full_distribution=as_full_distribution)\n",
    "            \n",
    "    next_token_ids, next_probdists = get_next(logits, temperature=temperature, top_k=top_k)\n",
    "    all_gen_logits = torch.cat((all_gen_logits, logits), dim=1)\n",
    "    all_ref_logits = torch.cat((all_ref_logits, ref_logits), dim=1)\n",
    "    generations_without_injection = torch.cat((generations_without_injection, next_token_ids), dim=1)\n",
    "    generations = torch.cat((generations, next_token_ids), dim=1)\n",
    "\n",
    "    gen_per_token_logps = torch.gather(all_gen_logits, 2, generations_without_injection.to(torch.long).unsqueeze(-1)).squeeze(-1)\n",
    "    ref_per_token_logps = torch.gather(all_ref_logits, 2, generations_without_injection.to(torch.long).unsqueeze(-1)).squeeze(-1)\n",
    "    x = (gen_per_token_logps, ref_per_token_logps)\n",
    "\n",
    "    pd = torch.nn.functional.softmax(all_gen_logits, dim=-1)\n",
    "    entropy = torch.logsumexp(all_gen_logits, dim=-1) - torch.sum(pd * all_gen_logits, dim=-1)\n",
    "    entropy = entropy.mean(-1)\n",
    "    metrics[\"logps\"] = gen_per_token_logps.mean().item()\n",
    "    metrics[\"logps_ref\"] = ref_per_token_logps.mean().item()\n",
    "    metrics[\"logps_diff\"] = (gen_per_token_logps - ref_per_token_logps).mean().item()\n",
    "    metrics[\"entropy\"] = entropy.mean().item()\n",
    "    metrics[\"entropy_std\"] = entropy.std().item()\n",
    "\n",
    "    x = (answer_logps, ref_answer_logps, gen_per_token_logps, ref_per_token_logps, entropy)\n",
    "    \n",
    "    assert x[0].requires_grad == True, f\"{x[0].requires_grad=}\"\n",
    "    assert x[1].requires_grad == False, f\"{x[1].requires_grad=}\"\n",
    "    assert x[2].requires_grad == True, f\"{x[2].requires_grad=}\"\n",
    "    assert x[3].requires_grad == False, f\"{x[3].requires_grad=}\"\n",
    "    assert x[4].requires_grad == True, f\"{x[4].requires_grad=}\"\n",
    "    return x, generations, metrics, past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "    ) -> None:\n",
    "        print(f\"\\nTrainer::-----------------------------------\")\n",
    "        self.world_size = 1\n",
    "        self.rank = 0\n",
    "        assert torch.cuda.is_available(), \"CUDA must be available for training\"\n",
    "        assert torch.cuda.device_count() == 1\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        # self.cfg = cfg\n",
    "        self.use_wandb = cfg.use_wandb\n",
    "\n",
    "        ### training\n",
    "        self.max_iters = cfg.max_iters\n",
    "        self.total_batch_size = cfg.total_batch_size\n",
    "        self.per_device_batch_size = cfg.per_device_batch_size\n",
    "        assert self.total_batch_size % (self.per_device_batch_size * self.world_size) == 0, f\"{self.total_batch_size=} {self.per_device_batch_size=}, {self.world_size=}\"\n",
    "        self.gradient_accumulation_steps = self.total_batch_size // (self.per_device_batch_size * self.world_size)\n",
    "        assert self.per_device_batch_size * self.world_size * self.gradient_accumulation_steps == self.total_batch_size, f\"{self.per_device_batch_size=} {self.world_size=} {self.gradient_accumulation_steps=} {self.total_batch_size=}\"\n",
    "        self.generations_per_prompt = cfg.generation.generations_per_prompt\n",
    "        assert self.per_device_batch_size % self.generations_per_prompt == 0, f\"{self.per_device_batch_size=} {self.generations_per_prompt=}\"\n",
    "        self.per_device_prompt_batch_size = self.per_device_batch_size // self.generations_per_prompt\n",
    "        print(f\"---TRAINING CONFIG:\")\n",
    "        print(f\"Max iters: {self.max_iters}\")\n",
    "        print(f\"Total batch size: {self.total_batch_size}\")\n",
    "        print(f\"Per device batch size: {self.per_device_batch_size}\")\n",
    "        print(f\"Gradient accumulation steps: {self.gradient_accumulation_steps}\")\n",
    "        print(f\"Generations per prompt: {self.generations_per_prompt}\")\n",
    "        print(f\"Per device prompt batch size: {self.per_device_prompt_batch_size}\")\n",
    "        print(f\"-----------------------------------\\n\")\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(cfg.base_model).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg.base_model)\n",
    "        self.ref_model = AutoModelForCausalLM.from_pretrained(cfg.base_model).to(device)\n",
    "        for param in self.ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=float(cfg.lr),\n",
    "            betas=cfg.get('betas', (0.9, 0.999)),\n",
    "            weight_decay=cfg.get('weight_decay', 1e-2))\n",
    "\n",
    "        # dataset\n",
    "        if cfg.dataset == \"gsm8k\":\n",
    "            train_dataset = Dataset.load_from_disk(f\"../data/my_data/gsm8k/train\")\n",
    "            if cfg.dataset_size is not None:\n",
    "                train_dataset = train_dataset.select(range(cfg.dataset_size))\n",
    "            val_dataset = Dataset.load_from_disk(f\"../data/my_data/gsm8k/test\")\n",
    "        self.train_loader = dataset_loader(train_dataset, self.per_device_prompt_batch_size, 0, 1, seed=cfg.get('seed', 0))\n",
    "        self.val_loader = dataset_loader(val_dataset, self.per_device_prompt_batch_size, 0, 1, seed=cfg.get('seed', 0))\n",
    "        print(f\"---DATASET CONFIG:\")\n",
    "        print(f\"Dataset: {cfg.dataset}\")\n",
    "        if cfg.dataset_size is not None:\n",
    "            print(f\"Careful! Reduced dataset size for testing: {len(train_dataset)}\")\n",
    "        print(f\"-----------------------------------\\n\")\n",
    "\n",
    "        # generation\n",
    "        self.loss_type = cfg.loss.loss_type\n",
    "        if self.loss_type in [\"pg\", \"logp\"]:\n",
    "            self.temperature = cfg.generation.temperature\n",
    "            self.top_k = cfg.generation.top_k\n",
    "            self.max_steps = cfg.generation.max_steps\n",
    "            self.step_for_answer = cfg.generation.step_for_answer\n",
    "            self.inject_answer_prompt = cfg.generation.inject_answer_prompt\n",
    "            self.as_full_distribution = cfg.generation.as_full_distribution\n",
    "            self.dot_by_dot = cfg.generation.dot_by_dot\n",
    "            self.answer_prompt_text = \"Answer:\"\n",
    "            self.answer_prompt_ids = self.tokenizer.encode(self.answer_prompt_text)\n",
    "            assert len(self.tokenizer.encode(\"....\")) == 1\n",
    "            self.dot_by_dot_id = self.tokenizer.encode(\"....\")[0]\n",
    "            print(f\"---GENERATION CONFIG:\")\n",
    "            print(f\"Temperature: {self.temperature}\")\n",
    "            print(f\"Top k: {self.top_k}\")\n",
    "            print(f\"Max length: {self.max_steps}\")\n",
    "            print(f\"Step for answer: {self.step_for_answer}\")\n",
    "            print(f\"Inject answer prompt: {self.inject_answer_prompt}\")\n",
    "            print(f\"As full distribution: {self.as_full_distribution}\")\n",
    "            print(f\"Answer prompt text: {self.answer_prompt_text}, ids: {self.answer_prompt_ids}\")\n",
    "            print(f\"-----------------------------------\\n\")\n",
    "\n",
    "        # loss\n",
    "        if self.loss_type == \"sft\":\n",
    "            self.sft_include_cot = cfg.loss.sft_include_cot\n",
    "            self.sft_predict_cot = cfg.loss.sft_predict_cot\n",
    "        elif self.loss_type == \"pg\":\n",
    "            self.pg_normalization_type = None if self.generations_per_prompt == 1 else cfg.loss.pg_normalization_type\n",
    "        self.entropy_coef = cfg.loss.entropy_coef\n",
    "        self.kl_loss_coef = cfg.loss.kl_loss_coef\n",
    "        print(f\"---LOSS CONFIG:\")\n",
    "        print(f\"Loss type: {self.loss_type}\")\n",
    "        if self.loss_type == \"sft\":\n",
    "            print(f\"SFT include cot: {self.sft_include_cot}\")\n",
    "            print(f\"SFT predict cot: {self.sft_predict_cot}\")\n",
    "        elif self.loss_type == \"pg\":\n",
    "            print(f\"PG normalization type: {self.pg_normalization_type}\")\n",
    "        print(f\"Entropy coef: {self.entropy_coef}\")\n",
    "        print(f\"KL loss coef: {self.kl_loss_coef}\")\n",
    "        print(f\"-----------------------------------\\n\")\n",
    "\n",
    "        self.ctx = self._setup_ctx()\n",
    "\n",
    "    def _setup_ctx(self):\n",
    "        \"\"\"Get the context manager\"\"\"\n",
    "        dtype = (\n",
    "            torch.bfloat16\n",
    "            if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "            else torch.float16\n",
    "        )\n",
    "        self._setup_scaler(dtype)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        ctx = torch.amp.autocast(device_type=\"cuda\", dtype=dtype)\n",
    "        return ctx\n",
    "\n",
    "    def _setup_scaler(self, dtype=torch.float16):\n",
    "        \"\"\"Setup the scaler\"\"\"\n",
    "        self.scaler = torch.amp.GradScaler(enabled=dtype == torch.float16, device='cuda')\n",
    "\n",
    "    def apply_update(self):\n",
    "        grad_clip = 1.0\n",
    "        # once gradients are accumulated, step \n",
    "        if grad_clip > 0:\n",
    "            # Unscale the gradients of the optimizer's assigned params in-place\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            # Clip the gradients with normalization\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n",
    "        # Perform a single optimization step\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        self.optimizer.zero_grad()  # Reset gradients after update\n",
    "\n",
    "\n",
    "    def run_training_loop(self, num_iters=None):\n",
    "        \"\"\"Run the training loop\"\"\"\n",
    "        start_time = time.time()\n",
    "        num_iters = self.total_iters if num_iters is None else num_iters\n",
    "        for i in tqdm.tqdm(range(num_iters), desc=\"Training\"):\n",
    "            # GRADIENT ACCUMULATION\n",
    "            for j in range(self.gradient_accumulation_steps):\n",
    "            \n",
    "                # GENERATE ROLLOUTS\n",
    "                start_time = time.time()\n",
    "                self.model.eval()\n",
    "                # with torch.no_grad():\n",
    "                # get questions (and answers)\n",
    "                dataset_batch = next(self.train_loader)\n",
    "                questions_text = dataset_batch[\"question\"]\n",
    "                # cot_text = dataset_batch[\"reasoning\"]\n",
    "                answers_text = dataset_batch[\"answer\"]\n",
    "\n",
    "                if self.loss_type == \"sft\":\n",
    "                    raise NotImplementedError\n",
    "                \n",
    "                else:\n",
    "                    questions_inputs = self.tokenizer(questions_text, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "                    answers_inputs = self.tokenizer(answers_text, return_tensors=\"pt\", padding=True)\n",
    "                    questions_inputs = {k: v.to(self.model.device) for k, v in questions_inputs.items()}\n",
    "                    answers_inputs = {k: v.to(self.model.device) for k, v in answers_inputs.items()}\n",
    "\n",
    "                    # repeat for generations_per_prompt\n",
    "                    questions_inputs = careful_repeat(questions_inputs, self.generations_per_prompt)\n",
    "                    answers_inputs = careful_repeat(answers_inputs, self.generations_per_prompt)\n",
    "\n",
    "                    # generate\n",
    "                    x, generations, generation_metrics, _ = batch_generate_rnn(\n",
    "                        model=self.model,\n",
    "                        ref_model=self.ref_model,\n",
    "                        questions_inputs=questions_inputs,\n",
    "                        answers_inputs=answers_inputs,\n",
    "                        max_steps=self.max_steps,\n",
    "                        step_for_answer=self.step_for_answer,\n",
    "                        temperature=self.temperature,\n",
    "                        top_k=self.top_k,\n",
    "                        as_full_distribution=self.as_full_distribution,\n",
    "                        dot_by_dot=self.dot_by_dot,\n",
    "                        dot_by_dot_id=self.dot_by_dot_id,\n",
    "                        inject_answer_prompt=self.inject_answer_prompt,\n",
    "                        answer_prompt_ids=self.answer_prompt_ids,\n",
    "                    )\n",
    "                    generation_metrics = {f\"gen/{k}\": v for k, v in generation_metrics.items()}\n",
    "\n",
    "                    ### rewards\n",
    "                    rewards, normalized_rewards, reward_metrics = self.get_rewards(generations, answers_text)\n",
    "                    reward_metrics = {k if k == \"REWARD\" else f\"reward/{k}\": v for k, v in reward_metrics.items()}\n",
    "                    \n",
    "                # COMPUTE LOSS\n",
    "                with self.ctx: \n",
    "                    loss, loss_metrics = self.get_loss(x, normalized_rewards)\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                loss_metrics = {f\"loss/{k}\": v for k, v in loss_metrics.items()}\n",
    "\n",
    "                # UPDATE METRICS\n",
    "                if j == 0:\n",
    "                    metrics_s = {**generation_metrics, **loss_metrics, **reward_metrics}\n",
    "                else:\n",
    "                    metrics = {**generation_metrics, **loss_metrics, **reward_metrics}\n",
    "                    metrics_s = {k: v + metrics[k] for k, v in metrics_s.items()}\n",
    "\n",
    "            # UPDATE MODEL\n",
    "            self.apply_update()\n",
    "\n",
    "            # LOG\n",
    "            metrics_s = {k: v / self.gradient_accumulation_steps for k, v in metrics_s.items()}\n",
    "            param_metrics = get_model_param_stats(self.model, self.ref_model)\n",
    "            log_dict = {\"iter\": i, \"lr\": self.optimizer.param_groups[0][\"lr\"]}\n",
    "            log_dict.update(metrics_s)\n",
    "            log_dict.update({f\"params/{k}\": v for k, v in param_metrics.items()})\n",
    "            if self.use_wandb:\n",
    "                wandb.log(log_dict)\n",
    "            if i % 10 == 0 or i == self.max_iters - 1:\n",
    "                print(f\"({log_dict})\\n\\niter {i}: REWARD={log_dict['REWARD']:.2f}\")\n",
    "                num_to_print = 3\n",
    "                decoded = [self.tokenizer.decode(generations[k]) for k in range(num_to_print)]\n",
    "                for k in range(num_to_print):\n",
    "                    print(f\"  EXAMPLE {k}: (REWARD={rewards[k].item()}):\\nQUESTION: {questions_text[k//self.generations_per_prompt]}\\nGENERATION: {decoded[k]}\\nANSWER: {answers_text[k//self.generations_per_prompt]}\\n\", \"-\"*50, \"\\n\")\n",
    "\n",
    "            \n",
    "        print(f\"Training time: {time.time()-start_time:.1f}s\")\n",
    "            \n",
    "        # save the final model\n",
    "        self._save_model(i, final_model=True)\n",
    "\n",
    "    def train(self, seed=42, num_iters=None):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        # set seed\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        self.run_training_loop(num_iters=num_iters)\n",
    "\n",
    "    def get_rewards(self, generations, answers_text):\n",
    "        \"\"\"Get rewards\"\"\"\n",
    "        ### check for answer\n",
    "        contains_answer_prompt = torch.zeros((self.per_device_prompt_batch_size, self.generations_per_prompt), device=self.model.device)\n",
    "        contains_answer = torch.zeros((self.per_device_prompt_batch_size, self.generations_per_prompt), device=self.model.device)\n",
    "        generations = generations.reshape(self.per_device_prompt_batch_size, self.generations_per_prompt, -1)\n",
    "        for i in range(self.per_device_prompt_batch_size):\n",
    "            answer_text = answers_text[i]\n",
    "            decoded_generations = self.tokenizer.batch_decode(generations[i])\n",
    "            for j in range(self.generations_per_prompt):\n",
    "                decoded = decoded_generations[j]\n",
    "                contains_answer_prompt_ij = self.answer_prompt_text in decoded\n",
    "                if contains_answer_prompt_ij:\n",
    "                    contains_answer_ij = answer_text in decoded.split(self.answer_prompt_text)[1]\n",
    "                else:\n",
    "                    contains_answer_ij = False\n",
    "                contains_answer_prompt[i, j] = contains_answer_prompt_ij\n",
    "                contains_answer[i, j] = contains_answer_ij\n",
    "        \n",
    "        ### caluclate rewards\n",
    "        rewards = contains_answer.float()\n",
    "        if not self.inject_answer_prompt:\n",
    "            rewards += self.answer_prompt_coef * contains_answer_prompt.float()\n",
    "        if self.pg_normalization_type == \"grpo\":\n",
    "            normalized_rewards = (rewards - rewards.mean(1, keepdim=True)) / (rewards.std(1, keepdim=True) + 1e-6)\n",
    "        elif self.pg_normalization_type == \"rloo\":\n",
    "            group_sum = rewards.sum(1, keepdim=True)\n",
    "            normalized_rewards = (group_sum - rewards) / (self.generations_per_prompt - 1)\n",
    "        else:\n",
    "            normalized_rewards = rewards\n",
    "\n",
    "        metrics = {\n",
    "            \"REWARD\": rewards.mean().item(),\n",
    "            \"reward/reward_std\": rewards.std().item(),\n",
    "            \"reward/reward_std_within_q\": rewards.std(1).mean().item(),\n",
    "            \"reward/reward_std_between_q\": rewards.mean(1).std().item(),\n",
    "        }\n",
    "\n",
    "        rewards = rewards.reshape(self.per_device_batch_size)\n",
    "        normalized_rewards = normalized_rewards.reshape(self.per_device_batch_size)\n",
    "        return rewards, normalized_rewards, metrics\n",
    "    \n",
    "    def get_loss(self, x, rewards):\n",
    "        metrics = {}\n",
    "        pg_loss, logp_loss = 0, 0\n",
    "        if self.loss_type == \"sft\":\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            answer_logps, _, gen_per_token_logps, ref_per_token_logps, entropy = x\n",
    "            if self.loss_type == \"pg\":\n",
    "                pg_loss = - torch.exp(gen_per_token_logps - gen_per_token_logps.detach()).mean(-1) * rewards\n",
    "            elif self.loss_type == \"logp\":\n",
    "                logp_loss = - answer_logps\n",
    "            else:\n",
    "                raise ValueError(f\"{self.loss_type=}\")\n",
    "            kl = (torch.exp(ref_per_token_logps - gen_per_token_logps) - (ref_per_token_logps - gen_per_token_logps) - 1).mean(-1)\n",
    "            loss = pg_loss + logp_loss + self.kl_loss_coef * kl - self.entropy_coef * entropy\n",
    "            loss = loss.mean()\n",
    "\n",
    "            metrics[\"loss\"] = loss.item()\n",
    "            metrics[\"pg_loss\"] = pg_loss.mean().item() if self.loss_type == \"pg\" else 0\n",
    "            metrics[\"logp_loss\"] = logp_loss.mean().item() if self.loss_type == \"logp\" else 0\n",
    "            metrics[\"kl\"] = kl.mean().item()\n",
    "            metrics[\"entropy\"] = entropy.mean().item()\n",
    "        return loss / self.gradient_accumulation_steps, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'dataset': 'gsm8k', 'base_model': 'Qwen/Qwen2.5-0.5b', 'use_wandb': False, 'run_name_prefix': 'RUNS0', 'wandb_project': 'coconut', 'max_iters': 3000, 'total_batch_size': 256, 'per_device_batch_size': 2, 'lr': '1e-6', 'dataset_size': 300, 'seed': 0, 'loss': {'loss_type': 'pg', 'sft_include_cot': True, 'sft_predict_cot': True, 'pg_normalization_type': 'grpo', 'answer_prompt_coef': 0.1, 'entropy_coef': 0.001, 'kl_loss_coef': 0.001}, 'generation': {'generations_per_prompt': 2, 'temperature': 1.0, 'top_k': None, 'max_steps': 30, 'step_for_answer': 20, 'inject_answer_prompt': False, 'as_full_distribution': False, 'dot_by_dot': False}}\n"
     ]
    }
   ],
   "source": [
    "config_file = \"../args/full_test.yaml\"\n",
    "\n",
    "### Load config\n",
    "with open(config_file) as f:\n",
    "        config_dict = yaml.safe_load(f)\n",
    "print(\"Config:\", config_dict)\n",
    "config = OmegaConf.create(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainer::-----------------------------------\n",
      "---TRAINING CONFIG:\n",
      "Max iters: 3000\n",
      "Total batch size: 256\n",
      "Per device batch size: 2\n",
      "Gradient accumulation steps: 128\n",
      "Generations per prompt: 2\n",
      "Per device prompt batch size: 1\n",
      "-----------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such files: '/scratch/local/homes/80/anya/Documents/llm_tiny_ideas/coconut-outer/coconut/notebooks/../data/my_data/gsm8k/train.bin/dataset_info.json', nor '/scratch/local/homes/80/anya/Documents/llm_tiny_ideas/coconut-outer/coconut/notebooks/../data/my_data/gsm8k/train.bin/state.json' found. Expected to load a `Dataset` object but provided path is not a `Dataset`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(config)\n",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# dataset\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgsm8k\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 50\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mload_from_disk(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/my_data/gsm8k/train.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mdataset_size))\n",
      "File \u001b[0;32m~/anaconda3/envs/coconut-1/lib/python3.12/site-packages/datasets/arrow_dataset.py:1656\u001b[0m, in \u001b[0;36mDataset.load_from_disk\u001b[0;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   1652\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dataset_dict_is_file:\n\u001b[1;32m   1653\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1654\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such files: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_info_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, nor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_state_json_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m found. Expected to load a `Dataset` object, but got a `DatasetDict`. Please use either `datasets.load_from_disk` or `DatasetDict.load_from_disk` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1655\u001b[0m         )\n\u001b[0;32m-> 1656\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1657\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such files: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_info_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, nor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_state_json_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m found. Expected to load a `Dataset` object but provided path is not a `Dataset`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1658\u001b[0m     )\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dataset_info_is_file:\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dataset_dict_is_file:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such files: '/scratch/local/homes/80/anya/Documents/llm_tiny_ideas/coconut-outer/coconut/notebooks/../data/my_data/gsm8k/train.bin/dataset_info.json', nor '/scratch/local/homes/80/anya/Documents/llm_tiny_ideas/coconut-outer/coconut/notebooks/../data/my_data/gsm8k/train.bin/state.json' found. Expected to load a `Dataset` object but provided path is not a `Dataset`."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 1/20 [00:03<01:11,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'iter': 0, 'lr': 1e-06, 'gen/answer_logps': 27.815303802490234, 'gen/answer_logps_ref': 27.700419425964355, 'gen/answer_logps_diff': 0.1148838996887207, 'gen/answer_perplexity': 3.281034310020914e-06, 'gen/answer_perplexity_ref': 2.421303520350193e-06, 'gen/answer_perplexity_diff': 8.597305622970453e-07, 'gen/logps': 22.891145706176758, 'gen/logps_ref': 21.601008415222168, 'gen/logps_diff': 1.2901363372802734, 'gen/entropy': 0.9073592126369476, 'gen/entropy_std': 0.29626742005348206, 'loss/loss': 0.00010825303616002202, 'loss/pg_loss': 0.0, 'loss/logp_loss': 0.0, 'loss/kl': 1.0156123042106628, 'loss/entropy': 0.9073592126369476, 'REWARD': 0.0, 'reward/reward/reward_std': 0.0, 'reward/reward/reward_std_within_q': 0.0, 'reward/reward/reward_std_between_q': 0.0, 'params/params_with_grads_mean': 0.00015669445565436035, 'params/params_with_grads_std': 0.03906893730163574, 'params/distance_to_ref': 6.772513322719775e-13})\n",
      "\n",
      "iter 0: REWARD=0.00\n",
      "  EXAMPLE 0: (REWARD=0.0):\n",
      "QUESTION: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Let's think step by step and output the final answer after \"Answer:\".\n",
      "GENERATION:  First, we need to determine how many clips Natalia sold in May. Since she sold half asAnswer: half the number of clips she sold in April\n",
      "ANSWER: 72\n",
      " -------------------------------------------------- \n",
      "\n",
      "  EXAMPLE 1: (REWARD=0.0):\n",
      "QUESTION: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Let's think step by step and output the final answer after \"Answer:\".\n",
      "GENERATION:  First, we need to find out how many clips Natalia sold in May. She sold half asAnswer: 24.<|endoftext|>Build a JavaScript function\n",
      "ANSWER: 72\n",
      " -------------------------------------------------- \n",
      "\n",
      "  EXAMPLE 2: (REWARD=0.0):\n",
      "QUESTION: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? Let's think step by step and output the final answer after \"Answer:\".\n",
      "GENERATION:  Weng earns $12 for every hour of babysitting and she did the job for 5Answer: 50 minutes.\n",
      "minutes. If we\n",
      "ANSWER: 10\n",
      " -------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 11/20 [00:42<00:34,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'iter': 10, 'lr': 1e-06, 'gen/answer_logps': 27.279375076293945, 'gen/answer_logps_ref': 27.962377548217773, 'gen/answer_logps_diff': -0.6830031871795654, 'gen/answer_perplexity': 4.188140906080662e-06, 'gen/answer_perplexity_ref': 3.7540717130468693e-06, 'gen/answer_perplexity_diff': 4.340692498772114e-07, 'gen/logps': 20.480884552001953, 'gen/logps_ref': 20.04889678955078, 'gen/logps_diff': 0.43198806047439575, 'gen/entropy': 1.7025251388549805, 'gen/entropy_std': 1.1711174845695496, 'loss/loss': -0.0006596555758733302, 'loss/pg_loss': 0.0, 'loss/logp_loss': 0.0, 'loss/kl': 1.0428695976734161, 'loss/entropy': 1.7025251388549805, 'REWARD': 0.0, 'reward/reward/reward_std': 0.0, 'reward/reward/reward_std_within_q': 0.0, 'reward/reward/reward_std_between_q': 0.0, 'params/params_with_grads_mean': 0.00015669455751776695, 'params/params_with_grads_std': 0.03906894102692604, 'params/distance_to_ref': 1.3103124488211826e-11})\n",
      "\n",
      "iter 10: REWARD=0.00\n",
      "  EXAMPLE 0: (REWARD=0.0):\n",
      "QUESTION: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Let's think step by step and output the final answer after \"Answer:\".\n",
      "GENERATION:  In April, Natalia sold clips to 48 friends.\n",
      "In May, she sold half asAnswer: 24) as many clips as she\n",
      "ANSWER: 72\n",
      " -------------------------------------------------- \n",
      "\n",
      "  EXAMPLE 1: (REWARD=0.0):\n",
      "QUESTION: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Let's think step by step and output the final answer after \"Answer:\".\n",
      "GENERATION:  To determine the total number of clips Natalia sold in April and May, we follow these steps:\n",
      "\n",
      "Answer: She sold 16 clips in April.\n",
      "\n",
      "\n",
      "ANSWER: 72\n",
      " -------------------------------------------------- \n",
      "\n",
      "  EXAMPLE 2: (REWARD=0.0):\n",
      "QUESTION: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? Let's think step by step and output the final answer after \"Answer:\".\n",
      "GENERATION:  To determine how much Wendy earned for babysitting, we need to perform some basic subtraction calculations:\n",
      "\n",
      "1Answer: \n",
      "\n",
      "Assistant: Wendy earned **$5.\n",
      "ANSWER: 10\n",
      " -------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [01:18<00:00,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'iter': 19, 'lr': 1e-06, 'gen/answer_logps': 27.143101692199707, 'gen/answer_logps_ref': 27.44046974182129, 'gen/answer_logps_diff': -0.29736852645874023, 'gen/answer_perplexity': 2.5355892603329266e-06, 'gen/answer_perplexity_ref': 3.5012234320674906e-06, 'gen/answer_perplexity_diff': -9.656342285779829e-07, 'gen/logps': 20.53018093109131, 'gen/logps_ref': 19.997366905212402, 'gen/logps_diff': 0.532814159989357, 'gen/entropy': 1.6038917303085327, 'gen/entropy_std': 0.5319118201732635, 'loss/loss': -0.0011174731771461666, 'loss/pg_loss': 0.0, 'loss/logp_loss': 0.0, 'loss/kl': 0.48641857504844666, 'loss/entropy': 1.6038917303085327, 'REWARD': 0.0, 'reward/reward/reward_std': 0.0, 'reward/reward/reward_std_within_q': 0.0, 'reward/reward/reward_std_between_q': 0.0, 'params/params_with_grads_mean': 0.00015669441199861467, 'params/params_with_grads_std': 0.03906893730163574, 'params/distance_to_ref': 2.054701027376371e-11})\n",
      "\n",
      "iter 19: REWARD=0.00\n",
      "  EXAMPLE 0: (REWARD=0.0):\n",
      "QUESTION: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Let's think step by step and output the final answer after \"Answer:\".\n",
      "GENERATION:  April: Natalia sold 48 clips to her friends. May: After selling half as manyAnswer: 24 clips.\n",
      "\n",
      "The steps to arrive\n",
      "ANSWER: 72\n",
      " -------------------------------------------------- \n",
      "\n",
      "  EXAMPLE 1: (REWARD=0.0):\n",
      "QUESTION: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Let's think step by step and output the final answer after \"Answer:\".\n",
      "GENERATION:  Assuming Natalia sold half as many clips in May as she did in April:\n",
      "May sales = Answer: .\n",
      "Round off the number of clips sold to\n",
      "ANSWER: 72\n",
      " -------------------------------------------------- \n",
      "\n",
      "  EXAMPLE 2: (REWARD=0.0):\n",
      "QUESTION: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? Let's think step by step and output the final answer after \"Answer:\".\n",
      "GENERATION:  A) $375, B) $45, C) $25, DAnswer: C Would you explain your steps? Total earned\n",
      "ANSWER: 10\n",
      " -------------------------------------------------- \n",
      "\n",
      "Training time: 2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute '_save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[77], line 233\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, seed, num_iters)\u001b[0m\n\u001b[1;32m    231\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m    232\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[77], line 225\u001b[0m, in \u001b[0;36mTrainer.run_training_loop\u001b[0;34m(self, num_iters)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# save the final model\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_model\u001b[49m(i, final_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute '_save_model'"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.train(seed=42, num_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_model = AutoModelForCausalLM.from_pretrained(config.base_model).to(device)\n",
    "dummy_ref_model = AutoModelForCausalLM.from_pretrained(config.base_model).to(device)\n",
    "dummy_tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "dummy_dataset = Dataset.load_from_disk(f\"../data/my_data/gsm8k/test\")\n",
    "dummy_loader = dataset_loader(dummy_dataset, per_device_batch_size=2, rank=0, world_size=1, seed=0)\n",
    "dummy_batch = next(dummy_loader)\n",
    "dummy_questions_text = dummy_batch[\"question\"]\n",
    "dummy_answers_text = dummy_batch[\"answer\"]\n",
    "dummy_questions_inputs = dummy_tokenizer(dummy_questions_text, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "dummy_answers_inputs = dummy_tokenizer(dummy_answers_text, return_tensors=\"pt\", padding=True)\n",
    "dummy_questions_inputs = {k: v.to(device) for k, v in dummy_questions_inputs.items()}\n",
    "dummy_answers_inputs = {k: v.to(device) for k, v in dummy_answers_inputs.items()}\n",
    "dummy_ref_model.eval()\n",
    "dummy_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2]) torch.Size([2, 31]) {'answer_logps': 13.639263153076172, 'answer_logps_ref': 13.970451354980469, 'answer_logps_diff': -0.3311886787414551, 'answer_perplexity': 0.002386221196502447, 'answer_perplexity_ref': 0.0011142903240397573, 'answer_perplexity_diff': 0.0012719309888780117, 'logps': 23.325300216674805, 'logps_ref': 20.66714096069336, 'logps_diff': 2.6581602096557617, 'entropy': 0.939110279083252, 'entropy_std': 0.0955267995595932}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\" First, we need to calculate the total number of eggs laid by the ducks in a day. Since Janet's ducks lay 16 eggs per day,\",\n",
       " ' To find the total number of bolts needed, we need to calculate the number of bolts required for blue and white fibers separately and then add them together.\\n\\n1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, generations, metrics, past_key_values = batch_generate_rnn(\n",
    "    model=dummy_model,\n",
    "    ref_model=dummy_ref_model,\n",
    "    questions_inputs=dummy_questions_inputs,\n",
    "    answers_inputs=dummy_answers_inputs,\n",
    "    max_steps=30,\n",
    "    step_for_answer=20,\n",
    "    temperature=0.0,\n",
    "    top_k=None,\n",
    "    as_full_distribution=False,\n",
    "    dot_by_dot=False,\n",
    "    dot_by_dot_id=dummy_tokenizer.encode(\"....\")[0],\n",
    "    inject_answer_prompt=False,\n",
    "    answer_prompt_ids=dummy_tokenizer.encode(\"...Answer:\"),\n",
    ")\n",
    "print(x[0].shape, generations.shape, metrics)\n",
    "dummy_tokenizer.batch_decode(generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" First, we need to calculate the total number of eggs laid by the ducks in a day. Since Janet's ducks lay 16 eggs per day\", ' To find the total number of bolts needed, we need to calculate the number of bolts required for blue and white fibers separately and then add them together.\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "gen_out = dummy_model.generate(\n",
    "    input_ids=dummy_questions_inputs[\"input_ids\"],\n",
    "    attention_mask=dummy_questions_inputs[\"attention_mask\"],\n",
    "    max_new_tokens=30,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    do_sample=False,\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "prompt_length = dummy_questions_inputs[\"input_ids\"].shape[1]\n",
    "generations = gen_out.sequences if isinstance(gen_out, dict) else gen_out\n",
    "generations = generations[:, prompt_length:]\n",
    "print(dummy_tokenizer.batch_decode(generations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=0: keys: [-8.410331726074219, -3.3071095943450928, -6.438131809234619, 0.6713922619819641, -0.20497480034828186]\n",
      "i=0: keys: [-8.410331726074219, -3.3071095943450928, -6.438131809234619, 0.6713922619819641, -0.20497480034828186]\n",
      "i=0: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=0: values: [-0.01414407417178154, 0.03389165550470352, -0.02601637691259384, 0.013034755364060402, -0.011828195303678513]\n",
      "i=0: values: [-0.01414407417178154, 0.03389165550470352, -0.02601637691259384, 0.013034755364060402, -0.011828195303678513]\n",
      "i=1: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=1: keys: [-9.862237930297852, -7.930609703063965, -7.341488361358643, 2.996467113494873, -0.14621633291244507]\n",
      "i=1: keys: [-9.862237930297852, -7.930609703063965, -7.341488361358643, 2.996467113494873, -0.14621633291244507]\n",
      "i=1: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=1: values: [0.010026691481471062, -0.028028618544340134, -0.011757634580135345, -0.024341082200407982, 0.01379705872386694]\n",
      "i=1: values: [0.010026691481471062, -0.028028618544340134, -0.011757634580135345, -0.024341082200407982, 0.01379705872386694]\n",
      "i=2: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=2: keys: [-2.1348609924316406, -9.202946662902832, -6.998022079467773, 4.451528549194336, 0.030788451433181763]\n",
      "i=2: keys: [-2.1348609924316406, -9.202946662902832, -6.998022079467773, 4.451528549194336, 0.030788451433181763]\n",
      "i=2: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=2: values: [0.030848845839500427, 0.02934252843260765, 0.00014420831575989723, -0.0016999400686472654, 0.003674456849694252]\n",
      "i=2: values: [0.030848845839500427, 0.02934252843260765, 0.00014420831575989723, -0.0016999400686472654, 0.003674456849694252]\n",
      "i=3: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=3: keys: [7.34650182723999, -7.3133225440979, -5.728171348571777, 5.3970136642456055, -0.17883673310279846]\n",
      "i=3: keys: [7.34650182723999, -7.3133225440979, -5.728171348571777, 5.3970136642456055, -0.17883673310279846]\n",
      "i=3: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=3: values: [0.002477455884218216, 0.021877046674489975, -0.018245941027998924, -0.015633821487426758, -0.0057307155802845955]\n",
      "i=3: values: [0.002477455884218216, 0.021877046674489975, -0.018245941027998924, -0.015633821487426758, -0.0057307155802845955]\n",
      "i=4: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=4: keys: [10.617332458496094, -1.936751365661621, -3.1306264400482178, 6.344401836395264, -0.15504883229732513]\n",
      "i=4: keys: [10.617332458496094, -1.936751365661621, -3.1306264400482178, 6.344401836395264, -0.15504883229732513]\n",
      "i=4: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=4: values: [0.004640286788344383, 0.012723531574010849, 0.010242493823170662, -0.017828021198511124, -0.0003912129905074835]\n",
      "i=4: values: [0.004640286788344383, 0.012723531574010849, 0.010242493823170662, -0.017828021198511124, -0.0003912129905074835]\n",
      "i=5: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=5: keys: [3.2649037837982178, 4.938666343688965, 0.5014722347259521, 7.364436149597168, 0.24105006456375122]\n",
      "i=5: keys: [3.2649037837982178, 4.938666343688965, 0.5014722347259521, 7.364436149597168, 0.24105006456375122]\n",
      "i=5: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=5: values: [-0.042069096118211746, -0.00618927925825119, 0.046420320868492126, 0.01419975608587265, -0.005290590226650238]\n",
      "i=5: values: [-0.042069096118211746, -0.00618927925825119, 0.046420320868492126, 0.01419975608587265, -0.005290590226650238]\n",
      "i=6: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=6: keys: [-5.913851261138916, 9.0438232421875, 4.00195837020874, 7.749184608459473, -0.1538846492767334]\n",
      "i=6: keys: [-5.913851261138916, 9.0438232421875, 4.00195837020874, 7.749184608459473, -0.1538846492767334]\n",
      "i=6: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=6: values: [-0.008110582828521729, -0.011999238282442093, 0.011179588735103607, -0.018040049821138382, 0.00447726808488369]\n",
      "i=6: values: [-0.008110582828521729, -0.011999238282442093, 0.011179588735103607, -0.018040049821138382, 0.00447726808488369]\n",
      "i=7: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=7: keys: [-10.08230209350586, 9.513843536376953, 6.560704708099365, 6.9967169761657715, -0.12185978144407272]\n",
      "i=7: keys: [-10.08230209350586, 9.513843536376953, 6.560704708099365, 6.9967169761657715, -0.12185978144407272]\n",
      "i=7: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=7: values: [-0.0032774657011032104, 0.004580378532409668, 0.005415185820311308, -0.009025564417243004, 0.000703057274222374]\n",
      "i=7: values: [-0.0032774657011032104, 0.004580378532409668, 0.005415185820311308, -0.009025564417243004, 0.000703057274222374]\n",
      "i=8: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=8: keys: [-5.636849880218506, 6.604637622833252, 6.912400245666504, 4.8306565284729, -0.016098424792289734]\n",
      "i=8: keys: [-5.636849880218506, 6.604637622833252, 6.912400245666504, 4.8306565284729, -0.016098424792289734]\n",
      "i=8: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=8: values: [-0.01029355451464653, 0.0011192169040441513, 0.007892685942351818, -0.004882362671196461, -0.020949238911271095]\n",
      "i=8: values: [-0.01029355451464653, 0.0011192169040441513, 0.007892685942351818, -0.004882362671196461, -0.020949238911271095]\n",
      "i=9: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=9: keys: [5.383584022521973, 0.7549359798431396, 7.463035583496094, 3.7670156955718994, 0.10842078924179077]\n",
      "i=9: keys: [5.383584022521973, 0.7549359798431396, 7.463035583496094, 3.7670156955718994, 0.10842078924179077]\n",
      "i=9: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=9: values: [0.01296982727944851, -0.03826059773564339, -0.008268261328339577, -0.0009136036969721317, 0.007700623944401741]\n",
      "i=9: values: [0.01296982727944851, -0.03826059773564339, -0.008268261328339577, -0.0009136036969721317, 0.007700623944401741]\n",
      "i=10: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=10: keys: [10.76364803314209, -4.919989109039307, 6.371460914611816, 1.8750680685043335, 0.0540572926402092]\n",
      "i=10: keys: [10.76364803314209, -4.919989109039307, 6.371460914611816, 1.8750680685043335, 0.0540572926402092]\n",
      "i=10: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=10: values: [-0.019065814092755318, -0.024128355085849762, -0.0009186333045363426, 0.00721325259655714, 0.00019186828285455704]\n",
      "i=10: values: [-0.019065814092755318, -0.024128355085849762, -0.0009186333045363426, 0.00721325259655714, 0.00019186828285455704]\n",
      "i=11: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=11: keys: [6.114045143127441, -9.139617919921875, 3.7421822547912598, 0.1714153289794922, 0.2844187021255493]\n",
      "i=11: keys: [6.114045143127441, -9.139617919921875, 3.7421822547912598, 0.1714153289794922, 0.2844187021255493]\n",
      "i=11: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=11: values: [-0.011283719912171364, -0.018521035090088844, 0.008460381999611855, -0.007884752936661243, 0.04855034500360489]\n",
      "i=11: values: [-0.011283719912171364, -0.018521035090088844, 0.008460381999611855, -0.007884752936661243, 0.04855034500360489]\n",
      "i=12: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=12: keys: [-3.893583297729492, -9.283998489379883, 1.6449437141418457, -1.6225931644439697, 0.16541486978530884]\n",
      "i=12: keys: [-3.893583297729492, -9.283998489379883, 1.6449437141418457, -1.6225931644439697, 0.16541486978530884]\n",
      "i=12: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=12: values: [0.02473018877208233, 0.03744085133075714, -0.008871842175722122, 0.002501990646123886, -0.029923565685749054]\n",
      "i=12: values: [0.02473018877208233, 0.03744085133075714, -0.008871842175722122, 0.002501990646123886, -0.029923565685749054]\n",
      "i=13: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=13: keys: [-10.539017677307129, -5.887823581695557, -1.6294105052947998, -3.3780946731567383, 0.2254360467195511]\n",
      "i=13: keys: [-10.539017677307129, -5.887823581695557, -1.6294105052947998, -3.3780946731567383, 0.2254360467195511]\n",
      "i=13: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=13: values: [0.015992652624845505, 0.018543202430009842, -0.01757153868675232, 0.003950626123696566, -0.00781946536153555]\n",
      "i=13: values: [0.015992652624845505, 0.018543202430009842, -0.01757153868675232, 0.003950626123696566, -0.00781946536153555]\n",
      "i=14: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=14: keys: [-7.820744514465332, 0.14271235466003418, -4.662927627563477, -5.031116008758545, 0.2962135970592499]\n",
      "i=14: keys: [-7.820744514465332, 0.14271235466003418, -4.662927627563477, -5.031116008758545, 0.2962135970592499]\n",
      "i=14: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=14: values: [0.0041368198581039906, -0.007348416373133659, 0.01807408779859543, 0.0114622563123703, 0.026213008910417557]\n",
      "i=14: values: [0.0041368198581039906, -0.007348416373133659, 0.01807408779859543, 0.0114622563123703, 0.026213008910417557]\n",
      "i=15: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=15: keys: [2.7010245323181152, 6.356115818023682, -6.676074981689453, -6.428705215454102, 0.0827341079711914]\n",
      "i=15: keys: [2.7010245323181152, 6.356115818023682, -6.676074981689453, -6.428705215454102, 0.0827341079711914]\n",
      "i=15: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=15: values: [5.544046871364117e-05, -0.007082164287567139, 0.0058471886441111565, 0.02311376854777336, -0.00472942553460598]\n",
      "i=15: values: [5.544046871364117e-05, -0.007082164287567139, 0.0058471886441111565, 0.02311376854777336, -0.00472942553460598]\n",
      "i=16: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=16: keys: [10.206550598144531, 9.196064949035645, -7.231804370880127, -6.415915012359619, 0.3072962760925293]\n",
      "i=16: keys: [10.206550598144531, 9.196064949035645, -7.231804370880127, -6.415915012359619, 0.3072962760925293]\n",
      "i=16: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=16: values: [-0.008147723972797394, -0.00758055504411459, -0.008460259065032005, 0.005298575386404991, -0.019745834171772003]\n",
      "i=16: values: [-0.008147723972797394, -0.00758055504411459, -0.008460259065032005, 0.005298575386404991, -0.019745834171772003]\n",
      "i=17: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=17: keys: [8.561214447021484, 8.47903060913086, -6.914931297302246, -6.858955383300781, 0.1709655523300171]\n",
      "i=17: keys: [8.561214447021484, 8.47903060913086, -6.914931297302246, -6.858955383300781, 0.1709655523300171]\n",
      "i=17: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=17: values: [-0.0008176745614036918, -0.013975793495774269, -0.004544749855995178, 0.012583652511239052, 0.030712490901350975]\n",
      "i=17: values: [-0.0008176745614036918, -0.013975793495774269, -0.004544749855995178, 0.012583652511239052, 0.030712490901350975]\n",
      "i=18: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=18: keys: [-0.34474802017211914, 5.0150837898254395, -5.3558454513549805, -6.263848781585693, 0.25258681178092957]\n",
      "i=18: keys: [-0.34474802017211914, 5.0150837898254395, -5.3558454513549805, -6.263848781585693, 0.25258681178092957]\n",
      "i=18: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=18: values: [-0.008456969633698463, -0.0015415162779390812, -0.019762719050049782, 0.023012103512883186, -0.032883964478969574]\n",
      "i=18: values: [-0.008456969633698463, -0.0015415162779390812, -0.019762719050049782, 0.023012103512883186, -0.032883964478969574]\n",
      "i=19: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=19: keys: [-9.801507949829102, -1.9160646200180054, -2.487842082977295, -6.0357890129089355, -0.04806207865476608]\n",
      "i=19: keys: [-9.801507949829102, -1.9160646200180054, -2.487842082977295, -6.0357890129089355, -0.04806207865476608]\n",
      "i=19: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=19: values: [0.011901721358299255, -0.0010180436074733734, 0.037717305123806, 0.0018865643069148064, 0.02024102956056595]\n",
      "i=19: values: [0.011901721358299255, -0.0010180436074733734, 0.037717305123806, 0.0018865643069148064, 0.02024102956056595]\n",
      "i=20: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=20: keys: [-9.76379680633545, -6.717892646789551, 0.436084508895874, -4.6602325439453125, 0.15011417865753174]\n",
      "i=20: keys: [-9.76379680633545, -6.717892646789551, 0.436084508895874, -4.6602325439453125, 0.15011417865753174]\n",
      "i=20: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=20: values: [-0.006898010615259409, -0.03339320048689842, 0.01888817548751831, -0.008966738358139992, 0.0008241385221481323]\n",
      "i=20: values: [-0.006898010615259409, -0.03339320048689842, 0.01888817548751831, -0.008966738358139992, 0.0008241385221481323]\n",
      "i=21: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=21: keys: [-1.3412623405456543, -9.111162185668945, 3.2505269050598145, -2.261319398880005, 0.26684117317199707]\n",
      "i=21: keys: [-1.3412623405456543, -9.111162185668945, 3.2505269050598145, -2.261319398880005, 0.26684117317199707]\n",
      "i=21: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=21: values: [-0.003241986967623234, -0.034855276346206665, -0.022579079493880272, 0.0033591624815016985, -0.013552755117416382]\n",
      "i=21: values: [-0.003241986967623234, -0.034855276346206665, -0.022579079493880272, 0.0033591624815016985, -0.013552755117416382]\n",
      "i=22: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=22: keys: [8.570951461791992, -8.631032943725586, 5.589956283569336, -0.9643257260322571, 0.26142603158950806]\n",
      "i=22: keys: [8.570951461791992, -8.631032943725586, 5.589956283569336, -0.9643257260322571, 0.26142603158950806]\n",
      "i=22: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=22: values: [-0.012194273993372917, -0.0007088826969265938, -0.01202918216586113, -0.007929242216050625, -0.013505207374691963]\n",
      "i=22: values: [-0.012194273993372917, -0.0007088826969265938, -0.01202918216586113, -0.007929242216050625, -0.013505207374691963]\n",
      "i=23: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=23: keys: [9.992849349975586, -3.6419899463653564, 7.1333394050598145, 1.2360765933990479, 0.18886756896972656]\n",
      "i=23: keys: [9.992849349975586, -3.6419899463653564, 7.1333394050598145, 1.2360765933990479, 0.18886756896972656]\n",
      "i=23: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=23: values: [0.023640548810362816, -0.017861835658550262, 4.3863896280527115e-05, -0.031206579878926277, -0.0037811603397130966]\n",
      "i=23: values: [0.023640548810362816, -0.017861835658550262, 4.3863896280527115e-05, -0.031206579878926277, -0.0037811603397130966]\n",
      "i=24: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=24: keys: [1.8719801902770996, 2.7793169021606445, 7.292719841003418, 2.837252616882324, -0.23204366862773895]\n",
      "i=24: keys: [1.8719801902770996, 2.7793169021606445, 7.292719841003418, 2.837252616882324, -0.23204366862773895]\n",
      "i=24: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=24: values: [5.544046871364117e-05, -0.007082164287567139, 0.0058471886441111565, 0.02311376854777336, -0.00472942553460598]\n",
      "i=24: values: [5.544046871364117e-05, -0.007082164287567139, 0.0058471886441111565, 0.02311376854777336, -0.00472942553460598]\n",
      "i=25: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=25: keys: [-7.699404716491699, 7.360507965087891, 6.321046829223633, 4.4714884757995605, -0.09881500899791718]\n",
      "i=25: keys: [-7.699404716491699, 7.360507965087891, 6.321046829223633, 4.4714884757995605, -0.09881500899791718]\n",
      "i=25: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=25: values: [0.014443589374423027, 0.030216634273529053, 0.0031355037353932858, -0.0187484510242939, -0.018295546993613243]\n",
      "i=25: values: [0.014443589374423027, 0.030216634273529053, 0.0031355037353932858, -0.0187484510242939, -0.018295546993613243]\n",
      "i=26: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=26: keys: [-10.565593719482422, 9.38346004486084, 4.0245137214660645, 5.664787292480469, -0.002253003418445587]\n",
      "i=26: keys: [-10.565593719482422, 9.38346004486084, 4.0245137214660645, 5.664787292480469, -0.002253003418445587]\n",
      "i=26: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=26: values: [-0.006095895543694496, -0.021186478435993195, -0.004267483949661255, 0.0016476064920425415, 0.008377866819500923]\n",
      "i=26: values: [-0.006095895543694496, -0.021186478435993195, -0.004267483949661255, 0.0016476064920425415, 0.008377866819500923]\n",
      "i=27: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=27: keys: [-3.6669294834136963, 7.493816375732422, 0.8860242366790771, 6.472823143005371, -0.18462860584259033]\n",
      "i=27: keys: [-3.6669294834136963, 7.493816375732422, 0.8860242366790771, 6.472823143005371, -0.18462860584259033]\n",
      "i=27: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=27: values: [-0.0008176745614036918, -0.013975793495774269, -0.004544749855995178, 0.012583652511239052, 0.030712490901350975]\n",
      "i=27: values: [-0.0008176745614036918, -0.013975793495774269, -0.004544749855995178, 0.012583652511239052, 0.030712490901350975]\n",
      "i=28: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=28: keys: [6.263861179351807, 2.999758005142212, -2.149522542953491, 6.839357852935791, -0.06801121681928635]\n",
      "i=28: keys: [6.263861179351807, 2.999758005142212, -2.149522542953491, 6.839357852935791, -0.06801121681928635]\n",
      "i=28: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=28: values: [-0.019065814092755318, -0.024128355085849762, -0.0009186333045363426, 0.00721325259655714, 0.00019186828285455704]\n",
      "i=28: values: [-0.019065814092755318, -0.024128355085849762, -0.0009186333045363426, 0.00721325259655714, 0.00019186828285455704]\n",
      "i=29: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=29: keys: [10.800434112548828, -3.6058290004730225, -5.008935451507568, 7.135194778442383, -0.20270998775959015]\n",
      "i=29: keys: [10.800434112548828, -3.6058290004730225, -5.008935451507568, 7.135194778442383, -0.20270998775959015]\n",
      "i=29: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=29: values: [-0.007670124527066946, -0.002676788717508316, 0.013221466913819313, 0.01771147921681404, 0.02537507563829422]\n",
      "i=29: values: [-0.007670124527066946, -0.002676788717508316, 0.013221466913819313, 0.01771147921681404, 0.02537507563829422]\n",
      "i=30: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=30: keys: [5.287685871124268, -7.826976776123047, -6.823024272918701, 6.116852760314941, -0.2107202261686325]\n",
      "i=30: keys: [5.287685871124268, -7.826976776123047, -6.823024272918701, 6.116852760314941, -0.2107202261686325]\n",
      "i=30: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=30: values: [0.0008542498107999563, 0.000703323632478714, 0.02188391238451004, 0.010203968733549118, 0.029208598658442497]\n",
      "i=30: values: [0.0008542498107999563, 0.000703323632478714, 0.02188391238451004, 0.010203968733549118, 0.029208598658442497]\n",
      "i=31: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=31: keys: [-5.650319576263428, -9.321941375732422, -6.856749057769775, 5.129765033721924, -0.23763185739517212]\n",
      "i=31: keys: [-5.650319576263428, -9.321941375732422, -6.856749057769775, 5.129765033721924, -0.23763185739517212]\n",
      "i=31: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=31: values: [-0.011283719912171364, -0.018521035090088844, 0.008460381999611855, -0.007884752936661243, 0.04855034500360489]\n",
      "i=31: values: [-0.011283719912171364, -0.018521035090088844, 0.008460381999611855, -0.007884752936661243, 0.04855034500360489]\n",
      "i=32: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=32: keys: [-11.093330383300781, -7.370632171630859, -6.992436408996582, 3.6207547187805176, -0.17655760049819946]\n",
      "i=32: keys: [-11.093330383300781, -7.370632171630859, -6.992436408996582, 3.6207547187805176, -0.17655760049819946]\n",
      "i=32: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=32: values: [0.02473018877208233, 0.03744085133075714, -0.008871842175722122, 0.002501990646123886, -0.029923565685749054]\n",
      "i=32: values: [0.02473018877208233, 0.03744085133075714, -0.008871842175722122, 0.002501990646123886, -0.029923565685749054]\n",
      "i=33: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=33: keys: [-6.7979655265808105, -2.2112417221069336, -5.248003005981445, 1.5277867317199707, -0.23857241868972778]\n",
      "i=33: keys: [-6.7979655265808105, -2.2112417221069336, -5.248003005981445, 1.5277867317199707, -0.23857241868972778]\n",
      "i=33: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=33: values: [0.008172755129635334, 0.0076424479484558105, -0.008106786757707596, -0.00026241783052682877, -0.030109312385320663]\n",
      "i=33: values: [0.008172755129635334, 0.0076424479484558105, -0.008106786757707596, -0.00026241783052682877, -0.030109312385320663]\n",
      "i=34: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=34: keys: [4.389172554016113, 4.415837287902832, -2.1390321254730225, -0.27915042638778687, -0.12758475542068481]\n",
      "i=34: keys: [4.389172554016113, 4.415837287902832, -2.1390321254730225, -0.27915042638778687, -0.12758475542068481]\n",
      "i=34: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=34: values: [0.017570771276950836, 0.005604807287454605, 0.013234655372798443, 0.010662296786904335, -0.009113812819123268]\n",
      "i=34: values: [0.017570771276950836, 0.005604807287454605, 0.013234655372798443, 0.010662296786904335, -0.009113812819123268]\n",
      "i=35: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=35: keys: [10.581768989562988, 8.28633975982666, 0.43107128143310547, -1.897810459136963, -0.2551476061344147]\n",
      "i=35: keys: [10.581768989562988, 8.28633975982666, 0.43107128143310547, -1.897810459136963, -0.2551476061344147]\n",
      "i=35: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=35: values: [0.01064934954047203, -0.00154083501547575, -0.02095283567905426, -0.01078600063920021, -0.006621962413191795]\n",
      "i=35: values: [0.01064934954047203, -0.00154083501547575, -0.02095283567905426, -0.01078600063920021, -0.006621962413191795]\n",
      "i=36: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=36: keys: [7.261360168457031, 9.191596031188965, 3.886387825012207, -3.777884006500244, 0.020788194611668587]\n",
      "i=36: keys: [7.261360168457031, 9.191596031188965, 3.886387825012207, -3.777884006500244, 0.020788194611668587]\n",
      "i=36: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=36: values: [0.0009755732025951147, -0.003382263705134392, 0.02600182592868805, -0.01302700862288475, 0.02150927297770977]\n",
      "i=36: values: [0.0009755732025951147, -0.003382263705134392, 0.02600182592868805, -0.01302700862288475, 0.02150927297770977]\n",
      "i=37: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=37: keys: [-2.9623525142669678, 5.7455291748046875, 6.094707489013672, -5.428316593170166, 0.02157733589410782]\n",
      "i=37: keys: [-2.9623525142669678, 5.7455291748046875, 6.094707489013672, -5.428316593170166, 0.02157733589410782]\n",
      "i=37: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=37: values: [0.017570771276950836, 0.005604807287454605, 0.013234655372798443, 0.010662296786904335, -0.009113812819123268]\n",
      "i=37: values: [0.017570771276950836, 0.005604807287454605, 0.013234655372798443, 0.010662296786904335, -0.009113812819123268]\n",
      "i=38: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=38: keys: [-10.120409965515137, 1.2561898231506348, 7.079216480255127, -5.913948059082031, -0.17774774134159088]\n",
      "i=38: keys: [-10.120409965515137, 1.2561898231506348, 7.079216480255127, -5.913948059082031, -0.17774774134159088]\n",
      "i=38: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=38: values: [-0.017954550683498383, -0.005517502315342426, -0.002643323503434658, 0.007015876937657595, -0.024682514369487762]\n",
      "i=38: values: [-0.017954550683498383, -0.005517502315342426, -0.002643323503434658, 0.007015876937657595, -0.024682514369487762]\n",
      "i=39: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=39: keys: [-8.106515884399414, -5.616954326629639, 6.865612983703613, -6.718009948730469, 0.12201886624097824]\n",
      "i=39: keys: [-8.106515884399414, -5.616954326629639, 6.865612983703613, -6.718009948730469, 0.12201886624097824]\n",
      "i=39: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=39: values: [-0.021417859941720963, 0.002246396616101265, 0.04015553742647171, -0.0020157936960458755, 0.018230658024549484]\n",
      "i=39: values: [-0.021417859941720963, 0.002246396616101265, 0.04015553742647171, -0.0020157936960458755, 0.018230658024549484]\n",
      "i=40: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=40: keys: [0.38527679443359375, -8.646570205688477, 6.026155471801758, -6.655004978179932, -0.20907431840896606]\n",
      "i=40: keys: [0.38527679443359375, -8.646570205688477, 6.026155471801758, -6.655004978179932, -0.20907431840896606]\n",
      "i=40: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=40: values: [0.0018903408199548721, -0.0032282117754220963, 0.0008034915663301945, 0.0014986866153776646, -0.048255354166030884]\n",
      "i=40: values: [0.0018903408199548721, -0.0032282117754220963, 0.0008034915663301945, 0.0014986866153776646, -0.048255354166030884]\n",
      "i=41: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=41: keys: [9.596843719482422, -9.210138320922852, 3.7594141960144043, -6.405407905578613, -0.09722305834293365]\n",
      "i=41: keys: [9.596843719482422, -9.210138320922852, 3.7594141960144043, -6.405407905578613, -0.09722305834293365]\n",
      "i=41: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=41: values: [-0.004697942640632391, -0.02421974390745163, -0.0021466482430696487, 0.009137158282101154, -0.010163897648453712]\n",
      "i=41: values: [-0.004697942640632391, -0.02421974390745163, -0.0021466482430696487, 0.009137158282101154, -0.010163897648453712]\n",
      "i=42: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=42: keys: [9.266149520874023, -5.176468372344971, 0.49121856689453125, -5.96397590637207, 0.2361339032649994]\n",
      "i=42: keys: [9.266149520874023, -5.176468372344971, 0.49121856689453125, -5.96397590637207, 0.2361339032649994]\n",
      "i=42: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=42: values: [5.544046871364117e-05, -0.007082164287567139, 0.0058471886441111565, 0.02311376854777336, -0.00472942553460598]\n",
      "i=42: values: [5.544046871364117e-05, -0.007082164287567139, 0.0058471886441111565, 0.02311376854777336, -0.00472942553460598]\n",
      "i=43: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=43: keys: [0.49207401275634766, 0.2997119426727295, -2.383255958557129, -4.302631855010986, 0.32093676924705505]\n",
      "i=43: keys: [0.49207401275634766, 0.2997119426727295, -2.383255958557129, -4.302631855010986, 0.32093676924705505]\n",
      "i=43: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=43: values: [-0.045584727078676224, -0.001737106591463089, -0.013719098642468452, 0.014772268012166023, 0.024544240906834602]\n",
      "i=43: values: [-0.045584727078676224, -0.001737106591463089, -0.013719098642468452, 0.014772268012166023, 0.024544240906834602]\n",
      "i=44: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=44: keys: [-8.027068138122559, 6.488775253295898, -6.006464958190918, -3.0555825233459473, -0.01422860473394394]\n",
      "i=44: keys: [-8.027068138122559, 6.488775253295898, -6.006464958190918, -3.0555825233459473, -0.01422860473394394]\n",
      "i=44: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=44: values: [-0.007878723554313183, -0.008207928389310837, 0.008532846346497536, -0.018350016325712204, 0.007643936201930046]\n",
      "i=44: values: [-0.007878723554313183, -0.008207928389310837, 0.008532846346497536, -0.018350016325712204, 0.007643936201930046]\n",
      "i=45: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=45: keys: [-10.224579811096191, 9.269365310668945, -6.881677150726318, -1.0002758502960205, 0.1294642984867096]\n",
      "i=45: keys: [-10.224579811096191, 9.269365310668945, -6.881677150726318, -1.0002758502960205, 0.1294642984867096]\n",
      "i=45: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=45: values: [0.01296982727944851, -0.03826059773564339, -0.008268261328339577, -0.0009136036969721317, 0.007700623944401741]\n",
      "i=45: values: [0.01296982727944851, -0.03826059773564339, -0.008268261328339577, -0.0009136036969721317, 0.007700623944401741]\n",
      "i=46: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=46: keys: [-2.4686365127563477, 8.779120445251465, -7.40776252746582, 1.0596263408660889, 0.10827688872814178]\n",
      "i=46: keys: [-2.4686365127563477, 8.779120445251465, -7.40776252746582, 1.0596263408660889, 0.10827688872814178]\n",
      "i=46: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=46: values: [0.0008347008842974901, 0.009060125797986984, 0.00035081803798675537, -0.007168794982135296, 0.03210063651204109]\n",
      "i=46: values: [0.0008347008842974901, 0.009060125797986984, 0.00035081803798675537, -0.007168794982135296, 0.03210063651204109]\n",
      "i=47: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=47: keys: [7.54304838180542, 5.033699989318848, -6.8456902503967285, 2.844072103500366, 0.17923787236213684]\n",
      "i=47: keys: [7.54304838180542, 5.033699989318848, -6.8456902503967285, 2.844072103500366, 0.17923787236213684]\n",
      "i=47: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=47: values: [-0.01669934391975403, -0.0004041055217385292, -0.01700674556195736, -0.005264570005238056, 0.013117420487105846]\n",
      "i=47: values: [-0.01669934391975403, -0.0004041055217385292, -0.01700674556195736, -0.005264570005238056, 0.013117420487105846]\n",
      "i=48: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=48: keys: [10.753965377807617, -0.763812780380249, -4.962551593780518, 4.359426021575928, 0.23869778215885162]\n",
      "i=48: keys: [10.753965377807617, -0.763812780380249, -4.962551593780518, 4.359426021575928, 0.23869778215885162]\n",
      "i=48: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=48: values: [-0.010338855907320976, -0.004892245400696993, 0.019367124885320663, -0.0013712006621062756, -0.026017967611551285]\n",
      "i=48: values: [-0.010338855907320976, -0.004892245400696993, 0.019367124885320663, -0.0013712006621062756, -0.026017967611551285]\n",
      "i=49: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=49: keys: [3.19514536857605, -7.147424221038818, -1.7680190801620483, 5.6754469871521, 0.22736991941928864]\n",
      "i=49: keys: [3.19514536857605, -7.147424221038818, -1.7680190801620483, 5.6754469871521, 0.22736991941928864]\n",
      "i=49: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=49: values: [-0.011283719912171364, -0.018521035090088844, 0.008460381999611855, -0.007884752936661243, 0.04855034500360489]\n",
      "i=49: values: [-0.011283719912171364, -0.018521035090088844, 0.008460381999611855, -0.007884752936661243, 0.04855034500360489]\n",
      "i=50: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=50: keys: [-6.746036052703857, -9.391170501708984, 0.9746465682983398, 6.418478012084961, 0.08685293048620224]\n",
      "i=50: keys: [-6.746036052703857, -9.391170501708984, 0.9746465682983398, 6.418478012084961, 0.08685293048620224]\n",
      "i=50: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=50: values: [-0.01685245707631111, -0.0071500204503536224, 0.010530443862080574, 0.0009505273774266243, -0.01182546466588974]\n",
      "i=50: values: [-0.01685245707631111, -0.0071500204503536224, 0.010530443862080574, 0.0009505273774266243, -0.01182546466588974]\n",
      "i=51: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=51: keys: [-10.687976837158203, -8.162444114685059, 3.940634250640869, 6.8653411865234375, 0.261278361082077]\n",
      "i=51: keys: [-10.687976837158203, -8.162444114685059, 3.940634250640869, 6.8653411865234375, 0.261278361082077]\n",
      "i=51: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=51: values: [0.008301091380417347, -0.02661163918673992, 0.012333236634731293, -0.0009931810200214386, -0.004434363916516304]\n",
      "i=51: values: [0.008301091380417347, -0.02661163918673992, 0.012333236634731293, -0.0009931810200214386, -0.004434363916516304]\n",
      "i=52: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=52: keys: [-4.612266540527344, -3.483471632003784, 6.306264400482178, 7.280170440673828, -0.016094908118247986]\n",
      "i=52: keys: [-4.612266540527344, -3.483471632003784, 6.306264400482178, 7.280170440673828, -0.016094908118247986]\n",
      "i=52: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=52: values: [0.002465474884957075, 0.0017446614801883698, 0.037480428814888, -0.0003672409802675247, 0.0234907828271389]\n",
      "i=52: values: [0.002465474884957075, 0.0017446614801883698, 0.037480428814888, -0.0003672409802675247, 0.0234907828271389]\n",
      "i=53: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=53: keys: [4.862089157104492, 1.6182903051376343, 7.17296028137207, 5.687560558319092, 0.3178473114967346]\n",
      "i=53: keys: [4.862089157104492, 1.6182903051376343, 7.17296028137207, 5.687560558319092, 0.3178473114967346]\n",
      "i=53: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=53: values: [-0.0003277973155491054, 0.008820456452667713, -0.008250861428678036, -0.006607989314943552, 0.0032703885808587074]\n",
      "i=53: values: [-0.0003277973155491054, 0.008820456452667713, -0.008250861428678036, -0.006607989314943552, 0.0032703885808587074]\n",
      "i=54: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=54: keys: [10.620946884155273, 7.272684097290039, 7.151604652404785, 5.090027332305908, 0.07007793337106705]\n",
      "i=54: keys: [10.620946884155273, 7.272684097290039, 7.151604652404785, 5.090027332305908, 0.07007793337106705]\n",
      "i=54: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=54: values: [-0.014316731132566929, -0.01463577151298523, -0.011298226192593575, -0.0022373911924660206, 0.004958348348736763]\n",
      "i=54: values: [-0.014316731132566929, -0.01463577151298523, -0.011298226192593575, -0.0022373911924660206, 0.004958348348736763]\n",
      "i=55: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=55: keys: [6.387019157409668, 9.422245025634766, 5.984134197235107, 3.3891549110412598, 0.10942237079143524]\n",
      "i=55: keys: [6.387019157409668, 9.422245025634766, 5.984134197235107, 3.3891549110412598, 0.10942237079143524]\n",
      "i=55: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=55: values: [0.012238831259310246, 0.030107494443655014, -9.697629138827324e-05, 0.006858213804662228, -0.03711722791194916]\n",
      "i=55: values: [0.012238831259310246, 0.030107494443655014, -9.697629138827324e-05, 0.006858213804662228, -0.03711722791194916]\n",
      "i=56: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=56: keys: [-3.880814790725708, 7.7236857414245605, 3.2257180213928223, 1.659106731414795, 0.1107446551322937]\n",
      "i=56: keys: [-3.880814790725708, 7.7236857414245605, 3.2257180213928223, 1.659106731414795, 0.1107446551322937]\n",
      "i=56: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=56: values: [-0.025808120146393776, -0.03131275251507759, 0.018312968313694, -0.0006018010899424553, 0.010360335931181908]\n",
      "i=56: values: [-0.025808120146393776, -0.03131275251507759, 0.018312968313694, -0.0006018010899424553, 0.010360335931181908]\n",
      "i=57: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=57: keys: [-10.526195526123047, 2.7955498695373535, 0.25971388816833496, -0.12989062070846558, 0.00272514671087265]\n",
      "i=57: keys: [-10.526195526123047, 2.7955498695373535, 0.25971388816833496, -0.12989062070846558, 0.00272514671087265]\n",
      "i=57: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=57: values: [-0.0008176745614036918, -0.013975793495774269, -0.004544749855995178, 0.012583652511239052, 0.030712490901350975]\n",
      "i=57: values: [-0.0008176745614036918, -0.013975793495774269, -0.004544749855995178, 0.012583652511239052, 0.030712490901350975]\n",
      "i=58: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=58: keys: [-7.75324010848999, -2.830063819885254, -2.7479119300842285, -2.030426025390625, 0.1499667763710022]\n",
      "i=58: keys: [-7.75324010848999, -2.830063819885254, -2.7479119300842285, -2.030426025390625, 0.1499667763710022]\n",
      "i=58: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=58: values: [-0.019065814092755318, -0.024128355085849762, -0.0009186333045363426, 0.00721325259655714, 0.00019186828285455704]\n",
      "i=58: values: [-0.019065814092755318, -0.024128355085849762, -0.0009186333045363426, 0.00721325259655714, 0.00019186828285455704]\n",
      "i=59: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=59: keys: [2.797804832458496, -8.032508850097656, -5.485958576202393, -3.8711092472076416, -0.1704806238412857]\n",
      "i=59: keys: [2.797804832458496, -8.032508850097656, -5.485958576202393, -3.8711092472076416, -0.1704806238412857]\n",
      "i=59: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=59: values: [0.0009755732025951147, -0.003382263705134392, 0.02600182592868805, -0.01302700862288475, 0.02150927297770977]\n",
      "i=59: values: [0.0009755732025951147, -0.003382263705134392, 0.02600182592868805, -0.01302700862288475, 0.02150927297770977]\n",
      "i=60: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=60: keys: [10.312079429626465, -9.01893424987793, -6.995319366455078, -5.4985551834106445, -0.24611127376556396]\n",
      "i=60: keys: [10.312079429626465, -9.01893424987793, -6.995319366455078, -5.4985551834106445, -0.24611127376556396]\n",
      "i=60: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=60: values: [0.017570771276950836, 0.005604807287454605, 0.013234655372798443, 0.010662296786904335, -0.009113812819123268]\n",
      "i=60: values: [0.017570771276950836, 0.005604807287454605, 0.013234655372798443, 0.010662296786904335, -0.009113812819123268]\n",
      "i=61: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=61: keys: [8.950027465820312, -7.449878692626953, -7.319313049316406, -5.956507682800293, -0.018784992396831512]\n",
      "i=61: keys: [8.950027465820312, -7.449878692626953, -7.319313049316406, -5.956507682800293, -0.018784992396831512]\n",
      "i=61: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=61: values: [-0.017954550683498383, -0.005517502315342426, -0.002643323503434658, 0.007015876937657595, -0.024682514369487762]\n",
      "i=61: values: [-0.017954550683498383, -0.005517502315342426, -0.002643323503434658, 0.007015876937657595, -0.024682514369487762]\n",
      "i=62: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=62: keys: [-1.1625046730041504, -1.1573946475982666, -6.1149749755859375, -6.737611293792725, -0.27324506640434265]\n",
      "i=62: keys: [-1.1625046730041504, -1.1573946475982666, -6.1149749755859375, -6.737611293792725, -0.27324506640434265]\n",
      "i=62: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=62: values: [-0.021417859941720963, 0.002246396616101265, 0.04015553742647171, -0.0020157936960458755, 0.018230658024549484]\n",
      "i=62: values: [-0.021417859941720963, 0.002246396616101265, 0.04015553742647171, -0.0020157936960458755, 0.018230658024549484]\n",
      "i=63: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=63: keys: [-9.59122371673584, 3.4721732139587402, -4.668501853942871, -6.644901275634766, -0.10122620314359665]\n",
      "i=63: keys: [-9.59122371673584, 3.4721732139587402, -4.668501853942871, -6.644901275634766, -0.10122620314359665]\n",
      "i=63: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=63: values: [0.0018903408199548721, -0.0032282117754220963, 0.0008034915663301945, 0.0014986866153776646, -0.048255354166030884]\n",
      "i=63: values: [0.0018903408199548721, -0.0032282117754220963, 0.0008034915663301945, 0.0014986866153776646, -0.048255354166030884]\n",
      "i=64: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=64: keys: [-9.216995239257812, 8.666255950927734, -1.484403371810913, -6.536676406860352, -0.2622182369232178]\n",
      "i=64: keys: [-9.216995239257812, 8.666255950927734, -1.484403371810913, -6.536676406860352, -0.2622182369232178]\n",
      "i=64: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=64: values: [-0.005827223416417837, 0.00031771138310432434, -0.02469661459326744, -0.014443060383200645, 0.009833818301558495]\n",
      "i=64: values: [-0.005827223416417837, 0.00031771138310432434, -0.02469661459326744, -0.014443060383200645, 0.009833818301558495]\n",
      "i=65: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=65: keys: [-0.7824687957763672, 9.422120094299316, 1.2312290668487549, -5.482077121734619, -0.1279328465461731]\n",
      "i=65: keys: [-0.7824687957763672, 9.422120094299316, 1.2312290668487549, -5.482077121734619, -0.1279328465461731]\n",
      "i=65: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=65: values: [-0.009519451297819614, 0.030915193259716034, 0.0008460185490548611, 0.0061403000727295876, 0.011360382661223412]\n",
      "i=65: values: [-0.009519451297819614, 0.030915193259716034, 0.0008460185490548611, 0.0061403000727295876, 0.011360382661223412]\n",
      "i=66: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=66: keys: [8.63992691040039, 5.9111433029174805, 4.21691370010376, -4.058136463165283, -0.1966821849346161]\n",
      "i=66: keys: [8.63992691040039, 5.9111433029174805, 4.21691370010376, -4.058136463165283, -0.1966821849346161]\n",
      "i=66: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=66: values: [0.029660765081644058, 0.014319139532744884, 0.009122289717197418, -0.00511194858700037, 0.003936826717108488]\n",
      "i=66: values: [0.029660765081644058, 0.014319139532744884, 0.009122289717197418, -0.00511194858700037, 0.003936826717108488]\n",
      "i=67: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=67: keys: [9.919234275817871, 0.9741885662078857, 6.449296951293945, -2.2730789184570312, -0.2861331105232239]\n",
      "i=67: keys: [9.919234275817871, 0.9741885662078857, 6.449296951293945, -2.2730789184570312, -0.2861331105232239]\n",
      "i=67: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=67: values: [-0.0032591293565928936, -0.003965182229876518, 0.021860381588339806, 0.0016344194300472736, -0.008613904006779194]\n",
      "i=67: values: [-0.0032591293565928936, -0.003965182229876518, 0.021860381588339806, 0.0016344194300472736, -0.008613904006779194]\n",
      "i=68: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=68: keys: [2.286348819732666, -4.717154026031494, 7.388641834259033, -0.6956940293312073, -0.2739865481853485]\n",
      "i=68: keys: [2.286348819732666, -4.717154026031494, 7.388641834259033, -0.6956940293312073, -0.2739865481853485]\n",
      "i=68: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=68: values: [-0.021000385284423828, -0.017061462625861168, 0.037776313722133636, 0.002166744787245989, -0.015242766588926315]\n",
      "i=68: values: [-0.021000385284423828, -0.017061462625861168, 0.037776313722133636, 0.002166744787245989, -0.015242766588926315]\n",
      "i=69: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=69: keys: [-8.049264907836914, -8.961466789245605, 7.074991226196289, 1.1120096445083618, -0.10028016567230225]\n",
      "i=69: keys: [-8.049264907836914, -8.961466789245605, 7.074991226196289, 1.1120096445083618, -0.10028016567230225]\n",
      "i=69: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=69: values: [-0.0015267357230186462, -0.006636585108935833, 0.024621784687042236, 0.0029389047995209694, 0.01141352578997612]\n",
      "i=69: values: [-0.0015267357230186462, -0.006636585108935833, 0.024621784687042236, 0.0029389047995209694, 0.01141352578997612]\n",
      "i=70: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=70: keys: [-10.490361213684082, -9.074172019958496, 5.593680381774902, 2.973135471343994, -0.2745590806007385]\n",
      "i=70: keys: [-10.490361213684082, -9.074172019958496, 5.593680381774902, 2.973135471343994, -0.2745590806007385]\n",
      "i=70: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=70: values: [-0.021000385284423828, -0.017061462625861168, 0.037776313722133636, 0.002166744787245989, -0.015242766588926315]\n",
      "i=70: values: [-0.021000385284423828, -0.017061462625861168, 0.037776313722133636, 0.002166744787245989, -0.015242766588926315]\n",
      "i=71: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=71: keys: [-2.852374792098999, -5.013594627380371, 2.6763083934783936, 4.560030937194824, 0.02944834530353546]\n",
      "i=71: keys: [-2.852374792098999, -5.013594627380371, 2.6763083934783936, 4.560030937194824, 0.02944834530353546]\n",
      "i=71: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=71: values: [-0.02465834468603134, 0.023726295679807663, 0.031847670674324036, -0.0029221586883068085, 0.025653354823589325]\n",
      "i=71: values: [-0.02465834468603134, 0.023726295679807663, 0.031847670674324036, -0.0029221586883068085, 0.025653354823589325]\n",
      "i=72: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=72: keys: [6.550941467285156, -0.27766966819763184, 0.1640152931213379, 5.733425140380859, -0.32598966360092163]\n",
      "i=72: keys: [6.550941467285156, -0.27766966819763184, 0.1640152931213379, 5.733425140380859, -0.32598966360092163]\n",
      "i=72: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=72: values: [0.0009778481908142567, 0.0045385099947452545, 0.010461587458848953, -0.002550048753619194, 0.02299218252301216]\n",
      "i=72: values: [0.0009778481908142567, 0.0045385099947452545, 0.010461587458848953, -0.002550048753619194, 0.02299218252301216]\n",
      "i=73: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=73: keys: [9.944974899291992, 5.954078197479248, -3.363574504852295, 6.858141899108887, -0.18538059294223785]\n",
      "i=73: keys: [9.944974899291992, 5.954078197479248, -3.363574504852295, 6.858141899108887, -0.18538059294223785]\n",
      "i=73: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=73: values: [0.025303978472948074, -0.019907165318727493, -0.0028811460360884666, -0.012172224000096321, 0.0016315225511789322]\n",
      "i=73: values: [0.025303978472948074, -0.019907165318727493, -0.0028811460360884666, -0.012172224000096321, 0.0016315225511789322]\n",
      "i=74: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=74: keys: [5.2700276374816895, 8.789735794067383, -5.255688667297363, 6.548020839691162, -0.15952132642269135]\n",
      "i=74: keys: [5.2700276374816895, 8.789735794067383, -5.255688667297363, 6.548020839691162, -0.15952132642269135]\n",
      "i=74: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=74: values: [0.02201455645263195, -0.0004506525583565235, -0.007620122283697128, 0.013106679543852806, -0.007683888077735901]\n",
      "i=74: values: [0.02201455645263195, -0.0004506525583565235, -0.007620122283697128, 0.013106679543852806, -0.007683888077735901]\n",
      "i=75: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=75: keys: [-5.928720951080322, 8.478107452392578, -6.809804916381836, 7.117136001586914, 0.23135855793952942]\n",
      "i=75: keys: [-5.928720951080322, 8.478107452392578, -6.809804916381836, 7.117136001586914, 0.23135855793952942]\n",
      "i=75: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=75: values: [-0.02465834468603134, 0.023726295679807663, 0.031847670674324036, -0.0029221586883068085, 0.025653354823589325]\n",
      "i=75: values: [-0.02465834468603134, 0.023726295679807663, 0.031847670674324036, -0.0029221586883068085, 0.025653354823589325]\n",
      "i=76: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=76: keys: [-10.895297050476074, 4.371698379516602, -7.256325721740723, 6.398353099822998, 0.19039146602153778]\n",
      "i=76: keys: [-10.895297050476074, 4.371698379516602, -7.256325721740723, 6.398353099822998, 0.19039146602153778]\n",
      "i=76: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=76: values: [0.011901721358299255, -0.0010180436074733734, 0.037717305123806, 0.0018865643069148064, 0.02024102956056595]\n",
      "i=76: values: [0.011901721358299255, -0.0010180436074733734, 0.037717305123806, 0.0018865643069148064, 0.02024102956056595]\n",
      "i=77: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=77: keys: [-6.236301422119141, -1.2096989154815674, -6.296175956726074, 4.733034133911133, 0.06613170355558395]\n",
      "i=77: keys: [-6.236301422119141, -1.2096989154815674, -6.296175956726074, 4.733034133911133, 0.06613170355558395]\n",
      "i=77: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=77: values: [0.005060714669525623, 0.030061598867177963, 0.0214121975004673, 0.02369208261370659, 0.008625559508800507]\n",
      "i=77: values: [0.005060714669525623, 0.030061598867177963, 0.0214121975004673, 0.02369208261370659, 0.008625559508800507]\n",
      "i=78: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=78: keys: [3.5578246116638184, -6.229510307312012, -4.367408275604248, 3.1512610912323, -0.01275680959224701]\n",
      "i=78: keys: [3.5578246116638184, -6.229510307312012, -4.367408275604248, 3.1512610912323, -0.01275680959224701]\n",
      "i=78: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=78: values: [-0.01566449925303459, -0.020384492352604866, 0.013296445831656456, -0.006370627786964178, -0.0004791971296072006]\n",
      "i=78: values: [-0.01566449925303459, -0.020384492352604866, 0.013296445831656456, -0.006370627786964178, -0.0004791971296072006]\n",
      "i=79: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=79: keys: [10.644098281860352, -9.017204284667969, -1.228360891342163, 1.4538296461105347, 0.2744661271572113]\n",
      "i=79: keys: [10.644098281860352, -9.017204284667969, -1.228360891342163, 1.4538296461105347, 0.2744661271572113]\n",
      "i=79: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=79: values: [0.017570771276950836, 0.005604807287454605, 0.013234655372798443, 0.010662296786904335, -0.009113812819123268]\n",
      "i=79: values: [0.017570771276950836, 0.005604807287454605, 0.013234655372798443, 0.010662296786904335, -0.009113812819123268]\n",
      "i=80: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=80: keys: [7.551797389984131, -8.304811477661133, 1.6117339134216309, -0.5168318748474121, 0.05144064128398895]\n",
      "i=80: keys: [7.551797389984131, -8.304811477661133, 1.6117339134216309, -0.5168318748474121, 0.05144064128398895]\n",
      "i=80: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=80: values: [-0.010015794076025486, -0.014976775273680687, 0.016056017950177193, -0.00558482063934207, 0.009931695647537708]\n",
      "i=80: values: [-0.010015794076025486, -0.014976775273680687, 0.016056017950177193, -0.00558482063934207, 0.009931695647537708]\n",
      "i=81: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=81: keys: [-2.0991926193237305, -3.941504716873169, 4.7220869064331055, -2.5348892211914062, 0.038914114236831665]\n",
      "i=81: keys: [-2.0991926193237305, -3.941504716873169, 4.7220869064331055, -2.5348892211914062, 0.038914114236831665]\n",
      "i=81: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=81: values: [-0.01101189199835062, -0.00786633975803852, 0.01097924169152975, -0.014047818258404732, -0.01860964298248291]\n",
      "i=81: values: [-0.01101189199835062, -0.00786633975803852, 0.01097924169152975, -0.014047818258404732, -0.01860964298248291]\n",
      "i=82: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=82: keys: [-10.434646606445312, 2.0499866008758545, 6.571138381958008, -3.6926352977752686, 0.21786926686763763]\n",
      "i=82: keys: [-10.434646606445312, 2.0499866008758545, 6.571138381958008, -3.6926352977752686, 0.21786926686763763]\n",
      "i=82: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=82: values: [-0.0033019036054611206, -0.01251131296157837, 0.0077509465627372265, 0.01682574674487114, -0.0002922220155596733]\n",
      "i=82: values: [-0.0033019036054611206, -0.01251131296157837, 0.0077509465627372265, 0.01682574674487114, -0.0002922220155596733]\n",
      "i=83: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=83: keys: [-8.245684623718262, 7.313815593719482, 7.072630882263184, -5.197901725769043, 0.2956792116165161]\n",
      "i=83: keys: [-8.245684623718262, 7.313815593719482, 7.072630882263184, -5.197901725769043, 0.2956792116165161]\n",
      "i=83: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=83: values: [-0.015080871060490608, -0.01218362245708704, -0.018983423709869385, -0.004795030690729618, -0.006167326122522354]\n",
      "i=83: values: [-0.015080871060490608, -0.01218362245708704, -0.018983423709869385, -0.004795030690729618, -0.006167326122522354]\n",
      "i=84: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=84: keys: [0.622246265411377, 9.32852840423584, 7.198423385620117, -6.123595237731934, 0.016228461638092995]\n",
      "i=84: keys: [0.622246265411377, 9.32852840423584, 7.198423385620117, -6.123595237731934, 0.016228461638092995]\n",
      "i=84: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=84: values: [-0.021929560229182243, 0.016946323215961456, 0.005516830366104841, -0.008646216243505478, -0.02680971287190914]\n",
      "i=84: values: [-0.021929560229182243, 0.016946323215961456, 0.005516830366104841, -0.008646216243505478, -0.02680971287190914]\n",
      "i=85: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=85: keys: [9.392400741577148, 7.9531331062316895, 5.578209400177002, -6.184056282043457, 0.2228158712387085]\n",
      "i=85: keys: [9.392400741577148, 7.9531331062316895, 5.578209400177002, -6.184056282043457, 0.2228158712387085]\n",
      "i=85: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=85: values: [0.007870092988014221, 0.018576934933662415, -0.056551337242126465, 0.04144789278507233, -0.006091563031077385]\n",
      "i=85: values: [0.007870092988014221, 0.018576934933662415, -0.056551337242126465, 0.04144789278507233, -0.006091563031077385]\n",
      "i=86: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=86: keys: [9.579904556274414, 3.3724918365478516, 2.898911714553833, -6.776119232177734, 0.2591373324394226]\n",
      "i=86: keys: [9.579904556274414, 3.3724918365478516, 2.898911714553833, -6.776119232177734, 0.2591373324394226]\n",
      "i=86: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=86: values: [0.008868999779224396, -0.025973062962293625, 0.000257998239248991, 0.00393444299697876, 0.013531540520489216]\n",
      "i=86: values: [0.008868999779224396, -0.025973062962293625, 0.000257998239248991, 0.00393444299697876, 0.013531540520489216]\n",
      "i=87: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=87: keys: [-0.23307418823242188, -3.799814462661743, -0.5350081920623779, -6.832003116607666, 0.06534942984580994]\n",
      "i=87: keys: [-0.23307418823242188, -3.799814462661743, -0.5350081920623779, -6.832003116607666, 0.06534942984580994]\n",
      "i=87: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=87: values: [-0.02465834468603134, 0.023726297542452812, 0.031847670674324036, -0.0029221600852906704, 0.025653358548879623]\n",
      "i=87: values: [-0.02465834468603134, 0.023726297542452812, 0.031847670674324036, -0.0029221600852906704, 0.025653358548879623]\n",
      "i=88: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=88: keys: [-8.613150596618652, -7.713072776794434, -3.486778497695923, -5.446897506713867, 0.0759546235203743]\n",
      "i=88: keys: [-8.613150596618652, -7.713072776794434, -3.486778497695923, -5.446897506713867, 0.0759546235203743]\n",
      "i=88: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=88: values: [-0.0009246049448847771, 0.000922888983041048, 0.006822931580245495, -0.0015917073469609022, -0.006216313689947128]\n",
      "i=88: values: [-0.0009246049448847771, 0.000922888983041048, 0.006822931580245495, -0.0015917073469609022, -0.006216313689947128]\n",
      "i=89: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=89: keys: [-9.946456909179688, -9.276987075805664, -5.7852678298950195, -4.037725448608398, 0.16478252410888672]\n",
      "i=89: keys: [-9.946456909179688, -9.276987075805664, -5.7852678298950195, -4.037725448608398, 0.16478252410888672]\n",
      "i=89: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=89: values: [-0.02674740180373192, -0.03246765583753586, 0.0030352603644132614, 0.011558408848941326, 0.009188827127218246]\n",
      "i=89: values: [-0.02674740180373192, -0.03246765583753586, 0.0030352603644132614, 0.011558408848941326, 0.009188827127218246]\n",
      "i=90: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=90: keys: [-1.5174014568328857, -6.757452011108398, -7.2218780517578125, -2.5852465629577637, -0.10826355218887329]\n",
      "i=90: keys: [-1.5174014568328857, -6.757452011108398, -7.2218780517578125, -2.5852465629577637, -0.10826355218887329]\n",
      "i=90: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=90: values: [0.010106559842824936, 0.007048763334751129, 0.013089342042803764, 0.013118644244968891, 0.037335105240345]\n",
      "i=90: values: [0.010106559842824936, 0.007048763334751129, 0.013089342042803764, 0.013118644244968891, 0.037335105240345]\n",
      "i=91: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=91: keys: [7.58329963684082, -2.348097801208496, -7.410022735595703, -0.5579246282577515, 0.15152233839035034]\n",
      "i=91: keys: [7.58329963684082, -2.348097801208496, -7.410022735595703, -0.5579246282577515, 0.15152233839035034]\n",
      "i=91: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=91: values: [0.0194671880453825, -0.01012340560555458, 0.0008926498703658581, 0.00011595431715250015, 0.0027886475436389446]\n",
      "i=91: values: [0.0194671880453825, -0.01012340560555458, 0.0008926498703658581, 0.00011595431715250015, 0.0027886475436389446]\n",
      "i=92: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=92: keys: [10.252689361572266, 4.135807991027832, -6.082813739776611, 1.5195913314819336, -0.12033525854349136]\n",
      "i=92: keys: [10.252689361572266, 4.135807991027832, -6.082813739776611, 1.5195913314819336, -0.12033525854349136]\n",
      "i=92: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=92: values: [0.017570767551660538, 0.00560480821877718, 0.013234657235443592, 0.010662302374839783, -0.009113810956478119]\n",
      "i=92: values: [0.017570767551660538, 0.00560480821877718, 0.013234657235443592, 0.010662302374839783, -0.009113810956478119]\n",
      "i=93: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=93: keys: [3.6732265949249268, 8.212058067321777, -4.145284652709961, 3.040566921234131, 0.10054803639650345]\n",
      "i=93: keys: [3.6732265949249268, 8.212058067321777, -4.145284652709961, 3.040566921234131, 0.10054803639650345]\n",
      "i=93: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=93: values: [0.006969214417040348, -0.003735107835382223, -0.004646465182304382, 0.007427141070365906, -0.009418437257409096]\n",
      "i=93: values: [0.006969214417040348, -0.003735107835382223, -0.004646465182304382, 0.007427141070365906, -0.009418437257409096]\n",
      "i=94: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=94: keys: [-6.353487968444824, 9.151771545410156, -1.023681640625, 4.752506732940674, 0.07877424359321594]\n",
      "i=94: keys: [-6.353487968444824, 9.151771545410156, -1.023681640625, 4.752506732940674, 0.07877424359321594]\n",
      "i=94: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=94: values: [-0.009796789847314358, -0.014799395576119423, 0.008242610841989517, 0.0028560608625411987, -0.005354326218366623]\n",
      "i=94: values: [-0.009796789847314358, -0.014799395576119423, 0.008242610841989517, 0.0028560608625411987, -0.005354326218366623]\n",
      "i=95: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=95: keys: [-10.646710395812988, 6.0584492683410645, 2.182339906692505, 6.194971561431885, -0.22286762297153473]\n",
      "i=95: keys: [-10.646710395812988, 6.0584492683410645, 2.182339906692505, 6.194971561431885, -0.22286762297153473]\n",
      "i=95: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=95: values: [0.002971109002828598, 0.0028248452581465244, 0.02487947978079319, -0.007722094655036926, 0.03723720833659172]\n",
      "i=95: values: [0.002971109002828598, 0.0028248452581465244, 0.02487947978079319, -0.007722094655036926, 0.03723720833659172]\n",
      "i=96: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=96: keys: [-5.301325798034668, 1.6100444793701172, 4.430847644805908, 6.394253253936768, 0.035480473190546036]\n",
      "i=96: keys: [-5.301325798034668, 1.6100444793701172, 4.430847644805908, 6.394253253936768, 0.035480473190546036]\n",
      "i=96: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=96: values: [-0.010293555445969105, 0.0011192201636731625, 0.007892688736319542, -0.004882361274212599, -0.020949238911271095]\n",
      "i=96: values: [-0.010293555445969105, 0.0011192201636731625, 0.007892688736319542, -0.004882361274212599, -0.020949238911271095]\n",
      "i=97: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=97: keys: [5.157680034637451, -4.521422863006592, 6.594635486602783, 6.69490909576416, -0.0033327462151646614]\n",
      "i=97: keys: [5.157680034637451, -4.521422863006592, 6.594635486602783, 6.69490909576416, -0.0033327462151646614]\n",
      "i=97: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=97: values: [-0.0007702386938035488, 0.0027561145834624767, -0.01138712652027607, -0.000508883036673069, 0.011759929358959198]\n",
      "i=97: values: [-0.0007702386938035488, 0.0027561145834624767, -0.01138712652027607, -0.000508883036673069, 0.011759929358959198]\n",
      "i=98: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=98: keys: [10.838508605957031, -8.905309677124023, 7.444509506225586, 6.964720249176025, -0.24738050997257233]\n",
      "i=98: keys: [10.838508605957031, -8.905309677124023, 7.444509506225586, 6.964720249176025, -0.24738050997257233]\n",
      "i=98: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=98: values: [-0.0015267347916960716, -0.006636582314968109, 0.024621788412332535, 0.0029389080591499805, 0.011413518339395523]\n",
      "i=98: values: [-0.0015267347916960716, -0.006636582314968109, 0.024621788412332535, 0.0029389080591499805, 0.011413518339395523]\n",
      "i=99: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=99: keys: [5.633742809295654, -8.654394149780273, 6.762666702270508, 6.066140651702881, -0.28665122389793396]\n",
      "i=99: keys: [5.633742809295654, -8.654394149780273, 6.762666702270508, 6.066140651702881, -0.28665122389793396]\n",
      "i=99: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=99: values: [0.017570767551660538, 0.00560480821877718, 0.013234657235443592, 0.010662302374839783, -0.009113810956478119]\n",
      "i=99: values: [0.017570767551660538, 0.00560480821877718, 0.013234657235443592, 0.010662302374839783, -0.009113810956478119]\n",
      "i=100: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=100: keys: [-3.761406421661377, -6.24078893661499, 5.41190242767334, 4.5938720703125, -0.14564894139766693]\n",
      "i=100: keys: [-3.761406421661377, -6.24078893661499, 5.41190242767334, 4.5938720703125, -0.14564894139766693]\n",
      "i=100: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=100: values: [0.0024774563498795033, 0.021877046674489975, -0.018245941027998924, -0.015633825212717056, -0.005730719305574894]\n",
      "i=100: values: [0.0024774563498795033, 0.021877046674489975, -0.018245941027998924, -0.015633825212717056, -0.005730719305574894]\n",
      "i=101: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=101: keys: [-10.757525444030762, 0.28377795219421387, 2.3471670150756836, 3.7094855308532715, -0.15049169957637787]\n",
      "i=101: keys: [-10.757525444030762, 0.28377795219421387, 2.3471670150756836, 3.7094855308532715, -0.15049169957637787]\n",
      "i=101: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=101: values: [0.0024654697626829147, 0.0017446477431803942, 0.037480428814888, -0.0003672422608360648, 0.0234907865524292]\n",
      "i=101: values: [0.0024654697626829147, 0.0017446477431803942, 0.037480428814888, -0.0003672422608360648, 0.0234907865524292]\n",
      "i=102: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=102: keys: [-6.962796688079834, 5.781400680541992, -0.781898021697998, 1.5845205783843994, -0.13357293605804443]\n",
      "i=102: keys: [-6.962796688079834, 5.781400680541992, -0.781898021697998, 1.5845205783843994, -0.13357293605804443]\n",
      "i=102: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=102: values: [-0.011762927286326885, -0.017561161890625954, 0.023646537214517593, 0.0046984972432255745, 0.0015243235975503922]\n",
      "i=102: values: [-0.011762927286326885, -0.017561161890625954, 0.023646537214517593, 0.0046984972432255745, 0.0015243235975503922]\n",
      "i=103: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=103: keys: [2.3515210151672363, 8.915623664855957, -3.607435703277588, -0.39285895228385925, -0.21138474345207214]\n",
      "i=103: keys: [2.3515210151672363, 8.915623664855957, -3.607435703277588, -0.39285895228385925, -0.21138474345207214]\n",
      "i=103: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=103: values: [-0.01906581223011017, -0.024128351360559464, -0.0009186314418911934, 0.007213253993541002, 0.00019186362624168396]\n",
      "i=103: values: [-0.01906581223011017, -0.024128351360559464, -0.0009186314418911934, 0.007213253993541002, 0.00019186362624168396]\n",
      "i=104: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=104: keys: [10.401402473449707, 8.527388572692871, -5.542749404907227, -2.1178228855133057, -0.15048374235630035]\n",
      "i=104: keys: [10.401402473449707, 8.527388572692871, -5.542749404907227, -2.1178228855133057, -0.15048374235630035]\n",
      "i=104: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=104: values: [-0.01128371898084879, -0.018521033227443695, 0.008460385724902153, -0.007884761318564415, 0.04855033755302429]\n",
      "i=104: values: [-0.01128371898084879, -0.018521033227443695, 0.008460385724902153, -0.007884761318564415, 0.04855033755302429]\n",
      "i=105: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=105: keys: [8.750008583068848, 5.19830846786499, -7.033651351928711, -3.6011157035827637, -0.04795912653207779]\n",
      "i=105: keys: [8.750008583068848, 5.19830846786499, -7.033651351928711, -3.6011157035827637, -0.04795912653207779]\n",
      "i=105: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=105: values: [0.01225441787391901, 0.028207819908857346, 0.01128990389406681, 0.0010638302192091942, -0.021981695666909218]\n",
      "i=105: values: [0.01225441787391901, 0.028207819908857346, 0.01128990389406681, 0.0010638302192091942, -0.021981695666909218]\n",
      "i=106: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=106: keys: [-0.8232808113098145, -0.4664123058319092, -7.275125503540039, -5.006447792053223, -0.307504266500473]\n",
      "i=106: keys: [-0.8232808113098145, -0.4664123058319092, -7.275125503540039, -5.006447792053223, -0.307504266500473]\n",
      "i=106: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=106: values: [0.014123428612947464, 0.031063631176948547, -0.013745849952101707, 0.005426926538348198, 0.00833403505384922]\n",
      "i=106: values: [0.014123428612947464, 0.031063631176948547, -0.013745849952101707, 0.005426926538348198, 0.00833403505384922]\n",
      "i=107: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=107: keys: [-9.44303035736084, -6.735913276672363, -5.900758266448975, -6.3686628341674805, -0.01107482984662056]\n",
      "i=107: keys: [-9.44303035736084, -6.735913276672363, -5.900758266448975, -6.3686628341674805, -0.01107482984662056]\n",
      "i=107: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=107: values: [0.029660765081644058, 0.01431913860142231, 0.009122289717197418, -0.005111946724355221, 0.003936827182769775]\n",
      "i=107: values: [0.029660765081644058, 0.01431913860142231, 0.009122289717197418, -0.005111946724355221, 0.003936827182769775]\n",
      "i=108: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=108: keys: [-9.530216217041016, -9.366204261779785, -4.087751388549805, -6.575412273406982, -0.2026996910572052]\n",
      "i=108: keys: [-9.530216217041016, -9.366204261779785, -4.087751388549805, -6.575412273406982, -0.2026996910572052]\n",
      "i=108: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=108: values: [0.0024774563498795033, 0.021877046674489975, -0.018245941027998924, -0.015633825212717056, -0.005730719305574894]\n",
      "i=108: values: [0.0024774563498795033, 0.021877046674489975, -0.018245941027998924, -0.015633825212717056, -0.005730719305574894]\n",
      "i=109: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=109: keys: [-0.496978759765625, -8.613722801208496, -1.0216500759124756, -6.7379889488220215, -0.1891113817691803]\n",
      "i=109: keys: [-0.496978759765625, -8.613722801208496, -1.0216500759124756, -6.7379889488220215, -0.1891113817691803]\n",
      "i=109: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=109: values: [0.004640286788344383, 0.01272352784872055, 0.010242496617138386, -0.017828021198511124, -0.00039121881127357483]\n",
      "i=109: values: [0.004640286788344383, 0.01272352784872055, 0.010242496617138386, -0.017828021198511124, -0.00039121881127357483]\n",
      "i=110: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=110: keys: [8.959752082824707, -3.4809703826904297, 2.5904107093811035, -6.808462142944336, 0.21883006393909454]\n",
      "i=110: keys: [8.959752082824707, -3.4809703826904297, 2.5904107093811035, -6.808462142944336, 0.21883006393909454]\n",
      "i=110: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=110: values: [-0.04206909239292145, -0.006189282983541489, 0.046420324593782425, 0.014199761673808098, -0.005290590226650238]\n",
      "i=110: values: [-0.04206909239292145, -0.006189282983541489, 0.046420324593782425, 0.014199761673808098, -0.005290590226650238]\n",
      "i=111: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=111: keys: [9.38316535949707, 2.3901796340942383, 5.861854553222656, -6.191394329071045, -0.19905440509319305]\n",
      "i=111: keys: [9.38316535949707, 2.3901796340942383, 5.861854553222656, -6.191394329071045, -0.19905440509319305]\n",
      "i=111: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=111: values: [-0.008110578171908855, -0.01199924387037754, 0.011179585941135883, -0.018040046095848083, 0.004477271810173988]\n",
      "i=111: values: [-0.008110578171908855, -0.01199924387037754, 0.011179585941135883, -0.018040046095848083, 0.004477271810173988]\n",
      "i=112: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=112: keys: [1.9643819332122803, 7.587276935577393, 7.688709735870361, -4.577970027923584, -0.16823986172676086]\n",
      "i=112: keys: [1.9643819332122803, 7.587276935577393, 7.688709735870361, -4.577970027923584, -0.16823986172676086]\n",
      "i=112: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=112: values: [-0.0032774638384580612, 0.004580378532409668, 0.005415184423327446, -0.00902556162327528, 0.0007030554115772247]\n",
      "i=112: values: [-0.0032774638384580612, 0.004580378532409668, 0.005415184423327446, -0.00902556162327528, 0.0007030554115772247]\n",
      "i=113: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=113: keys: [-7.744150161743164, 9.442644119262695, 7.300665855407715, -2.1814026832580566, -0.0739041119813919]\n",
      "i=113: keys: [-7.744150161743164, 9.442644119262695, 7.300665855407715, -2.1814026832580566, -0.0739041119813919]\n",
      "i=113: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=113: values: [-0.010293555445969105, 0.0011192201636731625, 0.007892688736319542, -0.004882361274212599, -0.020949238911271095]\n",
      "i=113: values: [-0.010293555445969105, 0.0011192201636731625, 0.007892688736319542, -0.004882361274212599, -0.020949238911271095]\n",
      "i=114: keys: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=114: keys: [-10.62864875793457, 8.122270584106445, 6.889054298400879, -0.6893782019615173, 0.07415860891342163]\n",
      "i=114: keys: [-10.62864875793457, 8.122270584106445, 6.889054298400879, -0.6893782019615173, 0.07415860891342163]\n",
      "i=114: values: torch.Size([2, 2, 116, 64]), torch.Size([2, 2, 115, 64])\n",
      "i=114: values: [0.012969828210771084, -0.03826059401035309, -0.008268263190984726, -0.0009136066073551774, 0.007700621150434017]\n",
      "i=114: values: [0.012969828210771084, -0.03826059401035309, -0.008268263190984726, -0.0009136066073551774, 0.007700621150434017]\n"
     ]
    }
   ],
   "source": [
    "for kv1, kv2 in zip(past_key_values, gen_out.past_key_values):\n",
    "    for i in range(min(kv1[0].shape[2], kv2[0].shape[2])):\n",
    "        print(f\"{i=}: keys: {kv1[0].shape}, {kv2[0].shape}\")\n",
    "        print(f\"{i=}: keys: {kv1[0][0, 0, i, :5].tolist()}\")\n",
    "        print(f\"{i=}: keys: {kv2[0][0, 0, i, :5].tolist()}\")\n",
    "        print(f\"{i=}: values: {kv1[1].shape}, {kv2[1].shape}\")\n",
    "        print(f\"{i=}: values: {kv1[1][0, 0, i, :5].tolist()}\")\n",
    "        print(f\"{i=}: values: {kv2[1][0, 0, i, :5].tolist()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits\n",
      "past_key_values\n"
     ]
    }
   ],
   "source": [
    "for k in dummy_model(**dummy_questions_inputs, return_dict=True).keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny-zero-1",
   "language": "python",
   "name": "tiny-zero-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
