{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=7\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/80/anya/anaconda3/envs/tiny-zero-1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from copy import copy\n",
    "import itertools\n",
    "import os, sys\n",
    "import yaml\n",
    "import json\n",
    "import gc\n",
    "import argparse\n",
    "import functools\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import time\n",
    "import random\n",
    "\n",
    "sys.path.append(os.path.abspath('/homes/80/anya/Documents/llm_tiny_ideas/super-tiny-lms-outer/super-tiny-lms'))\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# rank = int(os.environ[\"RANK\"])\n",
    "# print(rank)\n",
    "# world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "# print(world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # to access a dict with object.key\n",
    "    def __init__(self, dictionary):\n",
    "        self.__dict__ = dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constant_schedule_with_warmup(\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    num_warmup_steps: int,\n",
    "    last_epoch: int = -1,\n",
    "):\n",
    "    def lr_lambda(current_step):\n",
    "        return min(1, float(current_step) / float(max(1, num_warmup_steps)))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(dataset: Dataset, per_device_batch_size: int, rank : int, world_size : int, seed: int = 0):\n",
    "    dataset_length = len(dataset)\n",
    "    total_batch_size = per_device_batch_size * world_size\n",
    "    batches_per_dataset = dataset_length // total_batch_size\n",
    "    i = 0\n",
    "    epochs = 0\n",
    "    while True:\n",
    "        if i >= batches_per_dataset:\n",
    "            dataset = dataset.shuffle(seed + epochs)\n",
    "            i = 0\n",
    "            epochs += 1\n",
    "        start = i * total_batch_size + rank * per_device_batch_size\n",
    "        batch = dataset[start:start + per_device_batch_size]\n",
    "        yield batch\n",
    "\n",
    "def careful_repeat(data, num_repeats):\n",
    "    batch_size = data[list(data.keys())[0]].shape[0]\n",
    "    for k, v in data.items():\n",
    "        if v.ndim == 1:\n",
    "            data[k] = v.unsqueeze(1).repeat(1, num_repeats).reshape(batch_size*num_repeats, *v.shape[1:])\n",
    "        elif v.ndim == 2:\n",
    "            data[k] = v.unsqueeze(1).repeat(1, num_repeats, 1).reshape(batch_size*num_repeats, *v.shape[1:])\n",
    "    return data\n",
    "\n",
    "def get_model_param_stats(model, ref_model):\n",
    "    model_params = torch.cat([p.view(-1) for p in model.parameters() if p.requires_grad])\n",
    "    ref_model_params = torch.cat([p.view(-1) for p in ref_model.parameters()])\n",
    "    assert model_params.shape == ref_model_params.shape, f\"{model_params.shape=} {ref_model_params.shape=}\"\n",
    "    return {\n",
    "        \"params_with_grads_mean\": model_params.mean().item(),\n",
    "        \"params_with_grads_std\": model_params.std().item(),\n",
    "        \"distance_to_ref\": torch.nn.functional.mse_loss(model_params, ref_model_params).item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'dataset': 'gsm8k', 'base_model': 'Qwen/Qwen2.5-0.5b', 'max_iters': 20, 'total_batch_size': 8, 'per_device_batch_size': 4, 'lr': '1e-4', 'loss_type': 'sft', 'sft': {'include_cot': True, 'predict_cot': True}, 'generation': {'generations_per_prompt': 2, 'temperature': 1.0, 'top_k': None, 'max_length': 20, 'inject_answer_prompt': True, 'fixed_length': True}, 'pg': {'normalization_type': None}, 'entropy_coef': 0.001, 'kl_loss_coef': 0.001, 'kl_loss_type': 'low_var_kl'}\n"
     ]
    }
   ],
   "source": [
    "config_file = \"../args/full_test.yaml\"\n",
    "\n",
    "### Load config\n",
    "with open(config_file) as f:\n",
    "        config_dict = yaml.safe_load(f)\n",
    "print(\"Config:\", config_dict)\n",
    "config = Config(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.load_from_disk(f\"../data/my_data/gsm8k/test.bin\")\n",
    "test_model = AutoModelForCausalLM.from_pretrained(config.base_model).to(device)\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "\n",
    "test_tokenizer.encode(\"....\")\n",
    "\n",
    "get_model_param_stats(test_model, test_model)\n",
    "print(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers\\' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers\\' market? Let\\'s think step by step and output the final answer after \"Answer:\".',\n",
       " 'reasoning': 'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n',\n",
       " 'answer': '18'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "    ) -> None:\n",
    "        print(f\"\\nTrainer::-----------------------------------\")\n",
    "        self.world_size = 1\n",
    "        self.rank = 0\n",
    "        self.total_iters = cfg.total_iters\n",
    "        self.total_batch_size = cfg.total_batch_size\n",
    "        self.per_device_batch_size = cfg.per_device_batch_size\n",
    "        assert self.total_batch_size % (self.per_device_batch_size * self.world_size) == 0, f\"{self.total_batch_size=} {self.per_device_batch_size=}, {self.world_size=}\"\n",
    "        self.gradient_accumulation_steps = self.total_batch_size // (self.per_device_batch_size * self.world_size)\n",
    "        assert self.per_device_batch_size * self.world_size * self.gradient_accumulation_steps == self.total_batch_size, f\"{self.per_device_batch_size=} {self.world_size=} {self.gradient_accumulation_steps=} {self.total_batch_size=}\"\n",
    "        self.generations_per_prompt = cfg.generations_per_prompt\n",
    "        assert self.per_device_batch_size % self.generations_per_prompt == 0, f\"{self.per_device_batch_size=} {self.generations_per_prompt=}\"\n",
    "        self.per_device_prompt_batch_size = self.per_device_batch_size // self.generations_per_prompt\n",
    "        print(f\"---TRAINING CONFIG:\")\n",
    "        print(f\"Total iters: {self.total_iters}\")\n",
    "        print(f\"Total batch size: {self.total_batch_size}\")\n",
    "        print(f\"Per device batch size: {self.per_device_batch_size}\")\n",
    "        print(f\"Gradient accumulation steps: {self.gradient_accumulation_steps}\")\n",
    "        print(f\"Generations per prompt: {self.generations_per_prompt}\")\n",
    "        print(f\"Per device prompt batch size: {self.per_device_prompt_batch_size}\")\n",
    "        print(f\"-----------------------------------\\n\")\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(cfg.model_name).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "        self.ref_model = AutoModelForCausalLM.from_pretrained(cfg.model_name).to(device)\n",
    "        for param in self.ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        assert torch.cuda.is_available(), \"CUDA must be available for training\"\n",
    "        assert torch.cuda.device_count() == 1\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            betas=cfg.get('betas', (0.9, 0.999)),\n",
    "            weight_decay=cfg.get('weight_decay', 1e-2))\n",
    "        self.lr_scheduler = get_constant_schedule_with_warmup(\n",
    "            optimizer=self.optimizer,\n",
    "            num_warmup_steps=cfg.max_iters * cfg.get('lr_warmup_steps_ratio', 0.0))\n",
    "\n",
    "        # dataset\n",
    "        if cfg.dataset == \"gsm8k\":\n",
    "            train_dataset = Dataset.load_from_disk(f\"../data/my_data/gsm8k/train.bin\")\n",
    "            if cfg.dataset_size is not None:\n",
    "                train_dataset = train_dataset.select(range(cfg.dataset_size))\n",
    "            val_dataset = Dataset.load_from_disk(f\"../data/my_data/gsm8k/trest.bin\")\n",
    "        self.train_loader = dataset_loader(train_dataset, self.per_device_batch_size, 0, 1, seed=cfg.get('seed', 0))\n",
    "        self.val_loader = dataset_loader(val_dataset, self.per_device_batch_size, 0, 1, seed=cfg.get('seed', 0))\n",
    "        self.dataset_answer_prompt = train_dataset[0][\"search_string\"]\n",
    "        print(f\"---DATASET CONFIG:\")\n",
    "        print(f\"Dataset: {cfg.dataset}\")\n",
    "        if cfg.dataset_size is not None:\n",
    "            print(f\"Careful! Reduced dataset size for testing: {len(train_dataset)}\")\n",
    "        print(f\"-----------------------------------\\n\")\n",
    "\n",
    "\n",
    "        # generation\n",
    "        if self.loss_type in [\"pg\", \"logp\"]:\n",
    "            self.temperature = cfg.generation.temperature\n",
    "            self.top_k = cfg.generation.top_k\n",
    "            self.max_steps = cfg.generation.max_steps\n",
    "            self.step_for_answer = cfg.generation.step_for_answer\n",
    "            self.inject_answer_prompt = cfg.generation.inject_answer_prompt\n",
    "            self.fixed_length = cfg.generation.fixed_length\n",
    "            self.as_full_distribution = cfg.generation.as_full_distribution\n",
    "            self.answer_prompt_text = \"Answer:\"\n",
    "            assert len(self.tokenizer.encode(self.answer_prompt_text)) == 1\n",
    "            self.answer_prompt_id = self.tokenizer.encode(self.answer_prompt_text)[0]\n",
    "            assert len(self.tokenizer.encode(\"....\")) == 1\n",
    "            self.dot_by_dot_id = self.tokenizer.encode(\"....\")[0]\n",
    "            print(f\"---GENERATION CONFIG:\")\n",
    "            print(f\"Temperature: {self.temperature}\")\n",
    "            print(f\"Top k: {self.top_k}\")\n",
    "            print(f\"Max length: {self.max_length}\")\n",
    "            print(f\"Inject answer prompt: {self.inject_answer_prompt}\")\n",
    "            print(f\"Fixed length: {self.fixed_length}\")\n",
    "            print(f\"Answer prompt text: {self.answer_prompt_text}\")\n",
    "            print(f\"Answer prompt id: {self.answer_prompt_id}\")\n",
    "            print(f\"-----------------------------------\\n\")\n",
    "\n",
    "        # loss\n",
    "        self.loss_type = cfg.loss_type\n",
    "        if self.loss_type == \"sft\":\n",
    "            self.sft_include_cot = cfg.sft.include_cot\n",
    "            self.sft_predict_cot = cfg.sft.predict_cot\n",
    "        elif self.loss_type == \"pg\":\n",
    "            self.pg_normalization_type = None if self.generations_per_prompt == 1 else cfg.pg.normalization_type\n",
    "            self.pg_dot_by_dot = cfg.pg.dot_by_dot\n",
    "        self.entropy_coef = cfg.entropy_coef\n",
    "        self.kl_loss_coef = cfg.kl_loss_coef\n",
    "        self.kl_loss_type = cfg.kl_loss_type\n",
    "        print(f\"---LOSS CONFIG:\")\n",
    "        print(f\"Loss type: {self.loss_type}\")\n",
    "        if self.loss_type == \"sft\":\n",
    "            print(f\"SFT include cot: {self.sft_include_cot}\")\n",
    "            print(f\"SFT predict cot: {self.sft_predict_cot}\")\n",
    "        elif self.loss_type == \"pg\":\n",
    "            print(f\"PG normalization type: {self.pg_normalization_type}\")\n",
    "            print(f\"PG dot by dot: {self.pg_dot_by_dot}\")\n",
    "        print(f\"Entropy coef: {self.entropy_coef}\")\n",
    "        print(f\"KL loss coef: {self.kl_loss_coef}\")\n",
    "        print(f\"KL loss type: {self.kl_loss_type}\")\n",
    "        print(f\"-----------------------------------\\n\")\n",
    "\n",
    "        self._setup_ctx()\n",
    "\n",
    "    def _setup_ctx(self):\n",
    "        \"\"\"Get the context manager\"\"\"\n",
    "        dtype = (\n",
    "            torch.bfloat16\n",
    "            if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "            else torch.float16\n",
    "        )\n",
    "        self._setup_scaler(dtype)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        ctx = torch.amp.autocast(device_type=\"cuda\", dtype=dtype)\n",
    "        return ctx\n",
    "\n",
    "    def _setup_scaler(self, dtype=torch.float16):\n",
    "        \"\"\"Setup the scaler\"\"\"\n",
    "        self.scaler = torch.amp.GradScaler(enabled=dtype == torch.float16, device='cuda')\n",
    "\n",
    "    def apply_update(self):\n",
    "        # once gradients are accumulated, step \n",
    "        if self.cfg.trainer.optimizer.grad_clip > 0:\n",
    "            # Unscale the gradients of the optimizer's assigned params in-place\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            # Clip the gradients with normalization\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.trainer.optimizer.grad_clip)\n",
    "        # Perform a single optimization step\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        self.optimizer.zero_grad()  # Reset gradients after update\n",
    "\n",
    "\n",
    "    def run_training_loop(self):\n",
    "        \"\"\"Run the training loop\"\"\"\n",
    "        start_time = time.time()\n",
    "        for i in tqdm.tqdm(range(self.total_iters), desc=\"Training\"):\n",
    "            lr = self.lr_scheduler.step(self.optimizer, i) if self.lr_scheduler is not None else self.optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            # GRADIENT ACCUMULATION\n",
    "            for j in range(self.gradient_accumulation_steps):\n",
    "            \n",
    "                # GENERATE ROLLOUTS\n",
    "                start_time = time.time()\n",
    "                self.model.eval()\n",
    "                # with torch.no_grad():\n",
    "                # get questions (and answers)\n",
    "                dataset_batch = next(self.train_loader)\n",
    "                questions_text = dataset_batch[\"questions\"]\n",
    "                cot_text = dataset_batch[\"cot\"]\n",
    "                answers_text = dataset_batch[\"answers\"]\n",
    "\n",
    "                if self.loss_type == \"sft\":\n",
    "                    raise NotImplementedError\n",
    "                \n",
    "                else:\n",
    "                    questions_inputs = self.tokenizer(questions_text, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "                    answers_inputs = self.tokenizer(answers_text, return_tensors=\"pt\", padding=True)\n",
    "                    questions_inputs = {k: v.to(self.model.device) for k, v in questions_inputs.items()}\n",
    "                    answers_inputs = {k: v.to(self.model.device) for k, v in answers_inputs.items()}\n",
    "\n",
    "                    # repeat for generations_per_prompt\n",
    "                    questions_inputs = careful_repeat(questions_inputs, self.generations_per_prompt)\n",
    "                    answers_inputs = careful_repeat(answers_inputs, self.generations_per_prompt)\n",
    "\n",
    "                    # generate\n",
    "                    x, generations, generation_metrics = batch_generate_rnn(\n",
    "                        model=self.model,\n",
    "                        ref_model=self.ref_model,\n",
    "                        questions_inputs=questions_inputs,\n",
    "                        answers_inputs=answers_inputs,\n",
    "                        max_steps=self.max_length,\n",
    "                        step_for_answer=self.step_for_answer,\n",
    "                        temperature=self.temperature,\n",
    "                        top_k=self.top_k,\n",
    "                        as_full_distribution=self.as_full_distribution,\n",
    "                        dot_by_dot=self.pg_dot_by_dot,\n",
    "                        dot_by_dot_id=self.dot_by_dot_id,\n",
    "                        inject_answer_prompt=self.inject_answer_prompt,\n",
    "                        answer_prompt_ids=self.answer_prompt_id,\n",
    "                    )\n",
    "                    generation_metrics = {f\"gen/{k}\": v for k, v in generation_metrics.items()}\n",
    "\n",
    "                    ### rewards\n",
    "                    rewards, normalized_rewards, reward_metrics = self.get_rewards(generations, answers_text)\n",
    "                    reward_metrics = {k if k == \"REWARD\" else f\"reward/{k}\": v for k, v in reward_metrics.items()}\n",
    "                    \n",
    "                # COMPUTE LOSS\n",
    "                with self.ctx: \n",
    "                    loss, loss_metrics = self.get_loss(x, normalized_rewards)\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                loss_metrics = {f\"loss/{k}\": v for k, v in loss_metrics.items()}\n",
    "\n",
    "                # UPDATE METRICS\n",
    "                if j == 0:\n",
    "                    metrics_s = {**generation_metrics, **loss_metrics, **reward_metrics}\n",
    "                else:\n",
    "                    metrics = {**generation_metrics, **loss_metrics, **reward_metrics}\n",
    "                    metrics_s = {k: v + metrics[k] for k, v in metrics_s.items()}\n",
    "\n",
    "            # UPDATE MODEL\n",
    "            self.apply_update()\n",
    "\n",
    "            # LOG\n",
    "            metrics_s = {k: v / self.gradient_accumulation_steps for k, v in metrics_s.items()}\n",
    "            param_metrics = get_model_param_stats(self.model, self.ref_model)\n",
    "            log_dict = {\"iter\": i, \"lr\": lr}\n",
    "            log_dict.update(metrics_s)\n",
    "            log_dict.update({f\"params/{k}\": v for k, v in param_metrics.items()})\n",
    "            # if self.use_wandb:\n",
    "            #     wandb.log(log_dict)\n",
    "            if i % 10 == 0 or i == self.total_iters - 1:\n",
    "                print(f\"({log_dict})\\n\\niter {i}: REWARD={log_dict['REWARD']:.2f}\")\n",
    "                num_to_print = 3\n",
    "                decoded = [self.tokenizer.decode(generations[k]) for k in range(num_to_print)]\n",
    "                for k in range(num_to_print):\n",
    "                    print(f\"  EXAMPLE {k}: (REWARD={rewards[k].item()}):\\nQUESTION: {questions_text[k]}\\nGENERATION: {decoded[k]}\\nANSWER: {answers_text[k]}\\n\", \"-\"*50, \"\\n\")\n",
    "\n",
    "            \n",
    "        print(f\"Training time: {time.time()-start_time:.1f}s\")\n",
    "            \n",
    "        # save the final model\n",
    "        self._save_model(i, final_model=True)\n",
    "\n",
    "    def train(self, seed=42):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        # set seed\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        self.run_training_loop()\n",
    "\n",
    "    def get_rewards(self, generations, answers_text):\n",
    "        \"\"\"Get rewards\"\"\"\n",
    "        ### check for answer\n",
    "        contains_answer_prompt = torch.zeros((self.per_device_prompt_batch_size, self.generations_per_prompt), device=self.model.device)\n",
    "        contains_answer = torch.zeros((self.per_device_prompt_batch_size, self.generations_per_prompt), device=self.model.device)\n",
    "        generations = generations.reshape(self.per_device_prompt_batch_size, self.generations_per_prompt, -1)\n",
    "        for i in range(self.per_device_prompt_batch_size):\n",
    "            answer_text = answers_text[i]\n",
    "            decoded_generations = self.tokenizer.batch_decode(generations[i])\n",
    "            for j in range(self.generations_per_prompt):\n",
    "                decoded = decoded_generations[j]\n",
    "                contains_answer_prompt_ij = self.answer_prompt_text in decoded\n",
    "                if contains_answer_prompt_ij:\n",
    "                    contains_answer_ij = answer_text in decoded.split(self.answer_prompt_text)[1]\n",
    "                else:\n",
    "                    contains_answer_ij = False\n",
    "                contains_answer_prompt[i, j] = contains_answer_prompt_ij\n",
    "                contains_answer[i, j] = contains_answer_ij\n",
    "        \n",
    "        ### caluclate rewards\n",
    "        rewards = contains_answer.float()\n",
    "        if not self.inject_answer_prompt:\n",
    "            rewards += self.answer_prompt_coef * contains_answer_prompt.float()\n",
    "        if self.pg_normalization_type == \"grpo\":\n",
    "            normalized_rewards = (rewards - rewards.mean(1, keepdim=True)) / (rewards.std(1, keepdim=True) + 1e-6)\n",
    "        elif self.pg_normalization_type == \"rloo\":\n",
    "            group_sum = rewards.sum(1, keepdim=True)\n",
    "            normalized_rewards = (group_sum - rewards) / (self.generations_per_prompt - 1)\n",
    "        else:\n",
    "            normalized_rewards = rewards\n",
    "\n",
    "        metrics = {\n",
    "            \"REWARD\": rewards.mean().item(),\n",
    "            \"reward/reward_std\": rewards.std().item(),\n",
    "            \"reward/reward_std_within_q\": rewards.std(1).mean().item(),\n",
    "            \"reward/reward_std_between_q\": rewards.mean(1).std().item(),\n",
    "        }\n",
    "\n",
    "        rewards = rewards.reshape(-1)\n",
    "        normalized_rewards = normalized_rewards.reshape(-1)\n",
    "        return rewards, normalized_rewards, metrics\n",
    "    \n",
    "    def get_loss(self, x, rewards):\n",
    "        metrics = {}\n",
    "        pg_loss, logp_loss = 0, 0\n",
    "        if self.loss_type == \"sft\":\n",
    "            raise NotImplementedError\n",
    "        if self.as_full_distribution:\n",
    "            gen_answer_logp, ref_answer_logp = x\n",
    "            metrics[\"gen_answer_logp\"] = gen_answer_logp.mean().item()\n",
    "            metrics[\"ref_answer_logp\"] = ref_answer_logp.mean().item()\n",
    "            metrics[\"correct_answer_logp\"] = correct_answer_logp.mean().item()\n",
    "            metrics[\"correct_answer_p\"] = correct_answer_logp.exp().mean().item()\n",
    "            if self.loss_type == \"pg\":\n",
    "                pg_loss = - torch.exp(gen_answer_logp - gen_answer_logp.detach()) * rewards\n",
    "            elif self.loss_type == \"logp\":\n",
    "                logp_loss = - correct_answer_logp\n",
    "            else:\n",
    "                raise ValueError(f\"{self.loss_type=}\")\n",
    "            kl = torch.exp(ref_answer_logp - gen_answer_logp) - (ref_answer_logp - gen_answer_logp) - 1\n",
    "        elif self.loss_type == \"pg\":\n",
    "            gen_per_token_logps, ref_per_token_logps = x\n",
    "            metrics[\"gen_per_token_logp\"] = (gen_per_token_logps * loss_mask).sum().item() / loss_mask.sum().item()\n",
    "            metrics[\"ref_per_token_logp\"] = (ref_per_token_logps * loss_mask).sum().item() / loss_mask.sum().item()\n",
    "            assert self.loss_type == \"pg\", f\"{self.loss_type=}\"\n",
    "            pg_losses = - (torch.exp(gen_per_token_logps - gen_per_token_logps.detach()) * rewards.unsqueeze(1))\n",
    "            pg_loss = (pg_losses * loss_mask).sum(-1) / loss_mask.sum(-1)\n",
    "            kls = torch.exp(ref_per_token_logps - gen_per_token_logps) - (ref_per_token_logps - gen_per_token_logps) - 1\n",
    "            kl = (kls * loss_mask).mean(-1)\n",
    "        loss = pg_loss + logp_loss + self.kl_coef * kl\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        metrics[\"loss\"] = loss.item()\n",
    "        metrics[\"pg_loss\"] = pg_loss.mean().item() if self.loss_type == \"pg\" else 0\n",
    "        metrics[\"logp_loss\"] = logp_loss.mean().item() if self.loss_type == \"logp\" else 0\n",
    "        metrics[\"kl\"] = kl.mean().item()\n",
    "        return loss / self.gradient_accumulation_steps, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_model = AutoModelForCausalLM.from_pretrained(config.base_model).to(device)\n",
    "dummy_ref_model = AutoModelForCausalLM.from_pretrained(config.base_model).to(device)\n",
    "dummy_tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "dummy_dataset = Dataset.load_from_disk(f\"../data/my_data/gsm8k/test.bin\")\n",
    "dummy_loader = dataset_loader(dummy_dataset, per_device_batch_size=2, rank=0, world_size=1, seed=0)\n",
    "dummy_batch = next(dummy_loader)\n",
    "dummy_questions_text = dummy_batch[\"prompt\"]\n",
    "dummy_answers_text = dummy_batch[\"answer\"]\n",
    "dummy_questions_inputs = dummy_tokenizer(dummy_questions_text, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "dummy_answers_inputs = dummy_tokenizer(dummy_answers_text, return_tensors=\"pt\", padding=True)\n",
    "dummy_questions_inputs = {k: v.to(device) for k, v in dummy_questions_inputs.items()}\n",
    "dummy_answers_inputs = {k: v.to(device) for k, v in dummy_answers_inputs.items()}\n",
    "dummy_ref_model.eval()\n",
    "dummy_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(logitss, temperature=0.0, top_k=None):\n",
    "    \"\"\"Get the next token\"\"\"\n",
    "    batch_size, seq_length, vocab_size = logitss.shape\n",
    "    assert seq_length == 1\n",
    "    logitss = logitss.squeeze(1)\n",
    "    if temperature == 0.0:\n",
    "        token_ids = torch.argmax(logitss, dim=-1)\n",
    "        probdists = torch.zeros_like(logitss)\n",
    "        probdists[torch.arange(batch_size), token_ids] = 1.0\n",
    "        entropy = torch.zeros(batch_size).to(logitss.device)\n",
    "    else:\n",
    "        logitss = logitss / temperature\n",
    "        if top_k is not None:\n",
    "            logitss_k, idxs_k = torch.topk(logitss, min(top_k, vocab_size), dim=-1) # (batch_size, top_k)\n",
    "            probs_k = torch.nn.functional.softmax(logitss_k, dim=-1) # (batch_size, top_k)\n",
    "            idxs = torch.multinomial(probs_k, num_samples=1).squeeze(1) # (batch_size,)\n",
    "            token_ids = idxs_k[torch.arange(batch_size), idxs] # (batch_size)\n",
    "            probdists = torch.zeros_like(logitss)\n",
    "            # next_probdist_s[torch.arange(batch_size), idx_s_k.squeeze(1)] = probs_k.squeeze(1)\n",
    "            probdists.scatter_(1, idxs_k, probs_k)\n",
    "            if top_k == 1:\n",
    "                token_ids2 = torch.argmax(logitss, dim=-1)\n",
    "                probdists2 = torch.zeros_like(logitss)\n",
    "                probdists2[torch.arange(batch_size), token_ids2] = 1.0\n",
    "                assert (token_ids == token_ids2).all(), f\"{token_ids=}, {token_ids2=}\"\n",
    "                assert torch.allclose(probdists, probdists2), f\"{probdists2=}, {probdists2=}\"\n",
    "        else:\n",
    "            probdists = torch.nn.functional.softmax(logitss, dim=-1) # (batch_size, vocab_size)\n",
    "            token_ids = torch.multinomial(probdists, num_samples=1).squeeze(1) # (batch_size)\n",
    "        entropy = -torch.sum(probdists * torch.log(probdists), dim=-1)\n",
    "\n",
    "    token_ids = token_ids.unsqueeze(1)\n",
    "    probdists = probdists.unsqueeze(1)\n",
    "    return token_ids, probdists, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_step(model, next_input, attention_mask, position_ids, past_key_values, as_full_distribution=False):\n",
    "    batch_size = next_input.shape[0]\n",
    "    prev_seq_length = past_key_values[0][0].shape[2]\n",
    "    assert position_ids.shape == (batch_size, 1), f\"{position_ids.shape=}\"\n",
    "    assert attention_mask.shape == (batch_size, prev_seq_length+1), f\"{attention_mask.shape=}, {prev_seq_length=}\"\n",
    "\n",
    "    if as_full_distribution:\n",
    "        vocab_size = model.model.config.vocab_size\n",
    "        assert next_input.shape == (batch_size, 1, vocab_size)\n",
    "        all_embeds = model.model.embed_tokens.weight\n",
    "        hidden_dim = all_embeds.shape[1]\n",
    "        assert all_embeds.shape[0] == vocab_size\n",
    "        inputs_embeds = torch.matmul(next_input, all_embeds)\n",
    "        assert inputs_embeds.shape == (batch_size, 1, hidden_dim)\n",
    "        outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids=position_ids, past_key_values=past_key_values)\n",
    "    else:\n",
    "        assert next_input.shape == (batch_size, 1)\n",
    "        outputs = model(input_ids=next_input, attention_mask=attention_mask, position_ids=position_ids, past_key_values=past_key_values)\n",
    "    logits = outputs.logits\n",
    "    past_key_values = outputs.past_key_values\n",
    "    return logits, past_key_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "def batch_generate_rnn(\n",
    "        model,\n",
    "        ref_model,\n",
    "        questions_inputs,\n",
    "        answers_inputs,\n",
    "        max_steps=30,\n",
    "        step_for_answer=20,\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "        as_full_distribution=False,\n",
    "        dot_by_dot=False,\n",
    "        dot_by_dot_id=None,\n",
    "        inject_answer_prompt=False,\n",
    "        answer_prompt_ids=None,\n",
    "    ):\n",
    "    metrics = {}\n",
    "    assert not (as_full_distribution and dot_by_dot), f\"{as_full_distribution=}, {dot_by_dot=}\"\n",
    "    device = model.device\n",
    "    assert device.type == \"cuda\", f\"{model.device=}\"\n",
    "    assert ref_model.device == device, f\"{ref_model.device=}, {device=}\"\n",
    "    model.eval()\n",
    "    ref_model.eval()\n",
    "    batch_size = questions_inputs[\"input_ids\"].shape[0]\n",
    "    vocab_size = model.config.vocab_size\n",
    "    prompt_length = questions_inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    #### PROMPT FORWARD PASS\n",
    "    position_ids = questions_inputs[\"attention_mask\"].cumsum(dim=1) - 1\n",
    "    outputs = model(**questions_inputs, position_ids=position_ids)\n",
    "    with torch.no_grad():\n",
    "        ref_outputs = ref_model(**questions_inputs)\n",
    "    prompt_attention_mask = questions_inputs[\"attention_mask\"]\n",
    "    prompt_end_position_ids = position_ids[:, -1:] + 1\n",
    "    past_key_values = outputs.past_key_values\n",
    "    ref_past_keys_values = ref_outputs.past_key_values\n",
    "    logits = outputs.logits[:, -1:]\n",
    "    ref_logits = ref_outputs.logits[:, -1:]\n",
    "\n",
    "    def make_attention_mask(t, new_seq_length=1):\n",
    "        return torch.cat([prompt_attention_mask, torch.ones((batch_size, t+new_seq_length), device=device)], dim=1)\n",
    "    def make_position_ids(t, new_seq_length=1):\n",
    "        if new_seq_length == 1:\n",
    "            return prompt_end_position_ids + t\n",
    "        else:\n",
    "            return prompt_end_position_ids + t + torch.arange(new_seq_length).unsqueeze(0).to(device)\n",
    "\n",
    "    all_gen_logits = torch.zeros((batch_size, 0, vocab_size), device=device)\n",
    "    all_ref_logits = torch.zeros((batch_size, 0, vocab_size), device=device)\n",
    "    generations = torch.zeros((batch_size, 0), device=device, dtype=torch.int)\n",
    "    generation_mask = torch.ones((batch_size, 0), device=device)\n",
    "    entropy = torch.zeros((batch_size,), device=device)\n",
    "\n",
    "    ### REASONING FORWARD PASSES\n",
    "    for t in range(step_for_answer):\n",
    "        next_token_ids, next_probdists, step_entropy = get_next(logits, temperature=temperature, top_k=top_k)\n",
    "        all_gen_logits = torch.cat((all_gen_logits, logits), dim=1)\n",
    "        all_ref_logits = torch.cat((all_ref_logits, ref_logits), dim=1)\n",
    "        generations = torch.cat((generations, next_token_ids), dim=1)\n",
    "        generation_mask = torch.cat((generation_mask, torch.ones((batch_size, 1), device=device)), dim=1)\n",
    "        entropy += step_entropy\n",
    "        # forward pass of next token\n",
    "        if as_full_distribution:\n",
    "            next_input = next_probdists\n",
    "        elif dot_by_dot:\n",
    "            next_input = torch.full((batch_size, 1), dot_by_dot_id, dtype=torch.long, device=device)\n",
    "        else:\n",
    "            next_input = next_token_ids\n",
    "        logits, past_key_values = single_step(model,\n",
    "            next_input=next_input,\n",
    "            attention_mask=make_attention_mask(t),\n",
    "            position_ids=make_position_ids(t),\n",
    "            past_key_values=past_key_values,\n",
    "            as_full_distribution=as_full_distribution)\n",
    "        with torch.no_grad():\n",
    "            ref_logits, ref_past_keys_values = single_step(ref_model,\n",
    "                next_input=next_input,\n",
    "                attention_mask=make_attention_mask(t),\n",
    "                position_ids=make_position_ids(t),\n",
    "                past_key_values=ref_past_keys_values,\n",
    "                as_full_distribution=as_full_distribution)\n",
    "            \n",
    "    ### INJECT ANSWER PROMPT FORWARD PASS\n",
    "    t = step_for_answer\n",
    "    if inject_answer_prompt:\n",
    "        repeated_answer_prompt_ids = torch.tensor(answer_prompt_ids).unsqueeze(0).repeat(batch_size, 1).to(next_token_ids.dtype).to(device)\n",
    "        all_gen_logits = torch.cat((all_gen_logits, torch.zeros((batch_size, len(answer_prompt_ids), vocab_size), device=device)), dim=1)\n",
    "        all_ref_logits = torch.cat((all_ref_logits, torch.zeros((batch_size, len(answer_prompt_ids), vocab_size), device=device)), dim=1)\n",
    "        generations = torch.cat((generations, repeated_answer_prompt_ids), dim=1)\n",
    "        generation_mask = torch.cat((generation_mask, torch.zeros((batch_size, len(answer_prompt_ids)), device=device)), dim=1)\n",
    "        outputs = model(\n",
    "            input_ids=repeated_answer_prompt_ids,\n",
    "            attention_mask=make_attention_mask(step_for_answer, new_seq_length=len(answer_prompt_ids)),\n",
    "            position_ids=make_position_ids(step_for_answer, new_seq_length=len(answer_prompt_ids)),\n",
    "            past_key_values=past_key_values)\n",
    "        with torch.no_grad():\n",
    "            ref_outputs = ref_model(\n",
    "                input_ids=repeated_answer_prompt_ids,\n",
    "                attention_mask=make_attention_mask(t, new_seq_length=len(answer_prompt_ids)),\n",
    "                position_ids=make_position_ids(t, new_seq_length=len(answer_prompt_ids)),\n",
    "                past_key_values=ref_past_keys_values)\n",
    "        logits = outputs.logits\n",
    "        ref_logits = ref_outputs.logits\n",
    "        past_key_values = outputs.past_key_values\n",
    "        ref_past_keys_values = ref_outputs.past_key_values\n",
    "        t += len(answer_prompt_ids)\n",
    "        all_gen_logits = torch.cat((all_gen_logits, logits), dim=1)\n",
    "        all_ref_logits = torch.cat((all_ref_logits, ref_logits), dim=1)\n",
    "\n",
    "    ### ANSWER FORWARD PASS\n",
    "    if as_full_distribution:\n",
    "        answer_length = answers_inputs[\"input_ids\"].shape[1]\n",
    "        answer_logits = model(\n",
    "            input_ids=answers_inputs[\"input_ids\"][:, :-1],\n",
    "            attention_mask=make_attention_mask(t, new_seq_length=answer_length-1),\n",
    "            position_ids=make_position_ids(t, new_seq_length=answer_length-1),\n",
    "            past_key_values=past_key_values).logits\n",
    "        with torch.no_grad():\n",
    "            ref_answer_logits = ref_model(\n",
    "                input_ids=answers_inputs[\"input_ids\"][:, :-1],\n",
    "                attention_mask=make_attention_mask(t+1, new_seq_length=answer_length-1),\n",
    "                position_ids=make_position_ids(t+1, new_seq_length=answer_length-1),\n",
    "                past_key_values=ref_past_keys_values).logits\n",
    "        answer_logits = torch.cat((all_gen_logits[:, -1:], answer_logits), dim=1)\n",
    "        ref_answer_logits = torch.cat((all_ref_logits[:, -1:], ref_answer_logits), dim=1)\n",
    "        per_token_answer_logps = torch.gather(answer_logits, 2, answers_inputs[\"input_ids\"].to(torch.long).unsqueeze(-1)).squeeze(-1)\n",
    "        per_token_ref_answer_logps = torch.gather(ref_answer_logits, 2, answers_inputs[\"input_ids\"].to(torch.long).unsqueeze(-1)).squeeze(-1)\n",
    "        assert per_token_answer_logps.shape == answers_inputs[\"attention_mask\"].shape, f\"{per_token_answer_logps.shape=}, {answers_inputs['attention_mask'].shape=}\"\n",
    "        answer_logps = (per_token_answer_logps * answers_inputs[\"attention_mask\"]).sum(dim=-1)\n",
    "        ref_answer_logps = (per_token_ref_answer_logps * answers_inputs[\"attention_mask\"]).sum(dim=-1)\n",
    "        x = (answer_logps, ref_answer_logps)\n",
    "\n",
    "        answer_perplexity = torch.exp(-answer_logps / answers_inputs[\"attention_mask\"].sum(dim=-1))\n",
    "        answer_perplexity_ref = torch.exp(-ref_answer_logps / answers_inputs[\"attention_mask\"].sum(dim=-1))\n",
    "        metrics[\"answer_logps\"] = answer_logps.mean().item()\n",
    "        metrics[\"answer_logps_ref\"] = ref_answer_logps.mean().item()\n",
    "        metrics[\"answer_logps_diff\"] = (answer_logps - ref_answer_logps).mean().item()\n",
    "        metrics[\"answer_perplexity\"] = answer_perplexity.mean().item()\n",
    "        metrics[\"answer_perplexity_ref\"] = answer_perplexity_ref.mean().item()\n",
    "        metrics[\"answer_perplexity_diff\"] = (answer_perplexity - answer_perplexity_ref).mean().item()\n",
    "\n",
    "    ### CONTINUE FORWARD PASS STEPS\n",
    "    for t in range(t, max_steps):\n",
    "        next_token_ids, next_probdists, step_entropy = get_next(logits[:, -1:], temperature=temperature, top_k=top_k)\n",
    "        all_gen_logits = torch.cat((all_gen_logits, logits), dim=1)\n",
    "        all_ref_logits = torch.cat((all_ref_logits, ref_logits), dim=1)\n",
    "        generations = torch.cat((generations, next_token_ids), dim=1)\n",
    "        generation_mask = torch.cat((generation_mask, torch.ones((batch_size, 1), device=device)), dim=1)\n",
    "        entropy += step_entropy\n",
    "        # forward pass of next token\n",
    "        if as_full_distribution:\n",
    "            next_input = next_probdists\n",
    "        elif dot_by_dot:\n",
    "            next_input = torch.full((batch_size, 1), dot_by_dot_id, dtype=torch.long, device=device)\n",
    "        else:\n",
    "            next_input = next_token_ids\n",
    "        logits, past_key_values = single_step(model,\n",
    "            next_input=next_input,\n",
    "            attention_mask=make_attention_mask(t),\n",
    "            position_ids=make_position_ids(t),\n",
    "            past_key_values=past_key_values,\n",
    "            as_full_distribution=as_full_distribution)\n",
    "        with torch.no_grad():\n",
    "            ref_logits, ref_past_keys_values = single_step(ref_model,\n",
    "                next_input=next_input,\n",
    "                attention_mask=make_attention_mask(t),\n",
    "                position_ids=make_position_ids(t),\n",
    "                past_key_values=ref_past_keys_values,\n",
    "                as_full_distribution=as_full_distribution)\n",
    "            \n",
    "        all_gen_logits = torch.cat((all_gen_logits, logits), dim=1)\n",
    "        all_ref_logits = torch.cat((all_ref_logits, ref_logits), dim=1)\n",
    "    next_token_ids, next_probdists, step_entropy = get_next(logits, temperature=temperature, top_k=top_k)\n",
    "    all_gen_logits = torch.cat((all_gen_logits, logits), dim=1)\n",
    "    all_ref_logits = torch.cat((all_ref_logits, ref_logits), dim=1)\n",
    "    generations = torch.cat((generations, next_token_ids), dim=1)\n",
    "    generation_mask = torch.cat((generation_mask, torch.ones((batch_size, 1), device=device)), dim=1)\n",
    "    entropy += step_entropy\n",
    "\n",
    "    if not as_full_distribution:\n",
    "        gen_per_token_logps = torch.gather(all_gen_logits, 2, generations.to(torch.long).unsqueeze(-1)).squeeze(-1)\n",
    "        ref_per_token_logps = torch.gather(all_ref_logits, 2, generations.to(torch.long).unsqueeze(-1)).squeeze(-1)\n",
    "        if inject_answer_prompt:\n",
    "            gen_per_token_logps = gen_per_token_logps * generation_mask\n",
    "            ref_per_token_logps = ref_per_token_logps * generation_mask\n",
    "        x = (gen_per_token_logps, ref_per_token_logps)\n",
    "\n",
    "        metrics[\"logps\"] = gen_per_token_logps.mean().item()\n",
    "        metrics[\"logps_ref\"] = ref_per_token_logps.mean().item()\n",
    "        metrics[\"logps_diff\"] = (gen_per_token_logps - ref_per_token_logps).mean().item()\n",
    "\n",
    "    metrics = {\n",
    "        \"entropy\": entropy.mean().item() / max_steps,\n",
    "        \"entropy_std\": entropy.std().item() / max_steps,\n",
    "    }\n",
    "    \n",
    "    assert x[0].requires_grad == True, f\"{x[0].requires_grad=}\"\n",
    "    assert x[1].requires_grad == False, f\"{x[1].requires_grad=}\"\n",
    "    return x, generations, metrics, # past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 31]) torch.Size([2, 31]) {'entropy': 0.9281216939290364, 'entropy_std': 0.40576279958089195}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\" To find out how much Janet makes at the farmers' market daily from selling the remaining eggs, we can follow these steps:\\n\\n1. Calculate the total number\",\n",
       " ' To find the total number of bolts it takes, we need to first calculate the number of bolts of blue and white fiber required for the lord in each row']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, generations, metrics, past_key_values = batch_generate_rnn(\n",
    "    model=dummy_model,\n",
    "    ref_model=dummy_ref_model,\n",
    "    questions_inputs=dummy_questions_inputs,\n",
    "    answers_inputs=dummy_answers_inputs,\n",
    "    max_steps=30,\n",
    "    step_for_answer=20,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    as_full_distribution=False,\n",
    "    dot_by_dot=False,\n",
    "    dot_by_dot_id=dummy_tokenizer.encode(\"....\")[0],\n",
    "    inject_answer_prompt=False,\n",
    "    answer_prompt_ids=dummy_tokenizer.encode(\"...Answer:\"),\n",
    ")\n",
    "print(x[0].shape, generations.shape, metrics)\n",
    "dummy_tokenizer.batch_decode(generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" To determine how much Janet makes in dollars every day at the farmers' market, we need to follow these steps:\\n\\n1. Calculate the total number of\", ' To find the total number of bolts needed, we need to calculate the number of bolts required for blue and white fibers separately and then add them together.\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "gen_out = dummy_model.generate(\n",
    "    input_ids=dummy_questions_inputs[\"input_ids\"],\n",
    "    attention_mask=dummy_questions_inputs[\"attention_mask\"],\n",
    "    max_new_tokens=30,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    do_sample=False,\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "prompt_length = dummy_questions_inputs[\"input_ids\"].shape[1]\n",
    "generations = gen_out.sequences if isinstance(gen_out, dict) else gen_out\n",
    "generations = generations[:, prompt_length:]\n",
    "print(dummy_tokenizer.batch_decode(generations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=0: keys: [-8.410331726074219, -3.307109832763672, -6.438131332397461, 0.6713922619819641, -0.20497481524944305]\n",
      "i=0: keys: [-8.410331726074219, -3.307109832763672, -6.438131332397461, 0.6713922619819641, -0.20497481524944305]\n",
      "i=0: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=0: values: [-0.014144073240458965, 0.03389165922999382, -0.026016375049948692, 0.0130347590893507, -0.011828195303678513]\n",
      "i=0: values: [-0.014144073240458965, 0.03389165922999382, -0.026016375049948692, 0.0130347590893507, -0.011828195303678513]\n",
      "i=1: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=1: keys: [-9.8622407913208, -7.930610179901123, -7.341488361358643, 2.996467113494873, -0.14621634781360626]\n",
      "i=1: keys: [-9.8622407913208, -7.930610179901123, -7.341488361358643, 2.996467113494873, -0.14621634781360626]\n",
      "i=1: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=1: values: [0.010026690550148487, -0.028028609231114388, -0.011757636442780495, -0.02434108406305313, 0.013797059655189514]\n",
      "i=1: values: [0.010026690550148487, -0.028028609231114388, -0.011757636442780495, -0.02434108406305313, 0.013797059655189514]\n",
      "i=2: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=2: keys: [-2.134861707687378, -9.202947616577148, -6.99802303314209, 4.451529502868652, 0.030788470059633255]\n",
      "i=2: keys: [-2.134861707687378, -9.202947616577148, -6.99802303314209, 4.451529502868652, 0.030788470059633255]\n",
      "i=2: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=2: values: [0.03084884211421013, 0.0293425340205431, 0.00014420831575989723, -0.0016999389044940472, 0.003674454754218459]\n",
      "i=2: values: [0.03084884211421013, 0.0293425340205431, 0.00014420831575989723, -0.0016999389044940472, 0.003674454754218459]\n",
      "i=3: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=3: keys: [7.34650182723999, -7.313322067260742, -5.728172302246094, 5.3970136642456055, -0.17883670330047607]\n",
      "i=3: keys: [7.34650182723999, -7.313322067260742, -5.728172302246094, 5.3970136642456055, -0.17883670330047607]\n",
      "i=3: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=3: values: [0.002477454487234354, 0.021877041086554527, -0.018245944753289223, -0.015633821487426758, -0.005730717908591032]\n",
      "i=3: values: [0.002477454487234354, 0.021877041086554527, -0.018245944753289223, -0.015633821487426758, -0.005730717908591032]\n",
      "i=4: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=4: keys: [10.617332458496094, -1.9367518424987793, -3.1306264400482178, 6.344402313232422, -0.15504883229732513]\n",
      "i=4: keys: [10.617332458496094, -1.9367518424987793, -3.1306264400482178, 6.344402313232422, -0.15504883229732513]\n",
      "i=4: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=4: values: [0.0046402872540056705, 0.0127235297113657, 0.010242493823170662, -0.017828021198511124, -0.00039121275767683983]\n",
      "i=4: values: [0.0046402872540056705, 0.0127235297113657, 0.010242493823170662, -0.017828021198511124, -0.00039121275767683983]\n",
      "i=5: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=5: keys: [3.264904022216797, 4.938665390014648, 0.501471996307373, 7.364436626434326, 0.24105007946491241]\n",
      "i=5: keys: [3.264904022216797, 4.938665390014648, 0.501471996307373, 7.364436626434326, 0.24105007946491241]\n",
      "i=5: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=5: values: [-0.042069096118211746, -0.006189286708831787, 0.046420320868492126, 0.014199760742485523, -0.005290590226650238]\n",
      "i=5: values: [-0.042069096118211746, -0.006189286708831787, 0.046420320868492126, 0.014199760742485523, -0.005290590226650238]\n",
      "i=6: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=6: keys: [-5.913852214813232, 9.043824195861816, 4.001959323883057, 7.749185085296631, -0.1538846492767334]\n",
      "i=6: keys: [-5.913852214813232, 9.043824195861816, 4.001959323883057, 7.749185085296631, -0.1538846492767334]\n",
      "i=6: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=6: values: [-0.00811058096587658, -0.01199924573302269, 0.011179592460393906, -0.018040042370557785, 0.004477260634303093]\n",
      "i=6: values: [-0.00811058096587658, -0.01199924573302269, 0.011179592460393906, -0.018040042370557785, 0.004477260634303093]\n",
      "i=7: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=7: keys: [-10.08230209350586, 9.513842582702637, 6.560704708099365, 6.9967169761657715, -0.12185979634523392]\n",
      "i=7: keys: [-10.08230209350586, 9.513842582702637, 6.560704708099365, 6.9967169761657715, -0.12185979634523392]\n",
      "i=7: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=7: values: [-0.003277468727901578, 0.004580378532409668, 0.005415189545601606, -0.009025556966662407, 0.000703046505805105]\n",
      "i=7: values: [-0.003277468727901578, 0.004580378532409668, 0.005415189545601606, -0.009025556966662407, 0.000703046505805105]\n",
      "i=8: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=8: keys: [-5.636848449707031, 6.6046366691589355, 6.91240119934082, 4.8306565284729, -0.016098439693450928]\n",
      "i=8: keys: [-5.636848449707031, 6.6046366691589355, 6.91240119934082, 4.8306565284729, -0.016098439693450928]\n",
      "i=8: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=8: values: [-0.010293553583323956, 0.001119217835366726, 0.007892685942351818, -0.004882361274212599, -0.020949237048625946]\n",
      "i=8: values: [-0.010293553583323956, 0.001119217835366726, 0.007892685942351818, -0.004882361274212599, -0.020949237048625946]\n",
      "i=9: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=9: keys: [5.3835859298706055, 0.7549362182617188, 7.463035583496094, 3.767015218734741, 0.10842078924179077]\n",
      "i=9: keys: [5.3835859298706055, 0.7549362182617188, 7.463035583496094, 3.767015218734741, 0.10842078924179077]\n",
      "i=9: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=9: values: [0.012969829142093658, -0.038260601460933685, -0.008268265053629875, -0.0009136060252785683, 0.007700619287788868]\n",
      "i=9: values: [0.012969829142093658, -0.038260601460933685, -0.008268265053629875, -0.0009136060252785683, 0.007700619287788868]\n",
      "i=10: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=10: keys: [10.763646125793457, -4.919988632202148, 6.371460914611816, 1.875067949295044, 0.0540572851896286]\n",
      "i=10: keys: [10.763646125793457, -4.919988632202148, 6.371460914611816, 1.875067949295044, 0.0540572851896286]\n",
      "i=10: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=10: values: [-0.01906581223011017, -0.024128353223204613, -0.0009186347015202045, 0.007213250733911991, 0.0001918654888868332]\n",
      "i=10: values: [-0.01906581223011017, -0.024128353223204613, -0.0009186347015202045, 0.007213250733911991, 0.0001918654888868332]\n",
      "i=11: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=11: keys: [6.114045143127441, -9.139619827270508, 3.7421820163726807, 0.17141544818878174, 0.2844187021255493]\n",
      "i=11: keys: [6.114045143127441, -9.139619827270508, 3.7421820163726807, 0.17141544818878174, 0.2844187021255493]\n",
      "i=11: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=11: values: [-0.011283719912171364, -0.018521031364798546, 0.00846038106828928, -0.007884763181209564, 0.04855033755302429]\n",
      "i=11: values: [-0.011283719912171364, -0.018521031364798546, 0.00846038106828928, -0.007884763181209564, 0.04855033755302429]\n",
      "i=12: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=12: keys: [-3.8935818672180176, -9.283998489379883, 1.6449439525604248, -1.6225931644439697, 0.16541491448879242]\n",
      "i=12: keys: [-3.8935818672180176, -9.283998489379883, 1.6449439525604248, -1.6225931644439697, 0.16541491448879242]\n",
      "i=12: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=12: values: [0.024730192497372627, 0.037440843880176544, -0.008871843107044697, 0.0025019943714141846, -0.029923569411039352]\n",
      "i=12: values: [0.024730192497372627, 0.037440843880176544, -0.008871843107044697, 0.0025019943714141846, -0.029923569411039352]\n",
      "i=13: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=13: keys: [-10.539018630981445, -5.887822151184082, -1.6294116973876953, -3.378094434738159, 0.22543606162071228]\n",
      "i=13: keys: [-10.539018630981445, -5.887822151184082, -1.6294116973876953, -3.378094434738159, 0.22543606162071228]\n",
      "i=13: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=13: values: [0.015992650762200356, 0.01854320615530014, -0.01757153868675232, 0.0039506275206804276, -0.007819464430212975]\n",
      "i=13: values: [0.015992650762200356, 0.01854320615530014, -0.01757153868675232, 0.0039506275206804276, -0.007819464430212975]\n",
      "i=14: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=14: keys: [-7.820742607116699, 0.14271163940429688, -4.662928104400635, -5.031116008758545, 0.2962135970592499]\n",
      "i=14: keys: [-7.820742607116699, 0.14271163940429688, -4.662928104400635, -5.031116008758545, 0.2962135970592499]\n",
      "i=14: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=14: values: [0.004136819392442703, -0.007348422892391682, 0.018074089661240578, 0.011462254449725151, 0.0262130219489336]\n",
      "i=14: values: [0.004136819392442703, -0.007348422892391682, 0.018074089661240578, 0.011462254449725151, 0.0262130219489336]\n",
      "i=15: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=15: keys: [2.70102596282959, 6.356115341186523, -6.676074981689453, -6.42870569229126, 0.0827341303229332]\n",
      "i=15: keys: [2.70102596282959, 6.356115341186523, -6.676074981689453, -6.42870569229126, 0.0827341303229332]\n",
      "i=15: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=15: values: [5.544070154428482e-05, -0.007082168012857437, 0.005847190972417593, 0.023113764822483063, -0.004729418084025383]\n",
      "i=15: values: [5.544070154428482e-05, -0.007082168012857437, 0.005847190972417593, 0.023113764822483063, -0.004729418084025383]\n",
      "i=16: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=16: keys: [10.206549644470215, 9.196063995361328, -7.231802940368652, -6.415915012359619, 0.3072962760925293]\n",
      "i=16: keys: [10.206549644470215, 9.196063995361328, -7.231802940368652, -6.415915012359619, 0.3072962760925293]\n",
      "i=16: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=16: values: [-0.00814772117882967, -0.007580550387501717, -0.008460255339741707, 0.005298576317727566, -0.019745834171772003]\n",
      "i=16: values: [-0.00814772117882967, -0.007580550387501717, -0.008460255339741707, 0.005298576317727566, -0.019745834171772003]\n",
      "i=17: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=17: keys: [8.561214447021484, 8.479029655456543, -6.914931774139404, -6.858955383300781, 0.1709655523300171]\n",
      "i=17: keys: [8.561214447021484, 8.479029655456543, -6.914931774139404, -6.858955383300781, 0.1709655523300171]\n",
      "i=17: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=17: values: [-0.000817673746496439, -0.01397579163312912, -0.004544749855995178, 0.012583654373884201, 0.030712489038705826]\n",
      "i=17: values: [-0.000817673746496439, -0.01397579163312912, -0.004544749855995178, 0.012583654373884201, 0.030712489038705826]\n",
      "i=18: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=18: keys: [-0.34474849700927734, 5.015084266662598, -5.3558454513549805, -6.263848781585693, 0.25258684158325195]\n",
      "i=18: keys: [-0.34474849700927734, 5.015084266662598, -5.3558454513549805, -6.263848781585693, 0.25258684158325195]\n",
      "i=18: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=18: values: [-0.00845696683973074, -0.0015415139496326447, -0.019762715324759483, 0.023012099787592888, -0.032883960753679276]\n",
      "i=18: values: [-0.00845696683973074, -0.0015415139496326447, -0.019762715324759483, 0.023012099787592888, -0.032883960753679276]\n",
      "i=19: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=19: keys: [-9.801508903503418, -1.9160650968551636, -2.487841844558716, -6.035788536071777, -0.04806205630302429]\n",
      "i=19: keys: [-9.801508903503418, -1.9160650968551636, -2.487841844558716, -6.035788536071777, -0.04806205630302429]\n",
      "i=19: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=19: values: [0.011901716701686382, -0.0010180547833442688, 0.037717305123806, 0.0018865671008825302, 0.02024102956056595]\n",
      "i=19: values: [0.011901716701686382, -0.0010180547833442688, 0.037717305123806, 0.0018865671008825302, 0.02024102956056595]\n",
      "i=20: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=20: keys: [-9.763795852661133, -6.717891693115234, 0.436084508895874, -4.660233020782471, 0.15011419355869293]\n",
      "i=20: keys: [-9.763795852661133, -6.717891693115234, 0.436084508895874, -4.660233020782471, 0.15011419355869293]\n",
      "i=20: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=20: values: [-0.006898010149598122, -0.03339320793747902, 0.01888817548751831, -0.008966736495494843, 0.0008241422474384308]\n",
      "i=20: values: [-0.006898010149598122, -0.03339320793747902, 0.01888817548751831, -0.008966736495494843, 0.0008241422474384308]\n",
      "i=21: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=21: keys: [-1.3412609100341797, -9.111163139343262, 3.2505266666412354, -2.261319398880005, 0.26684117317199707]\n",
      "i=21: keys: [-1.3412609100341797, -9.111163139343262, 3.2505266666412354, -2.261319398880005, 0.26684117317199707]\n",
      "i=21: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=21: values: [-0.0032419830095022917, -0.03485526889562607, -0.022579075768589973, 0.0033591650426387787, -0.013552753254771233]\n",
      "i=21: values: [-0.0032419830095022917, -0.03485526889562607, -0.022579075768589973, 0.0033591650426387787, -0.013552753254771233]\n",
      "i=22: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=22: keys: [8.570952415466309, -8.631033897399902, 5.589956283569336, -0.9643257260322571, 0.26142600178718567]\n",
      "i=22: keys: [8.570952415466309, -8.631033897399902, 5.589956283569336, -0.9643257260322571, 0.26142600178718567]\n",
      "i=22: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=22: values: [-0.012194273993372917, -0.0007088836282491684, -0.012029183097183704, -0.007929245010018349, -0.013505207374691963]\n",
      "i=22: values: [-0.012194273993372917, -0.0007088836282491684, -0.012029183097183704, -0.007929245010018349, -0.013505207374691963]\n",
      "i=23: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=23: keys: [9.992851257324219, -3.64198899269104, 7.133339881896973, 1.2360763549804688, 0.18886756896972656]\n",
      "i=23: keys: [9.992851257324219, -3.64198899269104, 7.133339881896973, 1.2360763549804688, 0.18886756896972656]\n",
      "i=23: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=23: values: [0.023640548810362816, -0.017861833795905113, 4.386610817164183e-05, -0.031206585466861725, -0.003781156614422798]\n",
      "i=23: values: [0.023640548810362816, -0.017861833795905113, 4.386610817164183e-05, -0.031206585466861725, -0.003781156614422798]\n",
      "i=24: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=24: keys: [1.871978759765625, 2.7793164253234863, 7.292719841003418, 2.8372530937194824, -0.23204368352890015]\n",
      "i=24: keys: [1.871978759765625, 2.7793164253234863, 7.292719841003418, 2.8372530937194824, -0.23204368352890015]\n",
      "i=24: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=24: values: [5.544070154428482e-05, -0.007082168012857437, 0.005847190972417593, 0.023113764822483063, -0.004729418084025383]\n",
      "i=24: values: [5.544070154428482e-05, -0.007082168012857437, 0.005847190972417593, 0.023113764822483063, -0.004729418084025383]\n",
      "i=25: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=25: keys: [-7.699405670166016, 7.360509395599365, 6.321046829223633, 4.471487998962402, -0.09881500899791718]\n",
      "i=25: keys: [-7.699405670166016, 7.360509395599365, 6.321046829223633, 4.471487998962402, -0.09881500899791718]\n",
      "i=25: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=25: values: [0.014443587511777878, 0.030216634273529053, 0.0031355051323771477, -0.0187484510242939, -0.018295548856258392]\n",
      "i=25: values: [0.014443587511777878, 0.030216634273529053, 0.0031355051323771477, -0.0187484510242939, -0.018295548856258392]\n",
      "i=26: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=26: keys: [-10.565593719482422, 9.38346004486084, 4.024512767791748, 5.664787292480469, -0.002253033220767975]\n",
      "i=26: keys: [-10.565593719482422, 9.38346004486084, 4.024512767791748, 5.664787292480469, -0.002253033220767975]\n",
      "i=26: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=26: values: [-0.006095896475017071, -0.021186472848057747, -0.0042674848809838295, 0.0016476074233651161, 0.008377870544791222]\n",
      "i=26: values: [-0.006095896475017071, -0.021186472848057747, -0.0042674848809838295, 0.0016476074233651161, 0.008377870544791222]\n",
      "i=27: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=27: keys: [-3.6669297218322754, 7.4938154220581055, 0.886023998260498, 6.472823143005371, -0.18462860584259033]\n",
      "i=27: keys: [-3.6669297218322754, 7.4938154220581055, 0.886023998260498, 6.472823143005371, -0.18462860584259033]\n",
      "i=27: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=27: values: [-0.000817673746496439, -0.01397579163312912, -0.004544749855995178, 0.012583654373884201, 0.030712489038705826]\n",
      "i=27: values: [-0.000817673746496439, -0.01397579163312912, -0.004544749855995178, 0.012583654373884201, 0.030712489038705826]\n",
      "i=28: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=28: keys: [6.263859748840332, 2.999758005142212, -2.149522542953491, 6.839357376098633, -0.06801120191812515]\n",
      "i=28: keys: [6.263859748840332, 2.999758005142212, -2.149522542953491, 6.839357376098633, -0.06801120191812515]\n",
      "i=28: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=28: values: [-0.01906581223011017, -0.024128353223204613, -0.0009186347015202045, 0.007213250733911991, 0.0001918654888868332]\n",
      "i=28: values: [-0.01906581223011017, -0.024128353223204613, -0.0009186347015202045, 0.007213250733911991, 0.0001918654888868332]\n",
      "i=29: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=29: keys: [10.800436019897461, -3.6058290004730225, -5.008935451507568, 7.135194301605225, -0.20270997285842896]\n",
      "i=29: keys: [10.800436019897461, -3.6058290004730225, -5.008935451507568, 7.135194301605225, -0.20270997285842896]\n",
      "i=29: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=29: values: [-0.007670123130083084, -0.002676781266927719, 0.013221469707787037, 0.017711475491523743, 0.025375068187713623]\n",
      "i=29: values: [-0.007670123130083084, -0.002676781266927719, 0.013221469707787037, 0.017711475491523743, 0.025375068187713623]\n",
      "i=30: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=30: keys: [5.287684917449951, -7.826976776123047, -6.823024749755859, 6.116852760314941, -0.2107202410697937]\n",
      "i=30: keys: [5.287684917449951, -7.826976776123047, -6.823024749755859, 6.116852760314941, -0.2107202410697937]\n",
      "i=30: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=30: values: [0.000854248646646738, 0.0007033217698335648, 0.02188391238451004, 0.010203968733549118, 0.0292085949331522]\n",
      "i=30: values: [0.000854248646646738, 0.0007033217698335648, 0.02188391238451004, 0.010203968733549118, 0.0292085949331522]\n",
      "i=31: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=31: keys: [-5.6503214836120605, -9.321942329406738, -6.856749057769775, 5.129765033721924, -0.23763184249401093]\n",
      "i=31: keys: [-5.6503214836120605, -9.321942329406738, -6.856749057769775, 5.129765033721924, -0.23763184249401093]\n",
      "i=31: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=31: values: [-0.011283719912171364, -0.018521031364798546, 0.00846038106828928, -0.007884763181209564, 0.04855033755302429]\n",
      "i=31: values: [-0.011283719912171364, -0.018521031364798546, 0.00846038106828928, -0.007884763181209564, 0.04855033755302429]\n",
      "i=32: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=32: keys: [-11.093330383300781, -7.370632171630859, -6.99243688583374, 3.6207547187805176, -0.17655763030052185]\n",
      "i=32: keys: [-11.093330383300781, -7.370632171630859, -6.99243688583374, 3.6207547187805176, -0.17655763030052185]\n",
      "i=32: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=32: values: [0.024730192497372627, 0.037440843880176544, -0.008871843107044697, 0.0025019943714141846, -0.029923569411039352]\n",
      "i=32: values: [0.024730192497372627, 0.037440843880176544, -0.008871843107044697, 0.0025019943714141846, -0.029923569411039352]\n",
      "i=33: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=33: keys: [-6.7979655265808105, -2.2112412452697754, -5.2480034828186035, 1.5277869701385498, -0.23857244849205017]\n",
      "i=33: keys: [-6.7979655265808105, -2.2112412452697754, -5.2480034828186035, 1.5277869701385498, -0.23857244849205017]\n",
      "i=33: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=33: values: [0.00817275419831276, 0.00764244981110096, -0.008106788620352745, -0.00026241643354296684, -0.030109312385320663]\n",
      "i=33: values: [0.00817275419831276, 0.00764244981110096, -0.008106788620352745, -0.00026241643354296684, -0.030109312385320663]\n",
      "i=34: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=34: keys: [4.389171600341797, 4.415837287902832, -2.1390321254730225, -0.27915042638778687, -0.12758475542068481]\n",
      "i=34: keys: [4.389171600341797, 4.415837287902832, -2.1390321254730225, -0.27915042638778687, -0.12758475542068481]\n",
      "i=34: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=34: values: [0.017570769414305687, 0.005604814738035202, 0.013234658166766167, 0.010662300512194633, -0.009113814681768417]\n",
      "i=34: values: [0.017570769414305687, 0.005604814738035202, 0.013234658166766167, 0.010662300512194633, -0.009113814681768417]\n",
      "i=35: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=35: keys: [10.581768035888672, 8.286338806152344, 0.43107128143310547, -1.897810459136963, -0.2551475763320923]\n",
      "i=35: keys: [10.581768035888672, 8.286338806152344, 0.43107128143310547, -1.897810459136963, -0.2551475763320923]\n",
      "i=35: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=35: values: [0.010649347677826881, -0.0015408312901854515, -0.02095283381640911, -0.010785998776555061, -0.006621958687901497]\n",
      "i=35: values: [0.010649347677826881, -0.0015408312901854515, -0.02095283381640911, -0.010785998776555061, -0.006621958687901497]\n",
      "i=36: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=36: keys: [7.261360168457031, 9.191595077514648, 3.886387348175049, -3.777883529663086, 0.020788174122571945]\n",
      "i=36: keys: [7.261360168457031, 9.191595077514648, 3.886387348175049, -3.777883529663086, 0.020788174122571945]\n",
      "i=36: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=36: values: [0.0009755708742886782, -0.003382265567779541, 0.02600182220339775, -0.01302700862288475, 0.021509278565645218]\n",
      "i=36: values: [0.0009755708742886782, -0.003382265567779541, 0.02600182220339775, -0.01302700862288475, 0.021509278565645218]\n",
      "i=37: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=37: keys: [-2.9623515605926514, 5.745528221130371, 6.094707489013672, -5.428316593170166, 0.021577361971139908]\n",
      "i=37: keys: [-2.9623515605926514, 5.745528221130371, 6.094707489013672, -5.428316593170166, 0.021577361971139908]\n",
      "i=37: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=37: values: [0.017570769414305687, 0.005604814738035202, 0.013234658166766167, 0.010662300512194633, -0.009113814681768417]\n",
      "i=37: values: [0.017570769414305687, 0.005604814738035202, 0.013234658166766167, 0.010662300512194633, -0.009113814681768417]\n",
      "i=38: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=38: keys: [-10.120410919189453, 1.2561893463134766, 7.079216480255127, -5.913946151733398, -0.17774775624275208]\n",
      "i=38: keys: [-10.120410919189453, 1.2561893463134766, 7.079216480255127, -5.913946151733398, -0.17774775624275208]\n",
      "i=38: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=38: values: [-0.017954546958208084, -0.005517503246665001, -0.002643328160047531, 0.007015879265964031, -0.024682514369487762]\n",
      "i=38: values: [-0.017954546958208084, -0.005517503246665001, -0.002643328160047531, 0.007015879265964031, -0.024682514369487762]\n",
      "i=39: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=39: keys: [-8.10651683807373, -5.616954326629639, 6.865612506866455, -6.718010425567627, 0.12201883643865585]\n",
      "i=39: keys: [-8.10651683807373, -5.616954326629639, 6.865612506866455, -6.718010425567627, 0.12201883643865585]\n",
      "i=39: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=39: values: [-0.021417856216430664, 0.002246393822133541, 0.04015553742647171, -0.0020157925318926573, 0.01823066733777523]\n",
      "i=39: values: [-0.021417856216430664, 0.002246393822133541, 0.04015553742647171, -0.0020157925318926573, 0.01823066733777523]\n",
      "i=40: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=40: keys: [0.38527774810791016, -8.646570205688477, 6.026155948638916, -6.655004501342773, -0.20907433331012726]\n",
      "i=40: keys: [0.38527774810791016, -8.646570205688477, 6.026155948638916, -6.655004501342773, -0.20907433331012726]\n",
      "i=40: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=40: values: [0.0018903418676927686, -0.0032282075844705105, 0.0008034906350076199, 0.0014986870810389519, -0.04825535789132118]\n",
      "i=40: values: [0.0018903418676927686, -0.0032282075844705105, 0.0008034906350076199, 0.0014986870810389519, -0.04825535789132118]\n",
      "i=41: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=41: keys: [9.596842765808105, -9.210138320922852, 3.7594141960144043, -6.405407428741455, -0.09722305834293365]\n",
      "i=41: keys: [9.596842765808105, -9.210138320922852, 3.7594141960144043, -6.405407428741455, -0.09722305834293365]\n",
      "i=41: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=41: values: [-0.004697944037616253, -0.02421974390745163, -0.0021466491743922234, 0.009137154556810856, -0.01016390509903431]\n",
      "i=41: values: [-0.004697944037616253, -0.02421974390745163, -0.0021466491743922234, 0.009137154556810856, -0.01016390509903431]\n",
      "i=42: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=42: keys: [9.266149520874023, -5.176468849182129, 0.49121832847595215, -5.96397590637207, 0.23613393306732178]\n",
      "i=42: keys: [9.266149520874023, -5.176468849182129, 0.49121832847595215, -5.96397590637207, 0.23613393306732178]\n",
      "i=42: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=42: values: [5.544070154428482e-05, -0.007082168012857437, 0.005847190972417593, 0.023113764822483063, -0.004729418084025383]\n",
      "i=42: values: [5.544070154428482e-05, -0.007082168012857437, 0.005847190972417593, 0.023113764822483063, -0.004729418084025383]\n",
      "i=43: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=43: keys: [0.49207496643066406, 0.2997117042541504, -2.383256196975708, -4.302631855010986, 0.32093676924705505]\n",
      "i=43: keys: [0.49207496643066406, 0.2997117042541504, -2.383256196975708, -4.302631855010986, 0.32093676924705505]\n",
      "i=43: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=43: values: [-0.04558473452925682, -0.001737106591463089, -0.013719102367758751, 0.014772266149520874, 0.02454424276947975]\n",
      "i=43: values: [-0.04558473452925682, -0.001737106591463089, -0.013719102367758751, 0.014772266149520874, 0.02454424276947975]\n",
      "i=44: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=44: keys: [-8.027067184448242, 6.488775730133057, -6.006464004516602, -3.055582284927368, -0.014228599146008492]\n",
      "i=44: keys: [-8.027067184448242, 6.488775730133057, -6.006464004516602, -3.055582284927368, -0.014228599146008492]\n",
      "i=44: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=44: values: [-0.007878726348280907, -0.008207933977246284, 0.008532843552529812, -0.018350008875131607, 0.007643950171768665]\n",
      "i=44: values: [-0.007878726348280907, -0.008207933977246284, 0.008532843552529812, -0.018350008875131607, 0.007643950171768665]\n",
      "i=45: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=45: keys: [-10.224580764770508, 9.269366264343262, -6.881677627563477, -1.000275731086731, 0.1294642984867096]\n",
      "i=45: keys: [-10.224580764770508, 9.269366264343262, -6.881677627563477, -1.000275731086731, 0.1294642984867096]\n",
      "i=45: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=45: values: [0.012969829142093658, -0.038260601460933685, -0.008268265053629875, -0.0009136060252785683, 0.007700619287788868]\n",
      "i=45: values: [0.012969829142093658, -0.038260601460933685, -0.008268265053629875, -0.0009136060252785683, 0.007700619287788868]\n",
      "i=46: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=46: keys: [-2.468637228012085, 8.779119491577148, -7.40776252746582, 1.0596263408660889, 0.10827690362930298]\n",
      "i=46: keys: [-2.468637228012085, 8.779119491577148, -7.40776252746582, 1.0596263408660889, 0.10827690362930298]\n",
      "i=46: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=46: values: [0.0008346989052370191, 0.009060123935341835, 0.000350818270817399, -0.0071687959134578705, 0.03210063651204109]\n",
      "i=46: values: [0.0008346989052370191, 0.009060123935341835, 0.000350818270817399, -0.0071687959134578705, 0.03210063651204109]\n",
      "i=47: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=47: keys: [7.54304838180542, 5.033699035644531, -6.845690727233887, 2.844071865081787, 0.17923790216445923]\n",
      "i=47: keys: [7.54304838180542, 5.033699035644531, -6.845690727233887, 2.844071865081787, 0.17923790216445923]\n",
      "i=47: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=47: values: [-0.01669934019446373, -0.00040410365909338, -0.01700674742460251, -0.005264570005238056, 0.013117424212396145]\n",
      "i=47: values: [-0.01669934019446373, -0.00040410365909338, -0.01700674742460251, -0.005264570005238056, 0.013117424212396145]\n",
      "i=48: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=48: keys: [10.753965377807617, -0.7638130187988281, -4.962552070617676, 4.359426021575928, 0.23869779706001282]\n",
      "i=48: keys: [10.753965377807617, -0.7638130187988281, -4.962552070617676, 4.359426021575928, 0.23869779706001282]\n",
      "i=48: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=48: values: [-0.010338854975998402, -0.004892244469374418, 0.019367121160030365, -0.001371197635307908, -0.026017963886260986]\n",
      "i=48: values: [-0.010338854975998402, -0.004892244469374418, 0.019367121160030365, -0.001371197635307908, -0.026017963886260986]\n",
      "i=49: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=49: keys: [3.1951444149017334, -7.147424697875977, -1.7680188417434692, 5.675447463989258, 0.22736990451812744]\n",
      "i=49: keys: [3.1951444149017334, -7.147424697875977, -1.7680188417434692, 5.675447463989258, 0.22736990451812744]\n",
      "i=49: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=49: values: [-0.011283719912171364, -0.018521031364798546, 0.00846038106828928, -0.007884763181209564, 0.04855033755302429]\n",
      "i=49: values: [-0.011283719912171364, -0.018521031364798546, 0.00846038106828928, -0.007884763181209564, 0.04855033755302429]\n",
      "i=50: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=50: keys: [-6.746036529541016, -9.391172409057617, 0.9746465682983398, 6.418478488922119, 0.08685297518968582]\n",
      "i=50: keys: [-6.746036529541016, -9.391172409057617, 0.9746465682983398, 6.418478488922119, 0.08685297518968582]\n",
      "i=50: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=50: values: [-0.01685245707631111, -0.0071500204503536224, 0.010530441999435425, 0.0009505311027169228, -0.01182546652853489]\n",
      "i=50: values: [-0.01685245707631111, -0.0071500204503536224, 0.010530441999435425, 0.0009505311027169228, -0.01182546652853489]\n",
      "i=51: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=51: keys: [-10.687976837158203, -8.162443161010742, 3.9406332969665527, 6.865340709686279, 0.2612784206867218]\n",
      "i=51: keys: [-10.687976837158203, -8.162443161010742, 3.9406332969665527, 6.865340709686279, 0.2612784206867218]\n",
      "i=51: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=51: values: [0.008301091380417347, -0.02661164104938507, 0.012333233840763569, -0.0009931804379448295, -0.004434364847838879]\n",
      "i=51: values: [0.008301091380417347, -0.02661164104938507, 0.012333233840763569, -0.0009931804379448295, -0.004434364847838879]\n",
      "i=52: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=52: keys: [-4.61226749420166, -3.483471393585205, 6.306264400482178, 7.280170440673828, -0.016094926744699478]\n",
      "i=52: keys: [-4.61226749420166, -3.483471393585205, 6.306264400482178, 7.280170440673828, -0.016094926744699478]\n",
      "i=52: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=52: values: [0.0024654685985296965, 0.0017446614801883698, 0.037480428814888, -0.0003672391176223755, 0.023490794003009796]\n",
      "i=52: values: [0.0024654685985296965, 0.0017446614801883698, 0.037480428814888, -0.0003672391176223755, 0.023490794003009796]\n",
      "i=53: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=53: keys: [4.86208963394165, 1.6182899475097656, 7.17296028137207, 5.68756103515625, 0.3178473114967346]\n",
      "i=53: keys: [4.86208963394165, 1.6182899475097656, 7.17296028137207, 5.68756103515625, 0.3178473114967346]\n",
      "i=53: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=53: values: [-0.0003277960931882262, 0.008820454590022564, -0.00825086236000061, -0.006607989780604839, 0.003270386718213558]\n",
      "i=53: values: [-0.0003277960931882262, 0.008820454590022564, -0.00825086236000061, -0.006607989780604839, 0.003270386718213558]\n",
      "i=54: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=54: keys: [10.62094783782959, 7.272684097290039, 7.151603698730469, 5.090027332305908, 0.07007789611816406]\n",
      "i=54: keys: [10.62094783782959, 7.272684097290039, 7.151603698730469, 5.090027332305908, 0.07007789611816406]\n",
      "i=54: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=54: values: [-0.014316732063889503, -0.014635766856372356, -0.011298225261271, -0.002237391658127308, 0.0049583506770431995]\n",
      "i=54: values: [-0.014316732063889503, -0.014635766856372356, -0.011298225261271, -0.002237391658127308, 0.0049583506770431995]\n",
      "i=55: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=55: keys: [6.387019634246826, 9.42224407196045, 5.984134674072266, 3.3891549110412598, 0.10942234098911285]\n",
      "i=55: keys: [6.387019634246826, 9.42224407196045, 5.984134674072266, 3.3891549110412598, 0.10942234098911285]\n",
      "i=55: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=55: values: [0.01223883219063282, 0.030107494443655014, -9.697326458990574e-05, 0.0068582165986299515, -0.037117231637239456]\n",
      "i=55: values: [0.01223883219063282, 0.030107494443655014, -9.697326458990574e-05, 0.0068582165986299515, -0.037117231637239456]\n",
      "i=56: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=56: keys: [-3.880815029144287, 7.7236857414245605, 3.2257180213928223, 1.659106731414795, 0.1107446551322937]\n",
      "i=56: keys: [-3.880815029144287, 7.7236857414245605, 3.2257180213928223, 1.659106731414795, 0.1107446551322937]\n",
      "i=56: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=56: values: [-0.025808122009038925, -0.03131275251507759, 0.018312968313694, -0.0006017968989908695, 0.01036033220589161]\n",
      "i=56: values: [-0.025808122009038925, -0.03131275251507759, 0.018312968313694, -0.0006017968989908695, 0.01036033220589161]\n",
      "i=57: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=57: keys: [-10.526196479797363, 2.7955493927001953, 0.25971364974975586, -0.1298905611038208, 0.00272514671087265]\n",
      "i=57: keys: [-10.526196479797363, 2.7955493927001953, 0.25971364974975586, -0.1298905611038208, 0.00272514671087265]\n",
      "i=57: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=57: values: [-0.000817673746496439, -0.01397579163312912, -0.004544749855995178, 0.012583654373884201, 0.030712489038705826]\n",
      "i=57: values: [-0.000817673746496439, -0.01397579163312912, -0.004544749855995178, 0.012583654373884201, 0.030712489038705826]\n",
      "i=58: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=58: keys: [-7.753238677978516, -2.8300633430480957, -2.7479116916656494, -2.030425786972046, 0.149966761469841]\n",
      "i=58: keys: [-7.753238677978516, -2.8300633430480957, -2.7479116916656494, -2.030425786972046, 0.149966761469841]\n",
      "i=58: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=58: values: [-0.01906581223011017, -0.024128353223204613, -0.0009186347015202045, 0.007213250733911991, 0.0001918654888868332]\n",
      "i=58: values: [-0.01906581223011017, -0.024128353223204613, -0.0009186347015202045, 0.007213250733911991, 0.0001918654888868332]\n",
      "i=59: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=59: keys: [2.7978038787841797, -8.032508850097656, -5.485958099365234, -3.8711087703704834, -0.1704806089401245]\n",
      "i=59: keys: [2.7978038787841797, -8.032508850097656, -5.485958099365234, -3.8711087703704834, -0.1704806089401245]\n",
      "i=59: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=59: values: [0.0009755708742886782, -0.003382265567779541, 0.02600182220339775, -0.01302700862288475, 0.021509278565645218]\n",
      "i=59: values: [0.0009755708742886782, -0.003382265567779541, 0.02600182220339775, -0.01302700862288475, 0.021509278565645218]\n",
      "i=60: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=60: keys: [10.312078475952148, -9.018933296203613, -6.995319366455078, -5.4985551834106445, -0.24611130356788635]\n",
      "i=60: keys: [10.312078475952148, -9.018933296203613, -6.995319366455078, -5.4985551834106445, -0.24611130356788635]\n",
      "i=60: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=60: values: [0.017570769414305687, 0.005604814738035202, 0.013234658166766167, 0.010662300512194633, -0.009113814681768417]\n",
      "i=60: values: [0.017570769414305687, 0.005604814738035202, 0.013234658166766167, 0.010662300512194633, -0.009113814681768417]\n",
      "i=61: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=61: keys: [8.950028419494629, -7.44987678527832, -7.319313049316406, -5.956506729125977, -0.01878499984741211]\n",
      "i=61: keys: [8.950028419494629, -7.44987678527832, -7.319313049316406, -5.956506729125977, -0.01878499984741211]\n",
      "i=61: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=61: values: [-0.017954546958208084, -0.005517503246665001, -0.002643328160047531, 0.007015879265964031, -0.024682514369487762]\n",
      "i=61: values: [-0.017954546958208084, -0.005517503246665001, -0.002643328160047531, 0.007015879265964031, -0.024682514369487762]\n",
      "i=62: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=62: keys: [-1.1625046730041504, -1.1573963165283203, -6.1149749755859375, -6.737611770629883, -0.27324503660202026]\n",
      "i=62: keys: [-1.1625046730041504, -1.1573963165283203, -6.1149749755859375, -6.737611770629883, -0.27324503660202026]\n",
      "i=62: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=62: values: [-0.021417856216430664, 0.002246393822133541, 0.04015553742647171, -0.0020157925318926573, 0.01823066733777523]\n",
      "i=62: values: [-0.021417856216430664, 0.002246393822133541, 0.04015553742647171, -0.0020157925318926573, 0.01823066733777523]\n",
      "i=63: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=63: keys: [-9.591224670410156, 3.4721732139587402, -4.668501853942871, -6.644900798797607, -0.10122619569301605]\n",
      "i=63: keys: [-9.591224670410156, 3.4721732139587402, -4.668501853942871, -6.644900798797607, -0.10122619569301605]\n",
      "i=63: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=63: values: [0.0018903418676927686, -0.0032282075844705105, 0.0008034906350076199, 0.0014986870810389519, -0.04825535789132118]\n",
      "i=63: values: [0.0018903418676927686, -0.0032282075844705105, 0.0008034906350076199, 0.0014986870810389519, -0.04825535789132118]\n",
      "i=64: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=64: keys: [-9.216994285583496, 8.666255950927734, -1.4844032526016235, -6.536676406860352, -0.26221829652786255]\n",
      "i=64: keys: [-9.216994285583496, 8.666255950927734, -1.4844032526016235, -6.536676406860352, -0.26221829652786255]\n",
      "i=64: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=64: values: [-0.005827225744724274, 0.00031771138310432434, -0.02469661459326744, -0.014443058520555496, 0.009833810850977898]\n",
      "i=64: values: [-0.005827225744724274, 0.00031771138310432434, -0.02469661459326744, -0.014443058520555496, 0.009833810850977898]\n",
      "i=65: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=65: keys: [-0.782468318939209, 9.422120094299316, 1.231229305267334, -5.4820780754089355, -0.12793287634849548]\n",
      "i=65: keys: [-0.782468318939209, 9.422120094299316, 1.231229305267334, -5.4820780754089355, -0.12793287634849548]\n",
      "i=65: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=65: values: [-0.009519451297819614, 0.030915193259716034, 0.0008460194803774357, 0.006140300538390875, 0.011360383592545986]\n",
      "i=65: values: [-0.009519451297819614, 0.030915193259716034, 0.0008460194803774357, 0.006140300538390875, 0.011360383592545986]\n",
      "i=66: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=66: keys: [8.63992691040039, 5.9111433029174805, 4.216914176940918, -4.058135986328125, -0.1966821700334549]\n",
      "i=66: keys: [8.63992691040039, 5.9111433029174805, 4.216914176940918, -4.058135986328125, -0.1966821700334549]\n",
      "i=66: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=66: values: [0.02966076135635376, 0.014319139532744884, 0.00912228599190712, -0.005111947655677795, 0.003936827182769775]\n",
      "i=66: values: [0.02966076135635376, 0.014319139532744884, 0.00912228599190712, -0.005111947655677795, 0.003936827182769775]\n",
      "i=67: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=67: keys: [9.919234275817871, 0.9741880893707275, 6.4492974281311035, -2.2730789184570312, -0.2861331105232239]\n",
      "i=67: keys: [9.919234275817871, 0.9741880893707275, 6.4492974281311035, -2.2730789184570312, -0.2861331105232239]\n",
      "i=67: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=67: values: [-0.003259127726778388, -0.003965185023844242, 0.021860381588339806, 0.001634418498724699, -0.008613895624876022]\n",
      "i=67: values: [-0.003259127726778388, -0.003965185023844242, 0.021860381588339806, 0.001634418498724699, -0.008613895624876022]\n",
      "i=68: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=68: keys: [2.2863476276397705, -4.717153549194336, 7.388641834259033, -0.6956941485404968, -0.2739865779876709]\n",
      "i=68: keys: [2.2863476276397705, -4.717153549194336, 7.388641834259033, -0.6956941485404968, -0.2739865779876709]\n",
      "i=68: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=68: values: [-0.021000385284423828, -0.01706146076321602, 0.037776313722133636, 0.002166743390262127, -0.015242768451571465]\n",
      "i=68: values: [-0.021000385284423828, -0.01706146076321602, 0.037776313722133636, 0.002166743390262127, -0.015242768451571465]\n",
      "i=69: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=69: keys: [-8.049264907836914, -8.961466789245605, 7.074991226196289, 1.1120095252990723, -0.10028018057346344]\n",
      "i=69: keys: [-8.049264907836914, -8.961466789245605, 7.074991226196289, 1.1120095252990723, -0.10028018057346344]\n",
      "i=69: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=69: values: [-0.0015267349081113935, -0.006636586040258408, 0.024621782824397087, 0.0029389066621661186, 0.011413524858653545]\n",
      "i=69: values: [-0.0015267349081113935, -0.006636586040258408, 0.024621782824397087, 0.0029389066621661186, 0.011413524858653545]\n",
      "i=70: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=70: keys: [-10.490362167358398, -9.074172019958496, 5.593679904937744, 2.973135471343994, -0.2745591104030609]\n",
      "i=70: keys: [-10.490362167358398, -9.074172019958496, 5.593679904937744, 2.973135471343994, -0.2745591104030609]\n",
      "i=70: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=70: values: [-0.021000385284423828, -0.01706146076321602, 0.037776313722133636, 0.002166743390262127, -0.015242768451571465]\n",
      "i=70: values: [-0.021000385284423828, -0.01706146076321602, 0.037776313722133636, 0.002166743390262127, -0.015242768451571465]\n",
      "i=71: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=71: keys: [-3.096757173538208, -5.2293572425842285, 2.9155869483947754, 4.565681457519531, 0.005609037354588509]\n",
      "i=71: keys: [-3.096757173538208, -5.2293572425842285, 2.9155869483947754, 4.565681457519531, 0.005609037354588509]\n",
      "i=71: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=71: values: [0.011901716701686382, -0.0010180547833442688, 0.037717305123806, 0.0018865671008825302, 0.02024102956056595]\n",
      "i=71: values: [0.011901716701686382, -0.0010180547833442688, 0.037717305123806, 0.0018865671008825302, 0.02024102956056595]\n",
      "i=72: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=72: keys: [6.2282843589782715, -0.17204928398132324, 0.0800635814666748, 5.853600025177002, -0.1938571035861969]\n",
      "i=72: keys: [6.2282843589782715, -0.17204928398132324, 0.0800635814666748, 5.853600025177002, -0.1938571035861969]\n",
      "i=72: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=72: values: [-0.015664489939808846, -0.020384488627314568, 0.013296445831656456, -0.006370628252625465, -0.00047919806092977524]\n",
      "i=72: values: [-0.015664489939808846, -0.020384488627314568, 0.013296445831656456, -0.006370628252625465, -0.00047919806092977524]\n",
      "i=73: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=73: keys: [10.614986419677734, 5.855959415435791, -3.1729981899261475, 6.887206077575684, 0.05543981492519379]\n",
      "i=73: keys: [10.614986419677734, 5.855959415435791, -3.1729981899261475, 6.887206077575684, 0.05543981492519379]\n",
      "i=73: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=73: values: [0.017570769414305687, 0.005604814738035202, 0.013234658166766167, 0.010662300512194633, -0.009113814681768417]\n",
      "i=73: values: [0.017570769414305687, 0.005604814738035202, 0.013234658166766167, 0.010662300512194633, -0.009113814681768417]\n",
      "i=74: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=74: keys: [5.11307954788208, 9.014811515808105, -5.467425346374512, 6.915252208709717, -0.1481402963399887]\n",
      "i=74: keys: [5.11307954788208, 9.014811515808105, -5.467425346374512, 6.915252208709717, -0.1481402963399887]\n",
      "i=74: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=74: values: [-0.010015791282057762, -0.014976771548390388, 0.01605602167546749, -0.005584817845374346, 0.009931696578860283]\n",
      "i=74: values: [-0.010015791282057762, -0.014976771548390388, 0.01605602167546749, -0.005584817845374346, 0.009931696578860283]\n",
      "i=75: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=75: keys: [-4.832032203674316, 8.559340476989746, -7.351125240325928, 6.643563270568848, -0.01978052221238613]\n",
      "i=75: keys: [-4.832032203674316, 8.559340476989746, -7.351125240325928, 6.643563270568848, -0.01978052221238613]\n",
      "i=75: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=75: values: [-0.01101189199835062, -0.007866336032748222, 0.0109792435541749, -0.01404782198369503, -0.018609637394547462]\n",
      "i=75: values: [-0.01101189199835062, -0.007866336032748222, 0.0109792435541749, -0.01404782198369503, -0.018609637394547462]\n",
      "i=76: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=76: keys: [-10.956794738769531, 4.888485431671143, -7.480288505554199, 6.1730241775512695, 0.1348906010389328]\n",
      "i=76: keys: [-10.956794738769531, 4.888485431671143, -7.480288505554199, 6.1730241775512695, 0.1348906010389328]\n",
      "i=76: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=76: values: [-0.003301902674138546, -0.01251131109893322, 0.007750946097075939, 0.016825750470161438, -0.00029222946614027023]\n",
      "i=76: values: [-0.003301902674138546, -0.01251131109893322, 0.007750946097075939, 0.016825750470161438, -0.00029222946614027023]\n",
      "i=77: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=77: keys: [-5.991341590881348, -1.378488302230835, -5.985435485839844, 4.808521270751953, 0.2377016842365265]\n",
      "i=77: keys: [-5.991341590881348, -1.378488302230835, -5.985435485839844, 4.808521270751953, 0.2377016842365265]\n",
      "i=77: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=77: values: [-0.015080869197845459, -0.012183625251054764, -0.018983423709869385, -0.00479503208771348, -0.0061673251911997795]\n",
      "i=77: values: [-0.015080869197845459, -0.012183625251054764, -0.018983423709869385, -0.00479503208771348, -0.0061673251911997795]\n",
      "i=78: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=78: keys: [3.5062716007232666, -6.339954376220703, -4.4820756912231445, 3.0929782390594482, 0.05623784288764]\n",
      "i=78: keys: [3.5062716007232666, -6.339954376220703, -4.4820756912231445, 3.0929782390594482, 0.05623784288764]\n",
      "i=78: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=78: values: [-0.021929562091827393, 0.016946326941251755, 0.005516829900443554, -0.008646218106150627, -0.02680971473455429]\n",
      "i=78: values: [-0.021929562091827393, 0.016946326941251755, 0.005516829900443554, -0.008646218106150627, -0.02680971473455429]\n",
      "i=79: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=79: keys: [10.516676902770996, -8.991779327392578, -1.8622796535491943, 1.1832473278045654, 0.16051338613033295]\n",
      "i=79: keys: [10.516676902770996, -8.991779327392578, -1.8622796535491943, 1.1832473278045654, 0.16051338613033295]\n",
      "i=79: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=79: values: [0.007870092988014221, 0.018576927483081818, -0.05655134096741676, 0.04144789278507233, -0.006091557443141937]\n",
      "i=79: values: [0.007870092988014221, 0.018576927483081818, -0.05655134096741676, 0.04144789278507233, -0.006091557443141937]\n",
      "i=80: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=80: keys: [7.401490688323975, -8.3313627243042, 1.4504566192626953, -0.23045331239700317, 0.17627374827861786]\n",
      "i=80: keys: [7.401490688323975, -8.3313627243042, 1.4504566192626953, -0.23045331239700317, 0.17627374827861786]\n",
      "i=80: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=80: values: [0.007644207216799259, 0.013591557741165161, -0.0015822891145944595, 0.001692543737590313, -0.006403475999832153]\n",
      "i=80: values: [0.007644207216799259, 0.013591557741165161, -0.0015822891145944595, 0.001692543737590313, -0.006403475999832153]\n",
      "i=81: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=81: keys: [-2.2482457160949707, -3.882810592651367, 4.70955753326416, -2.3189868927001953, 0.16398882865905762]\n",
      "i=81: keys: [-2.0573959350585938, -4.006439208984375, 4.603074073791504, -2.2564873695373535, 0.13666515052318573]\n",
      "i=81: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=81: values: [-0.0004609939642250538, -0.010473654605448246, 0.006253233645111322, -0.005865316838026047, 0.007035722024738789]\n",
      "i=81: values: [0.008662698790431023, 0.0005561560392379761, 0.008996673859655857, -0.01207616738975048, 0.005271769128739834]\n",
      "i=82: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=82: keys: [-10.1902437210083, 1.8923985958099365, 6.689488887786865, -3.854337453842163, 0.13193592429161072]\n",
      "i=82: keys: [-10.181097030639648, 2.0508060455322266, 6.463529586791992, -3.85381817817688, 0.1885043829679489]\n",
      "i=82: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=82: values: [-0.00828542560338974, 0.004483524244278669, 0.01678924262523651, 0.0089088324457407, 0.016472872346639633]\n",
      "i=82: values: [-0.016189292073249817, -0.02173738181591034, 0.010493297129869461, 0.0059975688345730305, -0.00797728355973959]\n",
      "i=83: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=83: keys: [-8.354949951171875, 7.070951461791992, 7.290468215942383, -5.259913921356201, 0.1683405488729477]\n",
      "i=83: keys: [-8.553125381469727, 6.957249641418457, 7.197470664978027, -5.420056343078613, 0.18277384340763092]\n",
      "i=83: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=83: values: [-0.016189292073249817, -0.02173738181591034, 0.010493297129869461, 0.0059975688345730305, -0.00797728355973959]\n",
      "i=83: values: [0.008301092311739922, -0.02661164477467537, 0.012333233840763569, -0.0009931810200214386, -0.004434370435774326]\n",
      "i=84: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=84: keys: [0.8001976013183594, 9.231873512268066, 6.75833797454834, -6.357641220092773, 0.21462370455265045]\n",
      "i=84: keys: [0.7265701293945312, 9.513078689575195, 7.040389060974121, -5.9320831298828125, 0.2958158254623413]\n",
      "i=84: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=84: values: [0.008301092311739922, -0.02661164477467537, 0.012333233840763569, -0.0009931810200214386, -0.004434370435774326]\n",
      "i=84: values: [0.014123428612947464, 0.031063631176948547, -0.013745849952101707, 0.005426926538348198, 0.00833403505384922]\n",
      "i=85: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=85: keys: [9.589333534240723, 8.156442642211914, 5.5568318367004395, -6.481046199798584, 0.3153422772884369]\n",
      "i=85: keys: [9.602852821350098, 7.9043707847595215, 5.231091499328613, -6.778358459472656, 0.20987001061439514]\n",
      "i=85: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=85: values: [0.014123428612947464, 0.031063631176948547, -0.013745849952101707, 0.005426926538348198, 0.00833403505384922]\n",
      "i=85: values: [-0.012242800556123257, -0.009961606003344059, -0.0064076585695147514, 0.0014319880865514278, -0.01147998683154583]\n",
      "i=86: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=86: keys: [9.457822799682617, 3.167482852935791, 2.684664249420166, -6.822350025177002, 0.20621532201766968]\n",
      "i=86: keys: [9.112342834472656, 2.691537380218506, 2.6441144943237305, -7.397014617919922, 0.04287998750805855]\n",
      "i=86: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=86: values: [-0.012242800556123257, -0.009961606003344059, -0.0064076585695147514, 0.0014319880865514278, -0.01147998683154583]\n",
      "i=86: values: [0.0024654697626829147, 0.0017446477431803942, 0.037480428814888, -0.0003672422608360648, 0.0234907865524292]\n",
      "i=87: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=87: keys: [0.1086888313293457, -3.2644684314727783, -0.43734002113342285, -6.723240852355957, 0.048359934240579605]\n",
      "i=87: keys: [1.075131893157959, -2.4243292808532715, 0.030721664428710938, -6.012122631072998, 0.32158783078193665]\n",
      "i=87: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=87: values: [0.0009755748324096203, -0.0033822646364569664, 0.02600182220339775, -0.013027011416852474, 0.02150927484035492]\n",
      "i=87: values: [-0.00032779574394226074, 0.008820457383990288, -0.008250861428678036, -0.006607990711927414, 0.0032703890465199947]\n",
      "i=88: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=88: keys: [-9.115797996520996, -7.721057891845703, -3.451817035675049, -5.525010585784912, 0.07982572168111801]\n",
      "i=88: keys: [-8.793207168579102, -7.700722694396973, -3.375314474105835, -5.4536309242248535, 0.16159765422344208]\n",
      "i=88: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=88: values: [0.017570767551660538, 0.00560480821877718, 0.013234657235443592, 0.010662302374839783, -0.009113810956478119]\n",
      "i=88: values: [-0.0008176746778190136, -0.01397579163312912, -0.004544748924672604, 0.012583653442561626, 0.030712492763996124]\n",
      "i=89: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=89: keys: [-10.328571319580078, -9.471576690673828, -5.395925045013428, -3.814668893814087, 0.21907147765159607]\n",
      "i=89: keys: [-10.142770767211914, -9.380343437194824, -5.783740520477295, -4.063410758972168, 0.2408706694841385]\n",
      "i=89: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=89: values: [-0.017954552546143532, -0.005517502781003714, -0.0026433272287249565, 0.007015878334641457, -0.024682512506842613]\n",
      "i=89: values: [-0.01906581223011017, -0.024128351360559464, -0.0009186314418911934, 0.007213253993541002, 0.00019186362624168396]\n",
      "i=90: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=90: keys: [-1.674454689025879, -6.624052047729492, -6.924969673156738, -2.4673428535461426, -0.02779395878314972]\n",
      "i=90: keys: [-1.619530439376831, -7.014652252197266, -7.294772624969482, -2.623281955718994, -0.05495675653219223]\n",
      "i=90: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=90: values: [-0.021417858079075813, 0.0022464036010205746, 0.04015554115176201, -0.0020157922990620136, 0.018230658024549484]\n",
      "i=90: values: [0.0009755748324096203, -0.0033822646364569664, 0.02600182220339775, -0.013027011416852474, 0.02150927484035492]\n",
      "i=91: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=91: keys: [7.719846248626709, -2.731921911239624, -7.280368804931641, -0.5278131365776062, 0.29158344864845276]\n",
      "i=91: keys: [8.223836898803711, -1.5920071601867676, -7.2624287605285645, -0.4291067123413086, -0.07210825383663177]\n",
      "i=91: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=91: values: [0.0018903417512774467, -0.0032282061874866486, 0.0008034911006689072, 0.0014986847527325153, -0.048255354166030884]\n",
      "i=91: values: [0.017570767551660538, 0.00560480821877718, 0.013234657235443592, 0.010662302374839783, -0.009113810956478119]\n",
      "i=92: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=92: keys: [10.724635124206543, 3.5605762004852295, -6.2922821044921875, 1.2439978122711182, 0.16983357071876526]\n",
      "i=92: keys: [10.7421236038208, 3.4786102771759033, -6.34151554107666, 1.3132081031799316, 0.1482781320810318]\n",
      "i=92: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=92: values: [-0.004697943571954966, -0.024219749495387077, -0.0021466510370373726, 0.009137159213423729, -0.010163902305066586]\n",
      "i=92: values: [-0.017954552546143532, -0.005517502781003714, -0.0026433272287249565, 0.007015878334641457, -0.024682512506842613]\n",
      "i=93: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=93: keys: [2.9849627017974854, 8.543476104736328, -3.8791680335998535, 3.120070219039917, -0.2006695568561554]\n",
      "i=93: keys: [3.1029067039489746, 8.524087905883789, -3.594957113265991, 2.9989218711853027, -0.16411998867988586]\n",
      "i=93: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=93: values: [-0.014403942041099072, -0.0027071465738117695, 0.013661326840519905, 0.01775408908724785, 0.005385585129261017]\n",
      "i=93: values: [-0.021417858079075813, 0.0022464036010205746, 0.04015554115176201, -0.0020157922990620136, 0.018230658024549484]\n",
      "i=94: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=94: keys: [-6.416948318481445, 9.379037857055664, -1.3669774532318115, 4.470831871032715, 0.08725886046886444]\n",
      "i=94: keys: [-6.517375946044922, 9.451622009277344, -1.339876651763916, 4.518690586090088, 0.15717369318008423]\n",
      "i=94: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=94: values: [-0.012394349090754986, 0.011098179966211319, 0.008485275320708752, 0.0008390648290514946, -0.01303924061357975]\n",
      "i=94: values: [0.0018903417512774467, -0.0032282061874866486, 0.0008034911006689072, 0.0014986847527325153, -0.048255354166030884]\n",
      "i=95: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=95: keys: [-10.60025691986084, 5.987768173217773, 2.159166097640991, 6.157734394073486, -0.23680293560028076]\n",
      "i=95: keys: [-10.5350923538208, 5.967034339904785, 2.1414854526519775, 5.983325958251953, -0.301767498254776]\n",
      "i=95: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=95: values: [0.017570767551660538, 0.00560480821877718, 0.013234657235443592, 0.010662302374839783, -0.009113810956478119]\n",
      "i=95: values: [-0.02465834468603134, 0.023726297542452812, 0.031847670674324036, -0.0029221600852906704, 0.025653358548879623]\n",
      "i=96: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=96: keys: [-5.263991355895996, 1.5020196437835693, 4.480348587036133, 6.3817644119262695, 0.0044237710535526276]\n",
      "i=96: keys: [-4.765074253082275, 0.8605926036834717, 4.922771453857422, 6.674288749694824, -0.14679566025733948]\n",
      "i=96: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=96: values: [0.015863295644521713, -0.0037712682969868183, 0.01217263750731945, -0.008810972794890404, 0.001485634595155716]\n",
      "i=96: values: [-0.0009246049448847771, 0.000922888983041048, 0.006822931580245495, -0.0015917073469609022, -0.006216313689947128]\n",
      "i=97: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=97: keys: [5.190578460693359, -4.390320777893066, 6.419737339019775, 6.648077487945557, -0.022934667766094208]\n",
      "i=97: keys: [5.215269088745117, -4.669260501861572, 6.692017555236816, 6.8709893226623535, -0.14708463847637177]\n",
      "i=97: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=97: values: [-0.010293555445969105, 0.0011192201636731625, 0.007892688736319542, -0.004882361274212599, -0.020949238911271095]\n",
      "i=97: values: [-0.02674740180373192, -0.03246765583753586, 0.0030352603644132614, 0.011558408848941326, 0.009188827127218246]\n",
      "i=98: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=98: keys: [10.681550979614258, -9.093423843383789, 7.070146083831787, 7.094080924987793, -0.3163414001464844]\n",
      "i=98: keys: [10.505146980285645, -8.785048484802246, 7.447729110717773, 7.065521717071533, -0.21761377155780792]\n",
      "i=98: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=98: values: [-0.02465834468603134, 0.023726297542452812, 0.031847670674324036, -0.0029221600852906704, 0.025653358548879623]\n",
      "i=98: values: [0.010106559842824936, 0.007048763334751129, 0.013089342042803764, 0.013118644244968891, 0.037335105240345]\n",
      "i=99: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=99: keys: [6.0448431968688965, -8.831522941589355, 6.8911333084106445, 5.957552909851074, -0.1765936017036438]\n",
      "i=99: keys: [6.33702278137207, -8.99500846862793, 6.890186309814453, 5.824737071990967, -0.18832264840602875]\n",
      "i=99: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=99: values: [-0.0009246049448847771, 0.000922888983041048, 0.006822931580245495, -0.0015917073469609022, -0.006216313689947128]\n",
      "i=99: values: [0.0006017168052494526, 0.002438108902424574, 0.0056557729840278625, 0.002773883054032922, -0.022306103259325027]\n",
      "i=100: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=100: keys: [-4.192328453063965, -5.689151287078857, 5.092593669891357, 4.868651390075684, -0.12353883683681488]\n",
      "i=100: keys: [-3.975440740585327, -5.6155171394348145, 5.021691799163818, 4.5935163497924805, -0.19581010937690735]\n",
      "i=100: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=100: values: [-0.028107089921832085, -0.009384848177433014, -0.006786073558032513, 0.012964876368641853, 0.00013010390102863312]\n",
      "i=100: values: [0.010643396526575089, 0.015805702656507492, -0.019675791263580322, -0.0026559093967080116, -0.006014399230480194]\n",
      "i=101: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=101: keys: [-10.414251327514648, -0.2722663879394531, 2.4321060180664062, 3.124927520751953, -0.2523108422756195]\n",
      "i=101: keys: [-10.527911186218262, -0.37713170051574707, 2.5190889835357666, 3.2293877601623535, -0.22046339511871338]\n",
      "i=101: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=101: values: [0.0006017168052494526, 0.002438108902424574, 0.0056557729840278625, 0.002773883054032922, -0.022306103259325027]\n",
      "i=101: values: [-0.0004087600391358137, -0.00032507721334695816, 0.014015508815646172, -0.010043430142104626, -0.019759874790906906]\n",
      "i=102: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=102: keys: [-7.370508193969727, 5.428386688232422, -0.6390278339385986, 1.3596367835998535, -0.17613448202610016]\n",
      "i=102: keys: [-6.981027126312256, 5.656308174133301, -0.6479017734527588, 1.4215220212936401, -0.16943702101707458]\n",
      "i=102: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=102: values: [0.010643396526575089, 0.015805702656507492, -0.019675791263580322, -0.0026559093967080116, -0.006014399230480194]\n",
      "i=102: values: [-0.017966624349355698, 0.04328613728284836, -0.007292392663657665, 0.003237681230530143, 0.02941006049513817]\n",
      "i=103: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=103: keys: [2.3311867713928223, 8.94265365600586, -3.5061092376708984, -0.38834288716316223, -0.2669069468975067]\n",
      "i=103: keys: [2.358487844467163, 9.523696899414062, -4.438968181610107, -0.3841913938522339, -0.2646993398666382]\n",
      "i=103: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=103: values: [-0.0004087600391358137, -0.00032507721334695816, 0.014015508815646172, -0.010043430142104626, -0.019759874790906906]\n",
      "i=103: values: [-0.008110578171908855, -0.01199924387037754, 0.011179585941135883, -0.018040046095848083, 0.004477271810173988]\n",
      "i=104: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=104: keys: [9.875288963317871, 8.32226848602295, -5.706396579742432, -2.1904335021972656, -0.1236257553100586]\n",
      "i=104: keys: [10.401402473449707, 8.527388572692871, -5.542749404907227, -2.1178228855133057, -0.15048374235630035]\n",
      "i=104: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=104: values: [-0.017966624349355698, 0.04328613728284836, -0.007292392663657665, 0.003237681230530143, 0.02941006049513817]\n",
      "i=104: values: [-0.01128371898084879, -0.018521033227443695, 0.008460385724902153, -0.007884761318564415, 0.04855033755302429]\n",
      "i=105: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=105: keys: [7.957774639129639, 4.766595363616943, -7.960566520690918, -4.431833267211914, -0.3044557571411133]\n",
      "i=105: keys: [8.823738098144531, 5.426781177520752, -7.250297546386719, -3.7049407958984375, -0.17771786451339722]\n",
      "i=105: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=105: values: [-0.008110578171908855, -0.01199924387037754, 0.011179585941135883, -0.018040046095848083, 0.004477271810173988]\n",
      "i=105: values: [0.026682022958993912, 0.017565060406923294, 0.00047833751887083054, -0.0040550329722464085, 0.004538256675004959]\n",
      "i=106: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=106: keys: [-1.6317873001098633, -1.6304574012756348, -6.70042610168457, -5.252163887023926, -0.05478125438094139]\n",
      "i=106: keys: [-1.7581052780151367, -1.2635154724121094, -7.199853897094727, -5.6352152824401855, -0.06326872110366821]\n",
      "i=106: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=106: values: [-0.01128371898084879, -0.018521033227443695, 0.008460385724902153, -0.007884761318564415, 0.04855033755302429]\n",
      "i=106: values: [0.017570767551660538, 0.00560480821877718, 0.013234657235443592, 0.010662302374839783, -0.009113810956478119]\n",
      "i=107: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=107: keys: [-9.729253768920898, -6.122003555297852, -6.24085807800293, -6.056122779846191, -0.14760954678058624]\n",
      "i=107: keys: [-9.751258850097656, -6.191429138183594, -6.12668514251709, -6.2859039306640625, -0.20425830781459808]\n",
      "i=107: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=107: values: [0.026682022958993912, 0.017565060406923294, 0.00047833751887083054, -0.0040550329722464085, 0.004538256675004959]\n",
      "i=107: values: [0.006969214417040348, -0.003735107835382223, -0.004646465182304382, 0.007427141070365906, -0.009418437257409096]\n",
      "i=108: keys: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=108: keys: [-8.900175094604492, -8.997629165649414, -3.5675225257873535, -7.1106276512146, 0.03860584646463394]\n",
      "i=108: keys: [-9.3377685546875, -9.043362617492676, -3.6621956825256348, -6.827860355377197, -0.21577733755111694]\n",
      "i=108: values: torch.Size([2, 2, 110, 64]), torch.Size([2, 2, 109, 64])\n",
      "i=108: values: [0.017570767551660538, 0.00560480821877718, 0.013234657235443592, 0.010662302374839783, -0.009113810956478119]\n",
      "i=108: values: [-0.009796789847314358, -0.014799395576119423, 0.008242610841989517, 0.0028560608625411987, -0.005354326218366623]\n"
     ]
    }
   ],
   "source": [
    "for kv1, kv2 in zip(past_key_values, gen_out.past_key_values):\n",
    "    for i in range(min(kv1[0].shape[2], kv2[0].shape[2])):\n",
    "        print(f\"{i=}: keys: {kv1[0].shape}, {kv2[0].shape}\")\n",
    "        print(f\"{i=}: keys: {kv1[0][0, 0, i, :5].tolist()}\")\n",
    "        print(f\"{i=}: keys: {kv2[0][0, 0, i, :5].tolist()}\")\n",
    "        print(f\"{i=}: values: {kv1[1].shape}, {kv2[1].shape}\")\n",
    "        print(f\"{i=}: values: {kv1[1][0, 0, i, :5].tolist()}\")\n",
    "        print(f\"{i=}: values: {kv2[1][0, 0, i, :5].tolist()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits\n",
      "past_key_values\n"
     ]
    }
   ],
   "source": [
    "for k in dummy_model(**dummy_questions_inputs, return_dict=True).keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny-zero-1",
   "language": "python",
   "name": "tiny-zero-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
